{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sample usage case\n",
    "\n",
    "src_root = '/home/irteam/users/mjchoi/github/DL2DL/data/task1/source/'\n",
    "trg_root = '/home/irteam/users/mjchoi/github/DL2DL/data/task1/target/'\n",
    "data_loader = get_loader(src_root, trg_root, 2)\n",
    "src,trg, src_len, trg_len = dat_iter.next()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextFolder(data.Dataset):\n",
    "    def __init__(self, src_root, trg_root):\n",
    "        \"\"\"Initializes paths and preprocessing module\"\"\"\n",
    "        self.src_root = src_root\n",
    "        self.trg_root = trg_root\n",
    "        self.max_len = 100\n",
    "        self.source_filenames = os.listdir(src_root)\n",
    "        self.target_filenames = os.listdir(trg_root)\n",
    "        \n",
    "        self.source_filenames.sort()\n",
    "        self.target_filenames.sort()\n",
    "        self.load_dict()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_filename = self.source_filenames[index]\n",
    "        target_filename = self.target_filenames[index]\n",
    "        \n",
    "        with open(os.path.join(self.src_root,source_filename)) as f:\n",
    "            src = f.read()\n",
    "        with open(os.path.join(self.trg_root,target_filename)) as f:\n",
    "            trg = f.read()\n",
    "            \n",
    "        # tokenize source and target\n",
    "        src_tokens = self.tokenize(self.preprocess(src))\n",
    "        trg_tokens = self.tokenize(self.preprocess(trg),trg=True)\n",
    "        \n",
    "        # change lists of words to lists of idxs\n",
    "        src_tokens = self.wordlist2idxlist(src_tokens)\n",
    "        trg_tokens = self.wordlist2idxlist(trg_tokens)\n",
    "        return torch.LongTensor(src_tokens), torch.LongTensor(trg_tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_filenames)\n",
    "    \n",
    "    def preprocess(self, string):\n",
    "        string = string.replace('\\n',' \\n ').replace('\\t',' \\t ').replace('(',' ( ')\n",
    "        string = string.replace(')',' ) ').replace(',',' , ').replace('nn.','nn . ')\n",
    "        string = string.replace('  ',' ')\n",
    "        return string.strip()\n",
    "    \n",
    "    def load_dict(self):\n",
    "        # load dictionary\n",
    "        import json\n",
    "        with open('data/word2id.json') as f:\n",
    "            txt = f.read()\n",
    "            self.w2i = json.loads(txt)\n",
    "            \n",
    "    def wordlist2idxlist(self,wordlist):\n",
    "        out = []\n",
    "        for word in wordlist:\n",
    "            if word in self.w2i:\n",
    "                out.append(self.w2i[word])\n",
    "            else:\n",
    "                out.append(self.w2i['<UNK>'])\n",
    "        return out\n",
    "\n",
    "    def tokenize(self, string, trg=False):\n",
    "        out = string.split(' ')\n",
    "        if trg:\n",
    "            out = ['<SOS>'] + out + ['<EOS>']\n",
    "#         while(len(out)<self.max_len):\n",
    "#             out.append('<PAD>')\n",
    "#         return out[:self.max_len]\n",
    "        return out\n",
    "\n",
    "def collate_fn(data):\n",
    "    # Sort function: sorts in decreasing order by the length of the items in the right (targets)\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    sources, targets = zip(*data)\n",
    "    source_lengths = [len(x) for x in sources]\n",
    "    target_lengths = [len(x) for x in targets]\n",
    "    sources_out = torch.zeros(len(sources),max(source_lengths)).long()\n",
    "    targets_out = torch.zeros(len(targets),max(target_lengths)).long()\n",
    "    for i in range(len(sources)):\n",
    "        source_end = source_lengths[i]\n",
    "        target_end = target_lengths[i]\n",
    "        sources_out[i,:source_end] = sources[i]\n",
    "        targets_out[i,:target_end] = targets[i]\n",
    "#     prin(len(sources))\n",
    "#     print(sources)\n",
    "    return sources_out, targets_out, source_lengths, target_lengths\n",
    "\n",
    "def get_loader(src_root, trg_root, batch_size, num_workers=2, shuffle=True):\n",
    "    dataset = TextFolder(src_root, trg_root)\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
