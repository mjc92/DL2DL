{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import astunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/irteam/users/data/pytorch_code/'\n",
    "file_list = []\n",
    "for filename in glob.iglob(os.path.join(data_dir,'**/*.py'),recursive=True):\n",
    "    file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4201"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "for file_name in file_list:\n",
    "    with open(file_name) as f:\n",
    "        text = f.read()\n",
    "    if 'nn.Module' in text:\n",
    "        model_list.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/irteam/users/data/pytorch_code/pytorch/pytorch/test/test_nn.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/test/test_optim.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/test/data/network1.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/test/data/network2.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/torch/serialization.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/torch/nn/modules/container.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/torch/nn/modules/module.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/pytorch/torch/nn/utils/weight_norm.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/dcgan/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/fast_neural_style/neural_style/transformer_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/fast_neural_style/neural_style/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/mnist/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/mnist_hogwild/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/reinforcement_learning/actor_critic.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/reinforcement_learning/reinforce.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/snli/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/super_resolution/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/time_sequence_prediction/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/vae/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/examples/word_language_model/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/extension-ffi/package/test/test.py',\n",
       " '/home/irteam/users/data/pytorch_code/pytorch/extension-ffi/script/test.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-11-1-mnist_cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-11-2-mnist_deep_cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-11-3-mnist_cnn_class.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-12-1-hello-rnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-12-2-char-seq-rnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-12-4-rnn-long_char.py',\n",
       " '/home/irteam/users/data/pytorch_code/hunkim/DeepLearningZeroToAll/PyTorch/lab-12-5-stock_prediction.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/01-basics/feedforward_neural_network/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/01-basics/feedforward_neural_network/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/01-basics/linear_regression/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/01-basics/logistic_regression/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/convolutional_neural_network/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/convolutional_neural_network/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/deep_residual_network/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/deep_residual_network/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/language_model/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/language_model/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/recurrent_neural_network/main-gpu.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/02-intermediate/recurrent_neural_network/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/03-advanced/deep_convolutional_gan/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/03-advanced/image_captioning/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/03-advanced/neural_style_transfer/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/03-advanced/variational_auto_encoder/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/pytorch-tutorial/tutorials/04-utils/tensorboard/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/yunjey/mnist-svhn-transfer/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/junyanz/pytorch-CycleGAN-and-pix2pix/models/networks.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/Models.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/CopyGenerator.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/Gate.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/GlobalAttention.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/ImageEncoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/MultiHeadedAttn.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/StackedRNN.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/StructuredAttention.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/Transformer.py',\n",
       " '/home/irteam/users/data/pytorch_code/OpenNMT/OpenNMT-py/onmt/modules/Util.py',\n",
       " '/home/irteam/users/data/pytorch_code/jcjohnson/pytorch-examples/nn/dynamic_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/jcjohnson/pytorch-examples/nn/two_layer_net_module.py',\n",
       " '/home/irteam/users/data/pytorch_code/devnag/pytorch-generative-adversarial-networks/gan_pytorch.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/faster_rcnn_pytorch/faster_rcnn/faster_rcnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/faster_rcnn_pytorch/faster_rcnn/network.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/faster_rcnn_pytorch/faster_rcnn/vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/faster_rcnn_pytorch/faster_rcnn/roi_pooling/modules/roi_pool_py.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/darknet.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/layers/reorg/reorg_layer.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/layers/roi_pooling/roi_pool.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/layers/roi_pooling/roi_pool_py.py',\n",
       " '/home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/utils/network.py',\n",
       " '/home/irteam/users/data/pytorch_code/kimhc6028/relational-networks/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/kimhc6028/pathnet-pytorch/pathnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kimhc6028/pytorch-noreward-rl/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/kimhc6028/forward-thinking-pytorch/backpropagation.py',\n",
       " '/home/irteam/users/data/pytorch_code/kimhc6028/forward-thinking-pytorch/forward_thinking.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/cifar/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/imagenet/alexnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/imagenet/inception.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/imagenet/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/imagenet/squeezenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/imagenet/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/mnist/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/stl10/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/svhn/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/utee/misc.py',\n",
       " '/home/irteam/users/data/pytorch_code/aaron-xichen/pytorch-playground/utee/quant.py',\n",
       " '/home/irteam/users/data/pytorch_code/amdegroot/ssd.pytorch/ssd.py',\n",
       " '/home/irteam/users/data/pytorch_code/amdegroot/ssd.pytorch/layers/modules/l2norm.py',\n",
       " '/home/irteam/users/data/pytorch_code/amdegroot/ssd.pytorch/layers/modules/multibox_loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-a3c/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-trpo/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-meta-optimizer/layer_norm.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-meta-optimizer/layer_norm_lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-meta-optimizer/meta_optimizer.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-meta-optimizer/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/ikostrikov/pytorch-naf/naf.py',\n",
       " '/home/irteam/users/data/pytorch_code/felixgwu/mask_rcnn_pytorch/models/modules/mask_rcnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/felixgwu/mask_rcnn_pytorch/models/modules/roi_pooling/modules/roi_pool_py.py',\n",
       " '/home/irteam/users/data/pytorch_code/felixgwu/img_classification_pk_pytorch/models/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/felixgwu/img_classification_pk_pytorch/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/examples/mnist_example.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/examples/mnist_loader_example.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_complex/multi_input_multi_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_loader_simple/single_input_multi_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_loader_simple/single_input_single_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/simple_multi_input_multi_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/simple_multi_input_no_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/simple_multi_input_single_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/single_input_multi_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/single_input_no_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/tests/integration/fit_simple/single_input_single_target.py',\n",
       " '/home/irteam/users/data/pytorch_code/ncullen93/torchsample/torchsample/modules/module_trainer.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cysu/open-reid/reid/loss/oim.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cysu/open-reid/reid/loss/triplet.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cysu/open-reid/reid/models/inception.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cysu/open-reid/reid/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/jadore801120/attention-is-all-you-need-pytorch/transformer/Layers.py',\n",
       " '/home/irteam/users/data/pytorch_code/jadore801120/attention-is-all-you-need-pytorch/transformer/Models.py',\n",
       " '/home/irteam/users/data/pytorch_code/jadore801120/attention-is-all-you-need-pytorch/transformer/Modules.py',\n",
       " '/home/irteam/users/data/pytorch_code/jadore801120/attention-is-all-you-need-pytorch/transformer/SubLayers.py',\n",
       " '/home/irteam/users/data/pytorch_code/MaximumEntropy/Seq2Seq-PyTorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/SeanNaren/deepspeech.pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/picture_demo.py',\n",
       " '/home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/web_demo.py',\n",
       " '/home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/caffe_to_pytorch/convert.py',\n",
       " '/home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/training/TODO-train_from_scratch.py',\n",
       " '/home/irteam/users/data/pytorch_code/meijieru/crnn.pytorch/models/crnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/carpedm20/BEGAN-pytorch/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/BEGAN/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/DiscoGAN/model/Discriminator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/DiscoGAN/model/Generator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/NeuralSytleTransfer/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/NeuralSytleTransfer/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/VAE/vae.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/char-rnn/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/classification/model/CNN.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/classification/model/NIN.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/classification/model/ResNet.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/cycleGAN/model/Discriminator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/cycleGAN/model/Generator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/dcgan/model/Discriminator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/dcgan/model/Generator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/fast-neural-style/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/fast-neural-style/utils/transformer.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/fast-neural-style/utils/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/pix2pix/model/Discriminator.py',\n",
       " '/home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/pix2pix/model/Generator.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/vqa.pytorch/vqa/models/att.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/vqa.pytorch/vqa/models/fusion.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/vqa.pytorch/vqa/models/noatt.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/vqa.pytorch/vqa/models/seq2vec.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/tensorflow-model-zoo.torch/inceptionresnetv2/pytorch_load.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/tensorflow-model-zoo.torch/inceptionv4/pytorch_load.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/bninception.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/fbresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/inceptionresnetv2.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/inceptionv4.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/wideresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/fbresnet/resnet152_load.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/Att2inModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/AttModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/CaptionModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/FCModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/ShowTellModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/resnet_utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/neuraltalk2.pytorch/misc/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/pytorch-resnet/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/Att2inModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/FCModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/ShowTellModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/resnet_utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/ruotianluo/self-critical.pytorch/misc/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/wkentaro/pytorch-fcn/torchfcn/models/fcn32s.py',\n",
       " '/home/irteam/users/data/pytorch_code/cemoody/topicsne/topic_sne.py',\n",
       " '/home/irteam/users/data/pytorch_code/cemoody/topicsne/tsne.py',\n",
       " '/home/irteam/users/data/pytorch_code/cemoody/topicsne/vtsne.py',\n",
       " '/home/irteam/users/data/pytorch_code/jinfagang/pytorch_chatbot/models/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/jinfagang/pytorch_cycle_gan/models/networks.py',\n",
       " '/home/irteam/users/data/pytorch_code/jinfagang/pytorch_name_net/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/bamos/densenet.pytorch/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/bamos/densenet.pytorch/attic/numcheck-grads.py',\n",
       " '/home/irteam/users/data/pytorch_code/bodokaiser/piwise/piwise/criterion.py',\n",
       " '/home/irteam/users/data/pytorch_code/bodokaiser/piwise/piwise/network.py',\n",
       " '/home/irteam/users/data/pytorch_code/isht7/pytorch-deeplab-resnet/deeplab_resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/vinhkhuc/PyTorch-Mini-Tutorials/5_convolutional_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/vinhkhuc/PyTorch-Mini-Tutorials/6_lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/zuoxingdong/VIN_PyTorch_Visdom/VIN.py',\n",
       " '/home/irteam/users/data/pytorch_code/JianGoForIt/YellowFin_Pytorch/pytorch-cifar/models/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/JianGoForIt/YellowFin_Pytorch/word_language_model/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_language.py',\n",
       " '/home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_mnist.py',\n",
       " '/home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_toy.py',\n",
       " '/home/irteam/users/data/pytorch_code/jingweiz/pytorch-rl/core/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/jingweiz/pytorch-dnc/core/accessor.py',\n",
       " '/home/irteam/users/data/pytorch_code/jingweiz/pytorch-dnc/core/circuit.py',\n",
       " '/home/irteam/users/data/pytorch_code/jingweiz/pytorch-dnc/core/controller.py',\n",
       " '/home/irteam/users/data/pytorch_code/jingweiz/pytorch-dnc/core/head.py',\n",
       " '/home/irteam/users/data/pytorch_code/abhiskk/fast-neural-style/neural_style/transformer_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/abhiskk/fast-neural-style/neural_style/vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/oeway/pytorch-deform-conv/torch_deform_conv/cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/fcn.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/linknet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/pspnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/segnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/unet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meetshah1995/pytorch-semseg/ptsemseg/models/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/optnet/cls/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/optnet/cls/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/optnet/denoising/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/optnet/sudoku/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/e2e-model-learning/newsvendor/task_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/locuslab/e2e-model-learning/power_sched/model_classes.py',\n",
       " '/home/irteam/users/data/pytorch_code/atgambardella/pytorch-es/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/maciejkula/spotlight/spotlight/factorization/representations.py',\n",
       " '/home/irteam/users/data/pytorch_code/maciejkula/spotlight/spotlight/sequence/representations.py',\n",
       " '/home/irteam/users/data/pytorch_code/maciejkula/netrex/netrex/netrex.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/bytenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/conv.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/recurrent.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/seq2seq_base.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/transformer.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/modules/attention.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/modules/normalization.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/modules/recurrent.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/models/modules/vision_encoders.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/seq2seq.pytorch/seq2seq/tools/trainer.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/alexnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/inception_resnet_v2.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/inception_v2.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/mnist.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/eladhoffer/convNet.pytorch/models/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/lanpa/tensorboard-pytorch/demo_graph.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrzhu-cool/pix2pix-pytorch/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/frombeijingwithlove/dlcv_for_beginners/random_bonus/gan_n_cgan_2d_example/networks.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/dpn.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/googlenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/lenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/mobilenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/shufflenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-cifar/models/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-ssd/multibox_layer.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-ssd/multibox_loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/kuangliu/pytorch-ssd/ssd.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/01-Linear Regression/Linear_Regression.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/02-Logistic Regression/Logistic_Regression.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/03-Neural Network/neural_network.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/04-Convolutional Neural Network/convolution_network.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/05-Recurrent Neural Network/recurrent_network.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/06-Natural Language Process/LSTM.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/06-Natural Language Process/N-Gram.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/06-Natural Language Process/bag-of-word.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/07-Language Model/language model.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/08-AutoEncoder/Variational_autoencoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/08-AutoEncoder/conv_autoencoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/08-AutoEncoder/simple_autoencoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/09-Generative Adversarial network/conv_gan.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/09-Generative Adversarial network/simple_Gan.py',\n",
       " '/home/irteam/users/data/pytorch_code/SherlockLiao/pytorch-beginner/10-Deep Q learning/reinforcement learning.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/alexnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/preresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/cifar/wrn.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-classification/models/imagenet/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-pose/pose/models/hourglass.py',\n",
       " '/home/irteam/users/data/pytorch_code/bearpaw/pytorch-pose/pose/models/preresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/mattmacy/vnet.pytorch/vnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/Shawn1993/cnn-text-classification-pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/ycszen/pytorch-ss/duc.py',\n",
       " '/home/irteam/users/data/pytorch_code/ycszen/pytorch-ss/gcn.py',\n",
       " '/home/irteam/users/data/pytorch_code/ycszen/pytorch-ss/loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/ycszen/pytorch-ss/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ycszen/pytorch-ss/upsample.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/darknet.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/region_loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/layers/batchnorm/bn.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/models/caffe_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/models/tiny_yolo.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-mobilenet/benchmark.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-mobilenet/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-caffe-darknet-convert/caffenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-caffe-darknet-convert/darknet.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-caffe-darknet-convert/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/marvis/pytorch-caffe-darknet-convert/pytorch2caffe.py',\n",
       " '/home/irteam/users/data/pytorch_code/dpressel/baseline/python/baseline/pytorch/torchy.py',\n",
       " '/home/irteam/users/data/pytorch_code/dpressel/baseline/python/baseline/pytorch/classify/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/dpressel/baseline/python/baseline/pytorch/seq2seq/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/dpressel/baseline/python/baseline/pytorch/tagger/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/ClementPinard/FlowNetPytorch/multiscaleloss.py',\n",
       " '/home/irteam/users/data/pytorch_code/ClementPinard/FlowNetPytorch/models/FlowNetS.py',\n",
       " '/home/irteam/users/data/pytorch_code/ClementPinard/FlowNetPytorch/models/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/jacobgil/pytorch-pruning/finetune.py',\n",
       " '/home/irteam/users/data/pytorch_code/ShangtongZhang/DeepRL/network.py',\n",
       " '/home/irteam/users/data/pytorch_code/kendricktan/drawlikebobross/aae/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/aleju/self-driving-truck/train_reinforced/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/aleju/self-driving-truck/train_semisupervised/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/aleju/self-driving-truck/train_steering_wheel/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/aleju/gan-error-avoidance/common/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/aleju/cat-bbs/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/fxia22/pointnet.pytorch/pointnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/fxia22/kdnet.pytorch/kdnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/fxia22/kdnet.pytorch/test.py',\n",
       " '/home/irteam/users/data/pytorch_code/fxia22/kdnet.pytorch/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/fducau/AAE_pytorch/script/aae_pytorch_basic.py',\n",
       " '/home/irteam/users/data/pytorch_code/fducau/AAE_pytorch/script/aae_semisupervised.py',\n",
       " '/home/irteam/users/data/pytorch_code/fducau/AAE_pytorch/script/aae_supervised.py',\n",
       " '/home/irteam/users/data/pytorch_code/zhanghang1989/PyTorch-Style-Transfer/experiments/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/zhanghang1989/PyTorch-Style-Transfer/experiments/myutils/vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/zhanghang1989/PyTorch-Style-Transfer/experiments/net/msg_net_v1.py',\n",
       " '/home/irteam/users/data/pytorch_code/zhanghang1989/PyTorch-Style-Transfer/experiments/net/msg_net_v2.py',\n",
       " '/home/irteam/users/data/pytorch_code/zhanghang1989/PyTorch-Style-Transfer/experiments/net/mynn.py',\n",
       " '/home/irteam/users/data/pytorch_code/dasguptar/treelstm.pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/model/decoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/model/encoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/model/rvae.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/selfModules/embedding.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/selfModules/highway.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/selfModules/neg.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_RVAE/selfModules/tdnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/model/decoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/model/encoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/model/rvae_dilated.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/selfModules/embedding.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/selfModules/highway.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/selfModules/neg.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/selfModules/perplexity.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/contiguous-succotash/selfModules/tdnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_NEG_loss/NEG_loss/neg.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_Highway/highway/highway.py',\n",
       " '/home/irteam/users/data/pytorch_code/kefirski/pytorch_TDNN/TDNN/tdnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/twtygqyy/pytorch-SRResNet/srresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/twtygqyy/pytorch-vdsr/vdsr.py',\n",
       " '/home/irteam/users/data/pytorch_code/twtygqyy/pytorch-LapSRN/lapsrn.py',\n",
       " '/home/irteam/users/data/pytorch_code/twtygqyy/pytorch-LapSRN/lapsrn_wgan.py',\n",
       " '/home/irteam/users/data/pytorch_code/andreasveit/densenet-pytorch/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/andreasveit/triplet-network-pytorch/train.py',\n",
       " '/home/irteam/users/data/pytorch_code/andreasveit/triplet-network-pytorch/tripletnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/castorini/Castor/kim_cnn/model/cnnText/cnntext.py',\n",
       " '/home/irteam/users/data/pytorch_code/castorini/Castor/simple_qa_rnn/relation_prediction/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/castorini/Castor/sm_cnn/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/kentsommer/pytorch-value-iteration-networks/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/alexis-jacq/Pytorch-Sketch-RNN/sketch_rnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/alexis-jacq/Pytorch-DPPO/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/justdark/pytorch-poetry-gen/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/shiba24/3d-unet/pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/hitvoice/DrQA/drqa/layers.py',\n",
       " '/home/irteam/users/data/pytorch_code/hitvoice/DrQA/drqa/rnn_reader.py',\n",
       " '/home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/faster_rcnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/network.py',\n",
       " '/home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/rfcn.py',\n",
       " '/home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/roi_pooling/modules/roi_pool_py.py',\n",
       " '/home/irteam/users/data/pytorch_code/xternalz/WideResNet-pytorch/wideresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/mingyuliutw/CoGAN_PyTorch/src/net_cogan_mnist2usps.py',\n",
       " '/home/irteam/users/data/pytorch_code/mingyuliutw/CoGAN_PyTorch/src/net_cogan_mnistedge.py',\n",
       " '/home/irteam/users/data/pytorch_code/mingyuliutw/CoGAN_PyTorch/src/net_gan_mnist.py',\n",
       " '/home/irteam/users/data/pytorch_code/prlz77/ResNeXt.pytorch/models/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/ypxie/pytorch-NeuCom/neucom/controller.py',\n",
       " '/home/irteam/users/data/pytorch_code/ypxie/pytorch-NeuCom/neucom/dnc.py',\n",
       " '/home/irteam/users/data/pytorch_code/ypxie/pytorch-NeuCom/neucom/memory.py',\n",
       " '/home/irteam/users/data/pytorch_code/ypxie/pytorch-NeuCom/neucom/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/ypxie/pytorch-cramer-Gan/cramerGan/Models.py',\n",
       " '/home/irteam/users/data/pytorch_code/onlytailei/Value-Iteration-Networks-PyTorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/onlytailei/A3C-PyTorch/A3C.py',\n",
       " '/home/irteam/users/data/pytorch_code/imatge-upc/retrieval-2017-cam/pytorch_code/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/imatge-upc/retrieval-2017-cam/pytorch_code/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/github-pengge/GANs/utils/nets.py',\n",
       " '/home/irteam/users/data/pytorch_code/IBM/pytorch-seq2seq/seq2seq/models/DecoderRNN.py',\n",
       " '/home/irteam/users/data/pytorch_code/IBM/pytorch-seq2seq/seq2seq/models/TopKDecoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/IBM/pytorch-seq2seq/seq2seq/models/attention.py',\n",
       " '/home/irteam/users/data/pytorch_code/IBM/pytorch-seq2seq/seq2seq/models/baseRNN.py',\n",
       " '/home/irteam/users/data/pytorch_code/IBM/pytorch-seq2seq/seq2seq/models/seq2seq.py',\n",
       " '/home/irteam/users/data/pytorch_code/transedward/pytorch-dqn/dqn_model.py',\n",
       " '/home/irteam/users/data/pytorch_code/andrewliao11/dni.pytorch/cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/andrewliao11/dni.pytorch/dni.py',\n",
       " '/home/irteam/users/data/pytorch_code/andrewliao11/dni.pytorch/mlp.py',\n",
       " '/home/irteam/users/data/pytorch_code/andrewliao11/dni.pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/andrewliao11/pytorch-a3c-mujoco/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/dgriff777/rl_a3c_pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/thnkim/OpenFacePytorch/SpatialCrossMapLRN_temp.py',\n",
       " '/home/irteam/users/data/pytorch_code/thnkim/OpenFacePytorch/loadOpenFace.py',\n",
       " '/home/irteam/users/data/pytorch_code/cmusjtuliuyuan/SequenceTagging/CRF.py',\n",
       " '/home/irteam/users/data/pytorch_code/cmusjtuliuyuan/SequenceTagging/LstmCrfModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/cmusjtuliuyuan/SequenceTagging/LstmModel.py',\n",
       " '/home/irteam/users/data/pytorch_code/D-X-Y/ResNeXt/models/caffe_cifar.py',\n",
       " '/home/irteam/users/data/pytorch_code/D-X-Y/ResNeXt/models/preresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/D-X-Y/ResNeXt/models/res_utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/D-X-Y/ResNeXt/models/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/D-X-Y/ResNeXt/models/resnext.py',\n",
       " '/home/irteam/users/data/pytorch_code/guillitte/pytorch-sentiment-neuron/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/ajbrock/FreezeOut/WRN.py',\n",
       " '/home/irteam/users/data/pytorch_code/ajbrock/FreezeOut/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ajbrock/FreezeOut/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/mjacar/pytorch-trpo/trpo_agent.py',\n",
       " '/home/irteam/users/data/pytorch_code/mjacar/pytorch-trpo/models/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/mjacar/pytorch-nec/nec_agent.py',\n",
       " '/home/irteam/users/data/pytorch_code/mjacar/pytorch-nec/models/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/ShiyuLiang/odin-pytorch/code/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ShiyuLiang/odin-pytorch/code/wideresnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/sagiebenaim/DistanceGAN/cyclegan_arch/networks.py',\n",
       " '/home/irteam/users/data/pytorch_code/sagiebenaim/DistanceGAN/cyclegan_arch/mnist_to_svhn/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/sagiebenaim/DistanceGAN/discogan_arch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/BoyuanJiang/context_encoder_pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/AaronYALai/Generative_Adversarial_Networks_PyTorch/DCGAN/DCGAN.py',\n",
       " '/home/irteam/users/data/pytorch_code/AaronYALai/Generative_Adversarial_Networks_PyTorch/GAN/GAN.py',\n",
       " '/home/irteam/users/data/pytorch_code/AaronYALai/Generative_Adversarial_Networks_PyTorch/ImprovedGAN/ImprovedGAN.py',\n",
       " '/home/irteam/users/data/pytorch_code/AaronYALai/Generative_Adversarial_Networks_PyTorch/InfoGAN/InfoGAN.py',\n",
       " '/home/irteam/users/data/pytorch_code/AaronYALai/Generative_Adversarial_Networks_PyTorch/LAPGAN/LAPGAN.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/grammar_variational_autoencoder/grammar_vae.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/grammar_variational_autoencoder/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/modules/sampleRNN.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/modules/seq2seq.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/neural_vocoder/neural_vocoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/neural_vocoder/recursive_dilation.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/text_to_features/text_to_feature.py',\n",
       " '/home/irteam/users/data/pytorch_code/episodeyang/char2wav_pytorch/word_seq_2_seq/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/bengxy/FastNeuralStyle/net.py',\n",
       " '/home/irteam/users/data/pytorch_code/automan000/Convolution_LSTM_pytorch/convolution_lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/JamesChuanggg/pytorch-REINFORCE/reinforce_continuous.py',\n",
       " '/home/irteam/users/data/pytorch_code/JamesChuanggg/pytorch-REINFORCE/reinforce_discrete.py',\n",
       " '/home/irteam/users/data/pytorch_code/gujiuxiang/MIL.pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/GunhoChoi/FusionNet_Pytorch/FusionNet.py',\n",
       " '/home/irteam/users/data/pytorch_code/spro/pytorch-seq2seq-intent-parsing/attn.py',\n",
       " '/home/irteam/users/data/pytorch_code/spro/pytorch-seq2seq-intent-parsing/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/spro/char-rnn.pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/spro/pytorch-text-vae/model-cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/spro/pytorch-text-vae/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/dannysdeng/selu/selu.py',\n",
       " '/home/irteam/users/data/pytorch_code/ShubhangDesai/rnn-encoder-decoder/modules/Decoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/ShubhangDesai/rnn-encoder-decoder/modules/Encoder.py',\n",
       " '/home/irteam/users/data/pytorch_code/pemami4911/neural-combinatorial-rl-pytorch/neural_combinatorial_rl.py',\n",
       " '/home/irteam/users/data/pytorch_code/ghliu/pytorch-ddpg/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/deepsound-project/samplernn-pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/deepsound-project/samplernn-pytorch/nn.py',\n",
       " '/home/irteam/users/data/pytorch_code/gsp-27/pytorch_Squeezenet/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/gsp-27/pytorch_Squeezenet/temp.py',\n",
       " '/home/irteam/users/data/pytorch_code/delta-onera/segnet_pytorch/poolTOdetect.py',\n",
       " '/home/irteam/users/data/pytorch_code/delta-onera/segnet_pytorch/segnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/akolishchak/doom-net-pytorch/aac.py',\n",
       " '/home/irteam/users/data/pytorch_code/akolishchak/doom-net-pytorch/aac_lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/akolishchak/doom-net-pytorch/aac_noisy.py',\n",
       " '/home/irteam/users/data/pytorch_code/akolishchak/doom-net-pytorch/lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/akolishchak/doom-net-pytorch/noisy_linear.py',\n",
       " '/home/irteam/users/data/pytorch_code/c0nn3r/pytorch_highway_networks/main.py',\n",
       " '/home/irteam/users/data/pytorch_code/c0nn3r/pytorch_highway_networks/layers/highway.py',\n",
       " '/home/irteam/users/data/pytorch_code/ucla-vision/entropy-sgd/python/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/rogertrullo/pytorch_convlstm/conv_lstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/jhave/pytorch-poetry-generation/word_language_model/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/xiayandi/Pytorch_text_classification/cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/rarilurelo/pytorch_a3c/policy.py',\n",
       " '/home/irteam/users/data/pytorch_code/demelin/Noise-Contrastive-Estimation-NCE-for-pyTorch/nce_loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/AlfredXiangWu/LightCNN/light_cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/vzhong/chainer2pytorch/tc/nn.py',\n",
       " '/home/irteam/users/data/pytorch_code/joansj/pytorch-intro/src/cifar_model.py',\n",
       " '/home/irteam/users/data/pytorch_code/joansj/pytorch-intro/src/ptb_model.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/test/test_nn.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/test/data/network1.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/test/data/network2.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/torch/serialization.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/torch/nn/modules/container.py',\n",
       " '/home/irteam/users/data/pytorch_code/apaszke/pytorch-dist/torch/nn/modules/module.py',\n",
       " '/home/irteam/users/data/pytorch_code/meliketoy/fine-tuning.pytorch/networks/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meliketoy/wide-resnet.pytorch/networks/lenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meliketoy/wide-resnet.pytorch/networks/resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meliketoy/wide-resnet.pytorch/networks/vggnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/meliketoy/wide-resnet.pytorch/networks/wide_resnet.py',\n",
       " '/home/irteam/users/data/pytorch_code/jihunchoi/recurrent-batch-normalization-pytorch/bnlstm.py',\n",
       " '/home/irteam/users/data/pytorch_code/mailmahee/pytorch-generative-adversarial-networks/gan_pytorch.py',\n",
       " '/home/irteam/users/data/pytorch_code/jmtomczak/vae_vpflows/models/VAE.py',\n",
       " '/home/irteam/users/data/pytorch_code/jmtomczak/vae_vpflows/models/VAE_HF.py',\n",
       " '/home/irteam/users/data/pytorch_code/jmtomczak/vae_vpflows/models/VAE_ccLinIAF.py',\n",
       " '/home/irteam/users/data/pytorch_code/moskomule/eve.pytorch/eve_test.py',\n",
       " '/home/irteam/users/data/pytorch_code/jaxony/ShuffleNet/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/lantiga/pytorch2c/test/mnist.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/dynamic.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/dynamic2.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/fakedynamic.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/fakestatic.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/static.py',\n",
       " '/home/irteam/users/data/pytorch_code/mrdrozdov/pytorch-dynamic-batching/models/static2.py',\n",
       " '/home/irteam/users/data/pytorch_code/cxhernandez/molencoder/molencoder/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/cxhernandez/molencoder/molencoder/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/oyam/pytorch-DPNs/dpn.py',\n",
       " '/home/irteam/users/data/pytorch_code/Jiaming-Liu/pytorch-misc/feature_loader.py',\n",
       " '/home/irteam/users/data/pytorch_code/Jiaming-Liu/pytorch-misc/glorot_uniform_init.py',\n",
       " '/home/irteam/users/data/pytorch_code/dandelin/Dynamic-memory-networks-plus-Pytorch/babi_main.py',\n",
       " '/home/irteam/users/data/pytorch_code/bheinzerling/ran/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/bheinzerling/ran/ran.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/test/test_nn.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/test/data/network1.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/test/data/network2.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/torch/serialization.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/torch/nn/modules/container.py',\n",
       " '/home/irteam/users/data/pytorch_code/tylergenter/pytorch/torch/nn/modules/module.py',\n",
       " '/home/irteam/users/data/pytorch_code/sniklaus/pytorch-extension/test.py',\n",
       " '/home/irteam/users/data/pytorch_code/EthanRosenthal/torchmf/torchmf.py',\n",
       " '/home/irteam/users/data/pytorch_code/qbx2/PAAC.pytorch/paac.py',\n",
       " '/home/irteam/users/data/pytorch_code/vanzytay/pytorch_sentiment_rnn/models/attention.py',\n",
       " '/home/irteam/users/data/pytorch_code/vanzytay/pytorch_sentiment_rnn/models/rnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/vanzytay/pytorch_sentiment_rnn/models/td_rnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/gpleiss/efficient_densenet_pytorch/models/densenet.py',\n",
       " '/home/irteam/users/data/pytorch_code/gpleiss/efficient_densenet_pytorch/models/densenet_efficient.py',\n",
       " '/home/irteam/users/data/pytorch_code/tymokvo/pt-styletransfer/00_styletransfernet.py',\n",
       " '/home/irteam/users/data/pytorch_code/tymokvo/pt-styletransfer/vgg.py',\n",
       " '/home/irteam/users/data/pytorch_code/JunhongXu/planet-kaggle-pytorch/planet_models/blender.py',\n",
       " '/home/irteam/users/data/pytorch_code/JunhongXu/planet-kaggle-pytorch/planet_models/fpn.py',\n",
       " '/home/irteam/users/data/pytorch_code/JunhongXu/planet-kaggle-pytorch/planet_models/resnet_planet.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZeweiChu/nmt-seq2seq/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZeweiChu/nmt-seq2seq/utils.py',\n",
       " '/home/irteam/users/data/pytorch_code/bryanyzhu/two-stream-pytorch/models/flow_vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/bryanyzhu/two-stream-pytorch/models/rgb_vgg16.py',\n",
       " '/home/irteam/users/data/pytorch_code/edgarriba/ali-pytorch/models.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/fcn16.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/fcn32.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/fcn8.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/psp_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/seg_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/ZijunDeng/pytorch-semantic-segmentation/models/u_net.py',\n",
       " '/home/irteam/users/data/pytorch_code/emited/VariationalRecurrentNeuralNetwork/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/vabh/convolutional-neural-fabrics/neural_fabrics.py',\n",
       " '/home/irteam/users/data/pytorch_code/songrotek/a2c_cartpole_pytorch/a2c_cartpole.py',\n",
       " '/home/irteam/users/data/pytorch_code/pcyin/pytorch_nmt/nmt.py',\n",
       " '/home/irteam/users/data/pytorch_code/huaijin-chen/pytorch-PersonReID/module/TripletLoss.py',\n",
       " '/home/irteam/users/data/pytorch_code/huaijin-chen/pytorch-PersonReID/module/l2normalize.py',\n",
       " '/home/irteam/users/data/pytorch_code/huaijin-chen/pytorch-PersonReID/module/symbols.py',\n",
       " '/home/irteam/users/data/pytorch_code/bzcheeseman/pytorch-rwa/model/rwa.py',\n",
       " '/home/irteam/users/data/pytorch_code/Stonesjtu/Pytorch-NCE/crossEntropy.py',\n",
       " '/home/irteam/users/data/pytorch_code/Stonesjtu/Pytorch-NCE/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/Stonesjtu/Pytorch-NCE/nce.py',\n",
       " '/home/irteam/users/data/pytorch_code/jakeoung/Unet_pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/andabi/deep-text-corrector/seq2seq/seq2seq.py',\n",
       " '/home/irteam/users/data/pytorch_code/scientist1642/bombora/models/ff_basic.py',\n",
       " '/home/irteam/users/data/pytorch_code/scientist1642/bombora/models/lstm_nature.py',\n",
       " '/home/irteam/users/data/pytorch_code/scientist1642/bombora/models/lstm_universe.py',\n",
       " '/home/irteam/users/data/pytorch_code/scientist1642/bombora/models/nips_ff.py',\n",
       " '/home/irteam/users/data/pytorch_code/alantian/pytorch-char-rnnlm/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/sirius27/cnn-pytorch/cnn.py',\n",
       " '/home/irteam/users/data/pytorch_code/speedinghzl/Pytorch-Deeplab/deeplab/loss.py',\n",
       " '/home/irteam/users/data/pytorch_code/speedinghzl/Pytorch-Deeplab/deeplab/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/losses/BCELoss.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/losses/HingeLoss.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/losses/LeakyHingeLoss.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/losses/SumLoss.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/models/dcgan.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/models/mlp.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/models/toy.py',\n",
       " '/home/irteam/users/data/pytorch_code/lim0606/pytorch-geometric-gan/models/toy4.py',\n",
       " '/home/irteam/users/data/pytorch_code/Adoni/word2vec_pytorch/model.py',\n",
       " '/home/irteam/users/data/pytorch_code/berlino/MemNN/Model.py']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import random\n",
      "import string\n",
      "import unittest\n",
      "import itertools\n",
      "import contextlib\n",
      "import warnings\n",
      "import pickle\n",
      "from copy import deepcopy\n",
      "from itertools import repeat, product\n",
      "from functools import wraps, reduce\n",
      "from operator import mul\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.nn.parallel as dp\n",
      "import torch.nn.init as init\n",
      "import torch.nn.utils.rnn as rnn_utils\n",
      "import torch.legacy.nn as legacy\n",
      "from torch.nn.utils import clip_grad_norm\n",
      "from torch.autograd import Variable, gradcheck\n",
      "from torch.autograd.gradcheck import gradgradcheck\n",
      "from torch.nn import Parameter\n",
      "from common_nn import NNTestCase, ModuleTest, CriterionTest, TestBase, \\\n",
      "    module_tests, criterion_tests, TEST_CUDA, TEST_MULTIGPU, TEST_CUDNN, \\\n",
      "    TEST_CUDNN_VERSION\n",
      "from common import freeze_rng_state, run_tests, TestCase, skipIfNoLapack, TEST_SCIPY, download_file\n",
      "\n",
      "if TEST_SCIPY:\n",
      "    from scipy import stats\n",
      "\n",
      "\n",
      "@contextlib.contextmanager\n",
      "def use_cudnn(should_use):\n",
      "    orig = torch.backends.cudnn.enabled\n",
      "    torch.backends.cudnn.enabled = should_use\n",
      "    try:\n",
      "        yield\n",
      "    finally:\n",
      "        torch.backends.cudnn.enabled = orig\n",
      "\n",
      "\n",
      "def default_tensor_type(type):\n",
      "    type_str = torch.typename(type)\n",
      "\n",
      "    def decorator(fn):\n",
      "        @wraps(fn)\n",
      "        def wrapper(*args, **kwargs):\n",
      "            old_type = torch.typename(torch.Tensor())\n",
      "            torch.set_default_tensor_type(type_str)\n",
      "            try:\n",
      "                return fn(*args, **kwargs)\n",
      "            finally:\n",
      "                torch.set_default_tensor_type(old_type)\n",
      "\n",
      "        return wrapper\n",
      "\n",
      "    return decorator\n",
      "\n",
      "\n",
      "class InputVariableMixin(object):\n",
      "    def _get_input(self):\n",
      "        input = TestBase._get_input(self)\n",
      "\n",
      "        def map_variables(i):\n",
      "            if isinstance(i, Variable):\n",
      "                return i\n",
      "            elif torch.is_tensor(i):\n",
      "                return Variable(i, requires_grad=True)\n",
      "            else:\n",
      "                return type(i)(map_variables(elem) for elem in i)\n",
      "\n",
      "        return map_variables(input)\n",
      "\n",
      "\n",
      "class NewModuleTest(InputVariableMixin, ModuleTest):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(NewModuleTest, self).__init__(*args, **kwargs)\n",
      "        self.cudnn = kwargs.get('cudnn', False)\n",
      "        self.check_inplace = kwargs.get('check_inplace', False)\n",
      "\n",
      "    def _do_test(self, test_case, module, input):\n",
      "        test_case.check_jacobian(module, input, self.jacobian_input)\n",
      "\n",
      "        # check if module can be printed\n",
      "        module.__repr__()\n",
      "\n",
      "        if self.check_inplace:\n",
      "            module_ip = self.constructor(*self.constructor_args, inplace=True)\n",
      "\n",
      "            input_version = input._version\n",
      "            with freeze_rng_state():\n",
      "                output = module(input)\n",
      "            test_case.assertEqual(input._version, input_version)\n",
      "\n",
      "            input_ip = deepcopy(input)\n",
      "            input_ip_clone = input_ip.clone()\n",
      "            with freeze_rng_state():\n",
      "                output_ip = module_ip(input_ip_clone)\n",
      "            test_case.assertNotEqual(input_ip_clone._version, input_version)\n",
      "            test_case.assertEqual(output, output_ip)\n",
      "            grad = output.data.clone().normal_()\n",
      "            input.grad.data.zero_()\n",
      "            output.backward(grad)\n",
      "            output_ip.backward(grad)\n",
      "            test_case.assertEqual(input.grad, input_ip.grad)\n",
      "\n",
      "        if type(input.data) == torch.LongTensor and TEST_CUDA:\n",
      "            input = input.cuda()\n",
      "            module.float().cuda()\n",
      "            module(input)\n",
      "            for p in module.parameters():\n",
      "                test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                test_case.assertEqual(p.get_device(), 0)\n",
      "\n",
      "            if torch.cuda.device_count() > 1:\n",
      "                input = input.cuda(1)\n",
      "                module.cuda(1)\n",
      "                with torch.cuda.device(1):\n",
      "                    module(input)\n",
      "                for p in module.parameters():\n",
      "                    test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                    test_case.assertEqual(p.get_device(), 1)\n",
      "        else:\n",
      "            # to float\n",
      "            if type(input.data) != torch.LongTensor:\n",
      "                input = input.float()\n",
      "            module.float()\n",
      "            module(input)\n",
      "            for p in module.parameters():\n",
      "                test_case.assertEqual(type(p.data), torch.FloatTensor)\n",
      "\n",
      "            # and back to double\n",
      "            if type(input.data) != torch.LongTensor:\n",
      "                input = input.double()\n",
      "            module.double()\n",
      "            module(input)\n",
      "            for p in module.parameters():\n",
      "                test_case.assertEqual(type(p.data), torch.DoubleTensor)\n",
      "\n",
      "            # TODO: Hardshrink is lacking a CUDA implementation\n",
      "            if TEST_CUDA and type(module) != nn.Hardshrink:\n",
      "                # to GPU0\n",
      "                input = input.float().cuda()\n",
      "                module.float().cuda()\n",
      "                module(input)\n",
      "                for p in module.parameters():\n",
      "                    test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                    test_case.assertEqual(p.get_device(), 0)\n",
      "\n",
      "                # to CPU\n",
      "                input = input.cpu()\n",
      "                module.cpu()\n",
      "                module(input)\n",
      "                for p in module.parameters():\n",
      "                    test_case.assertEqual(type(p.data), torch.FloatTensor)\n",
      "\n",
      "                # back to GPU0\n",
      "                input = input.cuda()\n",
      "                module.cuda()\n",
      "                module(input)\n",
      "                for p in module.parameters():\n",
      "                    test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                    test_case.assertEqual(p.get_device(), 0)\n",
      "\n",
      "                if self.cudnn:\n",
      "                    torch.backends.cudnn.enabled = False\n",
      "                    try:\n",
      "                        module(input)\n",
      "                        for p in module.parameters():\n",
      "                            test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                            test_case.assertEqual(p.get_device(), 0)\n",
      "                    finally:\n",
      "                        torch.backends.cudnn.enabled = True\n",
      "\n",
      "                if torch.cuda.device_count() >= 2:\n",
      "                    # to GPU1\n",
      "                    input = input.cuda(1)\n",
      "                    module.cuda(1)\n",
      "                    with torch.cuda.device(1):\n",
      "                        module(input)\n",
      "                    for p in module.parameters():\n",
      "                        test_case.assertEqual(type(p.data), torch.cuda.FloatTensor)\n",
      "                        test_case.assertEqual(p.get_device(), 1)\n",
      "\n",
      "\n",
      "class NewCriterionTest(InputVariableMixin, CriterionTest):\n",
      "    # TODO: check that criterions don't ignore grad_output\n",
      "\n",
      "    def _get_target(self, target):\n",
      "        return Variable(target, requires_grad=False)\n",
      "\n",
      "\n",
      "class TestNN(NNTestCase):\n",
      "    def _forward(self, module, input):\n",
      "        with freeze_rng_state():\n",
      "            return module(input)\n",
      "\n",
      "    def _backward(self, module, input, output, grad_output):\n",
      "        output.backward(grad_output, retain_graph=True)\n",
      "        if input.grad is None:\n",
      "            return None\n",
      "        return input.grad.data\n",
      "\n",
      "    def _forward_criterion(self, criterion, input, target):\n",
      "        if isinstance(input, tuple):\n",
      "            args = input + (target,)\n",
      "            output = criterion(*args)\n",
      "        else:\n",
      "            output = criterion(input, target)\n",
      "        return output.data[0]\n",
      "\n",
      "    def _backward_criterion(self, criterion, input, target):\n",
      "        input_tuple = input if isinstance(input, tuple) else (input,)\n",
      "        for i in input_tuple:\n",
      "            if i.grad is not None:\n",
      "                i.grad.data.zero_()\n",
      "        args = input_tuple + (target,)\n",
      "        criterion(*args).backward()\n",
      "        if isinstance(input, tuple):\n",
      "            return tuple(map(lambda i: i.grad.data, input))\n",
      "        else:\n",
      "            return input.grad.data\n",
      "\n",
      "    def _zero_grad_parameters(self, module):\n",
      "        if hasattr(module, 'weight') and module.weight is not None:\n",
      "            if module.weight.grad is not None:\n",
      "                module.weight.grad.data.zero_()\n",
      "                module.weight.grad.detach_()\n",
      "        if hasattr(module, 'bias') and module.bias is not None:\n",
      "            if module.bias.grad is not None:\n",
      "                module.bias.grad.data.zero_()\n",
      "                module.bias.grad.detach_()\n",
      "\n",
      "    def _get_parameters(self, module):\n",
      "        params = []\n",
      "        d_params = []\n",
      "        for p in module.parameters():\n",
      "            if p.grad is None:\n",
      "                p._grad = Variable(p.data.clone().zero_(), volatile=True)\n",
      "            params.append(p.data)\n",
      "            d_params.append(p.grad.data)\n",
      "        return params, d_params\n",
      "\n",
      "    def _assertGradAndGradgradChecks(self, apply_fn, inputs):\n",
      "        self.assertTrue(gradcheck(apply_fn, inputs))\n",
      "        dummy_out = apply_fn(*inputs)\n",
      "        if isinstance(dummy_out, tuple):\n",
      "            grad_y = tuple(Variable(torch.randn(x.size()), requires_grad=x.requires_grad)\n",
      "                           for x in dummy_out if isinstance(x, Variable))\n",
      "        else:\n",
      "            grad_y = (Variable(torch.randn(dummy_out.size()), requires_grad=dummy_out.requires_grad),)\n",
      "\n",
      "        self.assertTrue(gradgradcheck(apply_fn, inputs, grad_y,))\n",
      "\n",
      "    def test_module_backcompat(self):\n",
      "        from torch.serialization import SourceChangeWarning\n",
      "        path = download_file('https://download.pytorch.org/test_data/linear.pt')\n",
      "        with warnings.catch_warnings():\n",
      "            warnings.simplefilter('ignore', SourceChangeWarning)\n",
      "            m = torch.load(path)\n",
      "        input = Variable(torch.randn(2, 3).float())\n",
      "        self.assertEqual(m(input).size(), (2, 5))\n",
      "\n",
      "    def test_hooks(self):\n",
      "        module = nn.Sigmoid()\n",
      "        input = Variable(torch.ones(5, 5), requires_grad=True)\n",
      "\n",
      "        counter = {\n",
      "            'forwards': 0,\n",
      "            'backwards': 0\n",
      "        }\n",
      "\n",
      "        def fw_hook(inc, h_module, input, output):\n",
      "            self.assertIsInstance(input, tuple)\n",
      "            self.assertIsInstance(output, Variable)\n",
      "            self.assertTrue(h_module is module)\n",
      "            self.assertEqual(input[0].data, torch.ones(5, 5))\n",
      "            self.assertEqual(output.data, torch.Tensor(5, 5).fill_(1 / (1 + 1 / math.e)))\n",
      "            counter['forwards'] += inc\n",
      "\n",
      "        def bw_hook(inc, h_module, grad_input, grad_output):\n",
      "            self.assertIsInstance(grad_input, tuple)\n",
      "            self.assertIsInstance(grad_output, tuple)\n",
      "            self.assertTrue(h_module is module)\n",
      "            self.assertEqual(grad_output[0].data, torch.ones(5, 5) * 2)\n",
      "            counter['backwards'] += inc\n",
      "\n",
      "        test_fwd = module.register_forward_hook(lambda *args: fw_hook(1, *args))\n",
      "\n",
      "        module(input)\n",
      "        module(input)\n",
      "        self.assertEqual(counter['forwards'], 2)\n",
      "        self.assertEqual(counter['backwards'], 0)\n",
      "\n",
      "        test_bwd = module.register_backward_hook(\n",
      "            lambda *args: bw_hook(1, *args))\n",
      "\n",
      "        output = module(input)\n",
      "        self.assertEqual(counter['forwards'], 3)\n",
      "        self.assertEqual(counter['backwards'], 0)\n",
      "\n",
      "        output.backward(torch.ones(5, 5) * 2, retain_graph=True)\n",
      "        self.assertEqual(counter['forwards'], 3)\n",
      "        self.assertEqual(counter['backwards'], 1)\n",
      "\n",
      "        output.backward(torch.ones(5, 5) * 2, retain_graph=True)\n",
      "        self.assertEqual(counter['forwards'], 3)\n",
      "        self.assertEqual(counter['backwards'], 2)\n",
      "\n",
      "        test2_fwd = module.register_forward_hook(lambda *args: fw_hook(2, *args))\n",
      "\n",
      "        output = module(input)\n",
      "        self.assertEqual(counter['forwards'], 6)\n",
      "        self.assertEqual(counter['backwards'], 2)\n",
      "\n",
      "        test2_bwd = module.register_backward_hook(lambda *args: bw_hook(2, *args))\n",
      "\n",
      "        module(input).backward(torch.ones(5, 5) * 2)\n",
      "        self.assertEqual(counter['forwards'], 9)\n",
      "        self.assertEqual(counter['backwards'], 5)\n",
      "\n",
      "        test2_bwd.remove()\n",
      "\n",
      "        module(input).backward(torch.ones(5, 5) * 2)\n",
      "        self.assertEqual(counter['forwards'], 12)\n",
      "        self.assertEqual(counter['backwards'], 6)\n",
      "\n",
      "        test2_fwd.remove()\n",
      "\n",
      "        module(input).backward(torch.ones(5, 5) * 2)\n",
      "        self.assertEqual(counter['forwards'], 13)\n",
      "        self.assertEqual(counter['backwards'], 7)\n",
      "\n",
      "        test_fwd.remove()\n",
      "        test_bwd.remove()\n",
      "\n",
      "    def test_hook_cpp(self):\n",
      "        counter = [0]\n",
      "        bn = nn.BatchNorm1d(5)\n",
      "\n",
      "        def hook(module, grad_inputs, grad_outputs):\n",
      "            counter[0] += 1\n",
      "            self.assertEqual(len(grad_inputs), 3)\n",
      "            self.assertEqual(len(grad_outputs), 1)\n",
      "            self.assertEqual(module, bn)\n",
      "\n",
      "        bn.register_backward_hook(hook)\n",
      "        output = bn(Variable(torch.randn(5, 5), requires_grad=True))\n",
      "        output.sum().backward()\n",
      "\n",
      "    def test_hook_fail(self):\n",
      "        module = nn.Sigmoid()\n",
      "        input = Variable(torch.randn(5, 5), requires_grad=True)\n",
      "\n",
      "        def fw_fail1(self, input, output):\n",
      "            return output\n",
      "\n",
      "        def fw_fail2(self, input, output):\n",
      "            return input\n",
      "\n",
      "        def bw_fail1(self, grad_input, grad_output):\n",
      "            return grad_input[:-1]\n",
      "\n",
      "        def bw_fail2(self, grad_input, grad_output):\n",
      "            return grad_input + (torch.randn(2, 2),)\n",
      "\n",
      "        with module.register_forward_hook(fw_fail1):\n",
      "            with self.assertRaises(RuntimeError) as err:\n",
      "                module(input)\n",
      "            self.assertIn(\"fw_fail\", err.exception.args[0])\n",
      "            self.assertIn(\"didn't return None\", err.exception.args[0])\n",
      "\n",
      "        with module.register_forward_hook(fw_fail2):\n",
      "            with self.assertRaises(RuntimeError) as err:\n",
      "                module(input)\n",
      "            self.assertIn(\"fw_fail2\", err.exception.args[0])\n",
      "            self.assertIn(\"didn't return None\", err.exception.args[0])\n",
      "\n",
      "        with module.register_backward_hook(bw_fail1):\n",
      "            with self.assertRaises(RuntimeError) as err:\n",
      "                module(input).sum().backward()\n",
      "            self.assertIn(\"bw_fail\", err.exception.args[0])\n",
      "            self.assertIn(\"got 0, but expected 1\", err.exception.args[0])\n",
      "\n",
      "        with module.register_backward_hook(bw_fail2):\n",
      "            with self.assertRaises(RuntimeError) as err:\n",
      "                module(input).sum().backward()\n",
      "            self.assertIn(\"bw_fail2\", err.exception.args[0])\n",
      "            self.assertIn(\"got 2, but expected 1\", err.exception.args[0])\n",
      "\n",
      "    def test_hook_writeable(self):\n",
      "        module = nn.Linear(5, 5)\n",
      "        input = Variable(torch.randn(5, 5), requires_grad=True)\n",
      "\n",
      "        def bw_hook(module, grad_input, grad_output):\n",
      "            for grad in grad_input:\n",
      "                self.assertIsInstance(grad, Variable)\n",
      "            for grad in grad_output:\n",
      "                self.assertIsInstance(grad, Variable)\n",
      "            return tuple(gi * 2 for gi in grad_input)\n",
      "\n",
      "        module.register_backward_hook(bw_hook)\n",
      "        module(input).backward(torch.ones(5, 5))\n",
      "        expected_grad = torch.ones(5, 5).mm(module.weight.data) * 2\n",
      "        self.assertEqual(input.grad.data, expected_grad)\n",
      "\n",
      "    def test_zero_grad(self):\n",
      "        i = Variable(torch.randn(2, 5), requires_grad=True)\n",
      "        module = nn.Linear(5, 5)\n",
      "        for p in module.parameters():\n",
      "            p.requires_grad = False\n",
      "        module.zero_grad()\n",
      "\n",
      "        module.weight.requires_grad = True\n",
      "        module.zero_grad()\n",
      "        self.assertIsNone(module.weight.grad)  # uninitialized grad\n",
      "\n",
      "        module(i).sum().backward()\n",
      "        self.assertIsNotNone(module.weight.grad)\n",
      "        self.assertGreater(module.weight.grad.data.abs().sum(), 0)\n",
      "        module.zero_grad()\n",
      "        self.assertEqual(module.weight.grad.data, module.weight.data.clone().zero_())\n",
      "\n",
      "        module.bias.requires_grad = True\n",
      "        module.zero_grad()\n",
      "        self.assertIsNotNone(module.weight.grad)\n",
      "        self.assertIsNone(module.bias.grad)\n",
      "        module(i).sum().backward()\n",
      "        self.assertIsNotNone(module.weight.grad)\n",
      "        self.assertIsNotNone(module.bias.grad)\n",
      "        self.assertGreater(module.weight.grad.data.abs().sum(), 0)\n",
      "        self.assertGreater(module.bias.grad.data.abs().sum(), 0)\n",
      "        module.zero_grad()\n",
      "        self.assertEqual(module.weight.grad.data, module.weight.data.clone().zero_())\n",
      "        self.assertEqual(module.bias.grad.data, module.bias.data.clone().zero_())\n",
      "\n",
      "        # non-volatile grad should be zeroed out of place\n",
      "        initial = module.weight.grad = Variable(torch.ones(5, 5))\n",
      "        module.zero_grad()\n",
      "        self.assertIsNot(module.weight.grad, initial)\n",
      "        self.assertEqual(module.weight.grad.data, torch.zeros(5, 5))\n",
      "\n",
      "    def test_volatile(self):\n",
      "        module = nn.Conv2d(2, 5, kernel_size=3, padding=1)\n",
      "        input = torch.randn(1, 2, 10, 10)\n",
      "        x = Variable(input)\n",
      "        y = Variable(input.clone(), volatile=True)\n",
      "\n",
      "        output = module(x)\n",
      "        self.assertFalse(output.volatile)\n",
      "        self.assertTrue(output.requires_grad)\n",
      "        output.backward(torch.ones(1, 5, 10, 10))\n",
      "\n",
      "        vol_output = module(y)\n",
      "        self.assertTrue(vol_output.volatile)\n",
      "        self.assertFalse(vol_output.requires_grad)\n",
      "        self.assertRaises(RuntimeError, lambda: vol_output.backward(torch.ones(1, 5, 10, 10)))\n",
      "\n",
      "    def _test_dropout(self, cls, input):\n",
      "        p = 0.2\n",
      "        input.fill_(1 - p)\n",
      "\n",
      "        module = cls(p)\n",
      "        input_var = Variable(input, requires_grad=True)\n",
      "        output = module(input_var)\n",
      "        self.assertLess(abs(output.data.mean() - (1 - p)), 0.05)\n",
      "        output.backward(input)\n",
      "        self.assertLess(abs(input_var.grad.data.mean() - (1 - p)), 0.05)\n",
      "\n",
      "        module = cls(p, True)\n",
      "        input_var = Variable(input.clone(), requires_grad=True)\n",
      "        output = module(input_var + 0)\n",
      "        self.assertLess(abs(output.data.mean() - (1 - p)), 0.05)\n",
      "        output.backward(input)\n",
      "        self.assertLess(abs(input_var.grad.data.mean() - (1 - p)), 0.05)\n",
      "\n",
      "        # Check that these don't raise errors\n",
      "        module.__repr__()\n",
      "        str(module)\n",
      "\n",
      "    def test_parameters(self):\n",
      "        def num_params(module):\n",
      "            return len(list(module.parameters()))\n",
      "\n",
      "        class Net(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Net, self).__init__()\n",
      "                self.l1 = l\n",
      "                self.l2 = l\n",
      "                self.param = Parameter(torch.Tensor(3, 5))\n",
      "\n",
      "        l = nn.Linear(10, 20)\n",
      "        n = Net()\n",
      "        s = nn.Sequential(n, n, n, n)\n",
      "        self.assertEqual(num_params(l), 2)\n",
      "        self.assertEqual(num_params(n), 3)\n",
      "        self.assertEqual(num_params(s), 3)\n",
      "\n",
      "    def test_named_parameters(self):\n",
      "        def num_params(module):\n",
      "            return len(dict(module.named_parameters()))\n",
      "\n",
      "        class Net(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Net, self).__init__()\n",
      "                self.l1 = l\n",
      "                self.l2 = l\n",
      "                self.param = Parameter(torch.Tensor(3, 5))\n",
      "\n",
      "        l = nn.Linear(10, 20)\n",
      "        n = Net()\n",
      "        s = nn.Sequential(n, n, n, n)\n",
      "\n",
      "        for name in dict(l.named_parameters()).keys():\n",
      "            self.assertTrue(name in ['bias', 'weight'])\n",
      "\n",
      "        for name in dict(n.named_parameters()).keys():\n",
      "            self.assertTrue(name in ['l1.bias', 'l1.weight', 'param'])\n",
      "\n",
      "        for name in dict(s.named_parameters()).keys():\n",
      "            self.assertTrue(name in ['0.l1.bias', '0.l1.weight', '0.param'])\n",
      "\n",
      "        self.assertEqual(num_params(l), 2)\n",
      "        self.assertEqual(num_params(n), 3)\n",
      "        self.assertEqual(num_params(s), 3)\n",
      "\n",
      "    def test_children(self):\n",
      "        l1 = nn.Linear(2, 2)\n",
      "        l2 = nn.Linear(2, 2)\n",
      "        l3 = nn.Linear(2, 2)\n",
      "        l4 = nn.Linear(2, 2)\n",
      "        subnet = nn.Sequential(l3, l4)\n",
      "        s = nn.Sequential(l1, l2, l1, l2, subnet)\n",
      "        self.assertEqual(list(s.children()), [l1, l2, subnet])\n",
      "\n",
      "    def test_dir(self):\n",
      "        linear = nn.Linear(2, 2)\n",
      "        linear._test_submodule = nn.Linear(2, 2)\n",
      "        linear._test_parameter = Parameter(torch.Tensor(2, 2))\n",
      "        linear.register_buffer('_test_buffer', torch.Tensor(2, 2))\n",
      "        keys = linear.__dir__()\n",
      "        self.assertIn('_test_submodule', keys)\n",
      "        self.assertIn('_test_parameter', keys)\n",
      "        self.assertIn('_test_buffer', keys)\n",
      "\n",
      "        for key in keys:\n",
      "            self.assertTrue(hasattr(linear, key))\n",
      "\n",
      "    def test_named_children(self):\n",
      "        l1 = nn.Linear(2, 2)\n",
      "        l2 = nn.Linear(2, 2)\n",
      "        l3 = nn.Linear(2, 2)\n",
      "        l4 = nn.Linear(2, 2)\n",
      "        subnet = nn.Sequential(l3, l4)\n",
      "        s = nn.Sequential()\n",
      "        s.add_module('layer1', l1)\n",
      "        s.add_module('layer2', l2)\n",
      "        s.add_module('layer3', l1)\n",
      "        s.add_module('layer4', l2)\n",
      "        s.add_module('subnet', subnet)\n",
      "        self.assertEqual(list(s.named_children()), [('layer1', l1), ('layer2', l2), ('subnet', subnet)])\n",
      "\n",
      "    def test_modules(self):\n",
      "        class Net(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Net, self).__init__()\n",
      "                self.l1 = l\n",
      "                self.l2 = l\n",
      "                self.param = Variable(torch.Tensor(3, 5))\n",
      "\n",
      "        l = nn.Linear(10, 20)\n",
      "        n = Net()\n",
      "        s = nn.Sequential(n, n, n, n)\n",
      "        self.assertEqual(list(s.modules()), [s, n, l])\n",
      "\n",
      "    def test_named_modules(self):\n",
      "        class Net(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Net, self).__init__()\n",
      "                self.l1 = l\n",
      "                self.l2 = l\n",
      "                self.param = Variable(torch.Tensor(3, 5))\n",
      "                self.block = block\n",
      "        l = nn.Linear(10, 20)\n",
      "        l1 = nn.Linear(10, 20)\n",
      "        l2 = nn.Linear(10, 20)\n",
      "        block = nn.Sequential()\n",
      "        block.add_module('linear1', l1)\n",
      "        block.add_module('linear2', l2)\n",
      "        n = Net()\n",
      "        s = nn.Sequential(n, n, n, n)\n",
      "        self.assertEqual(list(s.named_modules()), [('', s), ('0', n), ('0.l1', l),\n",
      "                                                   ('0.block', block), ('0.block.linear1', l1),\n",
      "                                                   ('0.block.linear2', l2)])\n",
      "\n",
      "    def test_Sequential_getitem(self):\n",
      "        l1 = nn.Linear(10, 20)\n",
      "        l2 = nn.Linear(20, 30)\n",
      "        l3 = nn.Linear(30, 40)\n",
      "        l4 = nn.Linear(40, 50)\n",
      "        n = nn.Sequential(l1, l2, l3, l4)\n",
      "        self.assertEqual(n[0], l1)\n",
      "        self.assertEqual(n[1], l2)\n",
      "        self.assertEqual(n[2], l3)\n",
      "        self.assertEqual(n[3], l4)\n",
      "\n",
      "    def test_ListModule(self):\n",
      "        modules = [nn.ReLU(), nn.Linear(5, 5)]\n",
      "        module_list = nn.ModuleList(modules)\n",
      "\n",
      "        def check():\n",
      "            self.assertEqual(len(module_list), len(modules))\n",
      "            for m1, m2 in zip(modules, module_list):\n",
      "                self.assertIs(m1, m2)\n",
      "            for m1, m2 in zip(modules, module_list.children()):\n",
      "                self.assertIs(m1, m2)\n",
      "            for i in range(len(modules)):\n",
      "                self.assertIs(module_list[i], modules[i])\n",
      "\n",
      "        check()\n",
      "        modules += [nn.Conv2d(3, 4, 3)]\n",
      "        module_list += [modules[-1]]\n",
      "        check()\n",
      "        modules.append(nn.Tanh())\n",
      "        module_list.append(modules[-1])\n",
      "        check()\n",
      "        next_modules = [nn.Linear(5, 5), nn.Sigmoid()]\n",
      "        modules.extend(next_modules)\n",
      "        module_list.extend(next_modules)\n",
      "        check()\n",
      "        modules[2] = nn.Conv2d(5, 3, 2)\n",
      "        module_list[2] = modules[2]\n",
      "        check()\n",
      "\n",
      "        with self.assertRaises(TypeError):\n",
      "            module_list += nn.ReLU()\n",
      "        with self.assertRaises(TypeError):\n",
      "            module_list.extend(nn.ReLU())\n",
      "\n",
      "    def test_ParameterList(self):\n",
      "        def make_param():\n",
      "            return Parameter(torch.randn(10, 10))\n",
      "        parameters = [make_param(), make_param()]\n",
      "        param_list = nn.ParameterList(parameters)\n",
      "\n",
      "        def check():\n",
      "            self.assertEqual(len(parameters), len(param_list))\n",
      "            for p1, p2 in zip(parameters, param_list):\n",
      "                self.assertIs(p1, p2)\n",
      "            for p1, p2 in zip(parameters, param_list.parameters()):\n",
      "                self.assertIs(p1, p2)\n",
      "            for i in range(len(parameters)):\n",
      "                self.assertIs(parameters[i], param_list[i])\n",
      "\n",
      "        check()\n",
      "        parameters += [make_param()]\n",
      "        param_list += [parameters[-1]]\n",
      "        check()\n",
      "        parameters.append(make_param())\n",
      "        param_list.append(parameters[-1])\n",
      "        check()\n",
      "        next_params = [make_param(), make_param()]\n",
      "        parameters.extend(next_params)\n",
      "        param_list.extend(next_params)\n",
      "        check()\n",
      "        parameters[2] = make_param()\n",
      "        param_list[2] = parameters[2]\n",
      "        check()\n",
      "\n",
      "        with self.assertRaises(TypeError):\n",
      "            param_list += make_param()\n",
      "        with self.assertRaises(TypeError):\n",
      "            param_list.extend(make_param())\n",
      "\n",
      "    def test_add_module(self):\n",
      "        l = nn.Linear(10, 20)\n",
      "        net = nn.Module()\n",
      "        net.l = l\n",
      "        net.l2 = l\n",
      "        net.add_module('empty', None)\n",
      "        self.assertEqual(net.l, l)\n",
      "        self.assertEqual(net.l2, l)\n",
      "        self.assertEqual(net.empty, None)\n",
      "        net.add_module('l3', l)\n",
      "        self.assertEqual(net.l3, l)\n",
      "        self.assertRaises(KeyError, lambda: net.add_module('l', l))\n",
      "        self.assertRaises(TypeError, lambda: net.add_module('x', 'non-module'))\n",
      "\n",
      "    def test_type(self):\n",
      "        l = nn.Linear(10, 20)\n",
      "        net = nn.Module()\n",
      "        net.l = l\n",
      "        net.l2 = l\n",
      "        net.add_module('empty', None)\n",
      "        net.float()\n",
      "        self.assertIsInstance(l.weight.data, torch.FloatTensor)\n",
      "        self.assertIsInstance(l.bias.data, torch.FloatTensor)\n",
      "        net.double()\n",
      "        self.assertIsInstance(l.weight.data, torch.DoubleTensor)\n",
      "        self.assertIsInstance(l.bias.data, torch.DoubleTensor)\n",
      "        net.type(torch.FloatTensor)\n",
      "        self.assertIsInstance(l.weight.data, torch.FloatTensor)\n",
      "        self.assertIsInstance(l.bias.data, torch.FloatTensor)\n",
      "        net.type(torch.DoubleTensor)\n",
      "        self.assertIsInstance(l.weight.data, torch.DoubleTensor)\n",
      "        self.assertIsInstance(l.bias.data, torch.DoubleTensor)\n",
      "        if TEST_CUDA:\n",
      "            net.type(torch.cuda.FloatTensor)\n",
      "            self.assertIsInstance(l.weight.data, torch.cuda.FloatTensor)\n",
      "            self.assertIsInstance(l.bias.data, torch.cuda.FloatTensor)\n",
      "\n",
      "    def test_non_leaf_parameters(self):\n",
      "        l1 = nn.Linear(10, 10)\n",
      "        l2 = nn.Linear(10, 10)\n",
      "\n",
      "        def assign_weight():\n",
      "            l2.weight = l1.weight + 2\n",
      "\n",
      "        self.assertRaises(TypeError, assign_weight)\n",
      "        # This should work though\n",
      "        l2.weight = Parameter(torch.randn(10, 10))\n",
      "\n",
      "    def test_clip_grad_norm(self):\n",
      "        l = nn.Linear(10, 10)\n",
      "        max_norm = 2\n",
      "\n",
      "        def compute_norm(norm_type):\n",
      "            norm_type = float(norm_type)\n",
      "            if norm_type != float('inf'):\n",
      "                total_norm = 0\n",
      "                for p in l.parameters():\n",
      "                    total_norm += p.grad.data.abs().pow(norm_type).sum()\n",
      "                return pow(total_norm, 1. / norm_type)\n",
      "            else:\n",
      "                return max(p.grad.data.abs().max() for p in l.parameters())\n",
      "\n",
      "        def compare_scaling(grads):\n",
      "            p_scale = [p.grad.data.div(g).view(-1) for p, g in zip(l.parameters(), grads)]\n",
      "            scale = torch.cat(p_scale)\n",
      "            self.assertEqual(scale.std(), 0)\n",
      "            return scale[0]\n",
      "\n",
      "        grads = torch.arange(1, 101).view(10, 10), torch.ones(10).div(1000)\n",
      "        for norm_type in [0.5, 1.5, 2, 4, 'inf']:\n",
      "            for p, g in zip(l.parameters(), grads):\n",
      "                p._grad = Variable(g.clone().view_as(p.data))\n",
      "            norm_before = compute_norm(norm_type)\n",
      "            norm = clip_grad_norm(l.parameters(), max_norm, norm_type=norm_type)\n",
      "            norm_after = compute_norm(norm_type)\n",
      "            self.assertEqual(norm, norm_before)\n",
      "            self.assertEqual(norm_after, max_norm)\n",
      "            self.assertLessEqual(norm_after, norm_before)\n",
      "            compare_scaling(grads)\n",
      "\n",
      "        # Small gradients should be left unchanged\n",
      "        grads = torch.rand(10, 10).div(10000), torch.ones(10).div(500)\n",
      "        for norm_type in [0.5, 1.5, 2, 4, 'inf']:\n",
      "            for p, g in zip(l.parameters(), grads):\n",
      "                p.grad.data.copy_(g)\n",
      "            norm_before = compute_norm(norm_type)\n",
      "            norm = clip_grad_norm(l.parameters(), max_norm, norm_type=norm_type)\n",
      "            norm_after = compute_norm(norm_type)\n",
      "            self.assertEqual(norm, norm_before)\n",
      "            self.assertEqual(norm_before, norm_after)\n",
      "            self.assertLessEqual(norm_after, max_norm)\n",
      "            scale = compare_scaling(grads)\n",
      "            self.assertEqual(scale, 1)\n",
      "\n",
      "    def test_weight_norm(self):\n",
      "        input = Variable(torch.randn(3, 5))\n",
      "        m = nn.Linear(5, 7)\n",
      "        expected_output = m(input)\n",
      "\n",
      "        # add weight normalization\n",
      "        m = torch.nn.utils.weight_norm(m)\n",
      "        self.assertEqual(m.weight_v.size(), m.weight.size())\n",
      "        self.assertEqual(m.weight_g.size(), (7, 1))\n",
      "        self.assertEqual(m(input), expected_output)\n",
      "\n",
      "        # remove weight norm\n",
      "        m = torch.nn.utils.remove_weight_norm(m)\n",
      "        self.assertFalse(hasattr(m, 'weight_g'))\n",
      "        self.assertFalse(hasattr(m, 'weight_v'))\n",
      "        self.assertEqual(m(input), expected_output)\n",
      "\n",
      "        # test with dim=1\n",
      "        m = torch.nn.utils.weight_norm(m, dim=1)\n",
      "        self.assertEqual(m.weight_v.size(), m.weight.size())\n",
      "        self.assertEqual(m.weight_g.size(), (1, 5))\n",
      "        self.assertEqual(m(input), expected_output)\n",
      "\n",
      "    def test_weight_norm_pickle(self):\n",
      "        m = torch.nn.utils.weight_norm(nn.Linear(5, 7))\n",
      "        m = pickle.loads(pickle.dumps(m))\n",
      "        self.assertIsInstance(m, nn.Linear)\n",
      "\n",
      "    def test_embedding_padding_idx(self):\n",
      "        embedding = nn.Embedding(10, 20, padding_idx=0)\n",
      "        input = Variable(torch.LongTensor([[0, 2, 4, 5], [4, 3, 0, 9]]))\n",
      "        output = embedding(input)\n",
      "        self.assertEqual(output[0][0].sum().data[0], 0)\n",
      "        self.assertEqual(output[1][2].sum().data[0], 0)\n",
      "\n",
      "    def test_embedding_functional(self):\n",
      "        a = Variable(torch.LongTensor([\n",
      "            [1, 3, 2],\n",
      "            [0, 2, 1]\n",
      "        ]))\n",
      "        embeddings = Variable(torch.rand(4, 3), requires_grad=True)\n",
      "\n",
      "        embed_old = torch.nn.Embedding(4, 3)\n",
      "        embed_old.weight.data = embeddings.data\n",
      "        res_old = embed_old(a)\n",
      "\n",
      "        res_F = F.embedding(a, embeddings)\n",
      "        self.assertEqual(res_old, res_F)\n",
      "\n",
      "    def _test_EmbeddingBag(self, cuda, mode):\n",
      "        # check a known test example\n",
      "        es = nn.EmbeddingBag(5, 2, mode=mode)\n",
      "        es.weight.data.copy_(torch.arange(1, 11).resize_as_(es.weight.data))\n",
      "        input = Variable(torch.LongTensor([3, 1, 1, 1, 4]))\n",
      "        offsets = Variable(torch.LongTensor([0, 2]))\n",
      "        grad_output = torch.arange(1, 5).view(2, 2).type(torch.Tensor)\n",
      "\n",
      "        if mode == 'sum':\n",
      "            expected_output = torch.Tensor(\n",
      "                [[10, 12],\n",
      "                 [15, 18]])\n",
      "            expected_grad_weight = torch.Tensor(\n",
      "                [[0, 0],\n",
      "                 [7, 10],\n",
      "                 [0, 0],\n",
      "                 [1, 2],\n",
      "                 [3, 4]])\n",
      "        else:\n",
      "            expected_output = torch.Tensor(\n",
      "                [[10. / 2, 12. / 2],\n",
      "                 [15. / 3, 18. / 3]])\n",
      "            expected_grad_weight = torch.Tensor(\n",
      "                [[0., 0.],\n",
      "                 [1. / 2 + 3. / 3 + 3. / 3, 2. / 2 + 4. / 3 + 4. / 3],\n",
      "                 [0., 0.],\n",
      "                 [1. / 2, 2. / 2],\n",
      "                 [3. / 3, 4. / 3]])\n",
      "\n",
      "        if cuda:\n",
      "            es = es.cuda()\n",
      "            input = input.cuda()\n",
      "            offsets = offsets.cuda()\n",
      "            grad_output = grad_output.cuda()\n",
      "            expected_output = expected_output.cuda()\n",
      "            expected_grad_weight = expected_grad_weight.cuda()\n",
      "\n",
      "        output = es(input, offsets)\n",
      "        output.backward(grad_output)\n",
      "\n",
      "        self.assertEqual(output.data, expected_output)\n",
      "        self.assertEqual(es.weight.grad.data, expected_grad_weight)\n",
      "\n",
      "        # now compare EmbeddingBag vs Embedding + Sum/Mean, for constant bag length\n",
      "        def _test_vs_Embedding(N, D, B, L):\n",
      "            es = nn.EmbeddingBag(N, D, mode=mode)\n",
      "            e = nn.Embedding(N, D)\n",
      "            e.weight.data.copy_(es.weight.data)\n",
      "            input = Variable(torch.rand(B, L).mul(N).long())\n",
      "            offsets = Variable(torch.arange(0, B).mul(L).long())\n",
      "            grad_output = torch.rand(B, D).type(torch.Tensor)\n",
      "\n",
      "            if cuda:\n",
      "                es = es.cuda()\n",
      "                e = e.cuda()\n",
      "                input = input.cuda()\n",
      "                offsets = offsets.cuda()\n",
      "                grad_output = grad_output.cuda()\n",
      "\n",
      "            output = es(input.view(-1), offsets)\n",
      "            if mode == 'sum':\n",
      "                ref_output = e(input).sum(1)\n",
      "            else:\n",
      "                ref_output = e(input).mean(1)\n",
      "\n",
      "            self.assertEqual(output, ref_output)\n",
      "\n",
      "            output.backward(grad_output)\n",
      "            ref_output.backward(grad_output)\n",
      "            self.assertEqual(es.weight.grad, e.weight.grad)\n",
      "\n",
      "        N, D, B, L = random.randint(1, 100), random.randint(1, 100), random.randint(1, 50), random.randint(1, 50)\n",
      "        _test_vs_Embedding(N, D, B, L)\n",
      "        for p in itertools.product([1, 2], repeat=4):\n",
      "            _test_vs_Embedding(*p)\n",
      "\n",
      "        # check that giving illegal input combos raises error\n",
      "        es = nn.EmbeddingBag(10, 20, mode=mode)\n",
      "        input = Variable(torch.ones(3, 4))\n",
      "        offset = Variable(torch.arange(0, 3))\n",
      "        self.assertRaises(ValueError, lambda: es(input, offset))\n",
      "        self.assertRaises(ValueError, lambda: es(input.view(-1)))\n",
      "        offset[0] = 1\n",
      "        self.assertRaises(ValueError, lambda: es(input.view(-1), offset))\n",
      "        offset[0] = 0\n",
      "        offset[-1] = 100\n",
      "        self.assertRaises(ValueError, lambda: es(input.view(-1), offset))\n",
      "\n",
      "    def test_EmbeddingBag(self):\n",
      "        self._test_EmbeddingBag(False, 'sum')\n",
      "        self._test_EmbeddingBag(False, 'mean')\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_EmbeddingBag_cuda(self):\n",
      "        self._test_EmbeddingBag(True, 'sum')\n",
      "        self._test_EmbeddingBag(True, 'mean')\n",
      "\n",
      "    def test_Dropout(self):\n",
      "        input = torch.Tensor(1000)\n",
      "        self._test_dropout(nn.Dropout, input)\n",
      "\n",
      "    def test_Dropout2d(self):\n",
      "        b = random.randint(1, 5)\n",
      "        w = random.randint(1, 5)\n",
      "        h = random.randint(1, 5)\n",
      "        num_features = 1000\n",
      "        input = torch.Tensor(num_features, b, w, h)\n",
      "        self._test_dropout(nn.Dropout2d, input)\n",
      "\n",
      "    def test_Dropout3d(self):\n",
      "        b = random.randint(1, 5)\n",
      "        w = random.randint(1, 5)\n",
      "        h = random.randint(1, 5)\n",
      "        d = random.randint(1, 2)\n",
      "        num_features = 1000\n",
      "        input = torch.Tensor(num_features, b, d, w, h)\n",
      "        self._test_dropout(nn.Dropout3d, input)\n",
      "\n",
      "    def test_AlphaDropout(self):\n",
      "        # generate random tensor with zero mean and unit std\n",
      "        input = torch.randn(5000)\n",
      "\n",
      "        mean = input.mean()\n",
      "        std = input.std()\n",
      "\n",
      "        for p in [0.2, 0.5, 0.8]:\n",
      "            module = nn.AlphaDropout(p)\n",
      "            input_var = Variable(input, requires_grad=True)\n",
      "            output = module(input_var)\n",
      "            # output mean should be close to input mean\n",
      "            self.assertLess(abs(output.data.mean() - mean), 0.1)\n",
      "            # output std should be close to input std\n",
      "            self.assertLess(abs(output.data.std() - std), 0.1)\n",
      "            output.backward(input)\n",
      "\n",
      "    def _test_InstanceNorm(self, cls, input):\n",
      "        b, c = input.size(0), input.size(1)\n",
      "        input_var = Variable(input)\n",
      "\n",
      "        IN = cls(c, eps=0)\n",
      "\n",
      "        output = IN(input_var)\n",
      "        out_reshaped = output.transpose(1, 0).contiguous().view(c, -1)\n",
      "\n",
      "        mean = out_reshaped.mean(1)\n",
      "        var = out_reshaped.var(1, unbiased=False)\n",
      "\n",
      "        self.assertAlmostEqual(torch.abs(mean.data).mean(), 0, delta=1e-5)\n",
      "        self.assertAlmostEqual(torch.abs(var.data).mean(), 1, delta=1e-5)\n",
      "\n",
      "        # If momentum==1 running_mean/var should be\n",
      "        # equal to mean/var of the input\n",
      "        IN = cls(c, momentum=1, eps=0)\n",
      "\n",
      "        output = IN(input_var)\n",
      "\n",
      "        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\n",
      "        mean = input_reshaped.mean(1)\n",
      "\n",
      "        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, b, -1)\n",
      "        var = input_reshaped.var(2, unbiased=True)[:, :]\n",
      "\n",
      "        self.assertAlmostEqual(torch.abs(mean.data - IN.running_mean).mean(), 0, delta=1e-5)\n",
      "        self.assertAlmostEqual(torch.abs(var.data.mean(1) - IN.running_var).mean(), 0, delta=1e-5)\n",
      "\n",
      "    def test_InstanceNorm2d(self):\n",
      "        b = random.randint(3, 5)\n",
      "        c = random.randint(1, 5)\n",
      "        w = random.randint(2, 5)\n",
      "        h = random.randint(2, 5)\n",
      "\n",
      "        input = torch.Tensor(b, c, h, w).uniform_()\n",
      "        self._test_InstanceNorm(nn.InstanceNorm2d, input)\n",
      "\n",
      "    def test_InstanceNorm1d(self):\n",
      "        b = random.randint(3, 5)\n",
      "        c = random.randint(1, 5)\n",
      "        d = random.randint(2, 5)\n",
      "\n",
      "        input = torch.Tensor(b, c, d).uniform_()\n",
      "        self._test_InstanceNorm(nn.InstanceNorm1d, input)\n",
      "\n",
      "    def test_InstanceNorm3d(self):\n",
      "        b = random.randint(3, 5)\n",
      "        c = random.randint(1, 5)\n",
      "        w = random.randint(2, 5)\n",
      "        h = random.randint(2, 5)\n",
      "        d = random.randint(2, 5)\n",
      "\n",
      "        input = torch.Tensor(b, c, h, w, d).uniform_()\n",
      "        self._test_InstanceNorm(nn.InstanceNorm3d, input)\n",
      "\n",
      "    def test_pad(self):\n",
      "        inputs = Variable(torch.randn(1, 3, 4, 4), requires_grad=True)\n",
      "        self._assertGradAndGradgradChecks(lambda x: F.pad(x, (1, 1, 1, 1)), (inputs,))\n",
      "        self._assertGradAndGradgradChecks(lambda x: F.pad(x, (-1, 1, -2, 1)), (inputs,))\n",
      "        self._assertGradAndGradgradChecks(lambda x: F.pad(x, (-1, 1, -2, 1), value=2), (inputs,))\n",
      "        self.assertTrue(gradcheck(lambda x: F.pad(x, (-1, 1, -2, 1), mode='replicate'), (inputs,)))\n",
      "        self.assertTrue(gradcheck(lambda x: F.pad(x, (-1, 1, -2, 1), mode='reflect'), (inputs,)))\n",
      "\n",
      "        inputs = Variable(torch.randn(1, 2, 3, 4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.pad(x, (1, 1, 1, 1, 1, 1), mode='replicate'), (inputs,)))\n",
      "\n",
      "    def test_normalize(self):\n",
      "        inputs = Variable(torch.randn(1, 3, 4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.normalize(x, p=1, dim=-1), (inputs,)))\n",
      "        self.assertTrue(gradcheck(lambda x: F.normalize(x, p=2, dim=-2), (inputs,)))\n",
      "\n",
      "    def _test_maxpool_indices(self, num_dim, type=torch.FloatTensor):\n",
      "        def expected_indices(dim):\n",
      "            if dim == 1:\n",
      "                return torch.DoubleTensor([1, 3]).repeat(2, 2, 1)\n",
      "            if dim == 2:\n",
      "                return torch.DoubleTensor([[5, 7], [13, 15]]).repeat(2, 2, 1, 1)\n",
      "\n",
      "        def expected_grad(dim):\n",
      "            if dim == 1:\n",
      "                return torch.DoubleTensor([0, 1, 0, 1]).repeat(2, 2, 1)\n",
      "            grad = expected_grad(dim - 1)\n",
      "            zero = torch.zeros(grad.size())\n",
      "            return torch.stack((zero, grad, zero, grad), 2)\n",
      "\n",
      "        def expected_output(dim):\n",
      "            if dim == 1:\n",
      "                return torch.arange(2, 17, 2).view(2, 2, 2)\n",
      "            if dim == 2:\n",
      "                col = torch.arange(6, 63, 8)\n",
      "                return torch.stack([col, col + 2], 1).view(2, 2, 2, 2)\n",
      "\n",
      "        module_cls = getattr(nn, 'MaxPool{}d'.format(num_dim))\n",
      "        module = module_cls(2, return_indices=True).type(type)\n",
      "        numel = 4 ** (num_dim + 1)\n",
      "        input = torch.arange(1, numel + 1).view(2, 2, *repeat(4, num_dim)).type(type)\n",
      "        input_var = Variable(input, requires_grad=True)\n",
      "\n",
      "        # Check forward\n",
      "        output, indices = module(input_var)\n",
      "        if num_dim != 3:\n",
      "            expected_indices = expected_indices(num_dim)\n",
      "            expected_output = expected_output(num_dim)\n",
      "            self.assertEqual(indices.dim(), input.dim())\n",
      "            self.assertEqual(indices.data.squeeze(), expected_indices)\n",
      "            self.assertEqual(output.data.squeeze(), expected_output)\n",
      "        self.assertTrue(output.requires_grad)\n",
      "        self.assertFalse(indices.requires_grad)\n",
      "\n",
      "        # Make sure backward works\n",
      "        grad_output = torch.ones(output.size()).type(type)\n",
      "        output.backward(grad_output, retain_graph=True)\n",
      "        expected_grad = expected_grad(num_dim)\n",
      "        self.assertEqual(input_var.grad.data, expected_grad.view_as(input))\n",
      "\n",
      "        # Make sure backward after changing indices will result in an error\n",
      "        indices.add_(1)\n",
      "        self.assertRaises(RuntimeError, lambda: output.backward(grad_output))\n",
      "\n",
      "    def test_MaxPool1d_indices(self):\n",
      "        self._test_maxpool_indices(1)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_MaxPool1d_indices_cuda(self):\n",
      "        self._test_maxpool_indices(1, torch.cuda.FloatTensor)\n",
      "\n",
      "    def test_MaxPool2d_indices(self):\n",
      "        self._test_maxpool_indices(2)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_MaxPool2d_indices_cuda(self):\n",
      "        self._test_maxpool_indices(2, torch.cuda.FloatTensor)\n",
      "\n",
      "    def test_MaxPool3d_indices(self):\n",
      "        self._test_maxpool_indices(3)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_MaxPool3d_indices_cuda(self):\n",
      "        self._test_maxpool_indices(3, torch.cuda.FloatTensor)\n",
      "\n",
      "    def test_AdaptiveMaxPool1d_indices(self):\n",
      "        self._test_maxpool_indices(1)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_AdaptiveMaxPool1d_indices_cuda(self):\n",
      "        self._test_maxpool_indices(1, torch.cuda.FloatTensor)\n",
      "\n",
      "    def test_AdaptiveMaxPool2d_indices(self):\n",
      "        self._test_maxpool_indices(2)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_AdaptiveMaxPool2d_indices_cuda(self):\n",
      "        self._test_maxpool_indices(2, torch.cuda.FloatTensor)\n",
      "\n",
      "    def _test_scatter(self, tensor):\n",
      "        x = Variable(tensor, requires_grad=True)\n",
      "        result = dp.scatter(x, (0, 1))\n",
      "        self.assertEqual(len(result), 2)\n",
      "        self.assertEqual(result[0], x[:2])\n",
      "        self.assertEqual(result[0].get_device(), 0)\n",
      "        self.assertEqual(result[1], x[2:])\n",
      "        self.assertEqual(result[1].get_device(), 1)\n",
      "        grad = result[0].data.clone().fill_(2)\n",
      "        result[0].backward(grad)\n",
      "        self.assertEqual(x.grad.data[:2], grad)\n",
      "        self.assertEqual(x.grad.data[2:], grad.clone().zero_())\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_scatter_cpu(self):\n",
      "        self._test_scatter(torch.randn(4, 4))\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_scatter_gpu(self):\n",
      "        self._test_scatter(torch.randn(4, 4).cuda())\n",
      "\n",
      "    def _test_gather(self, output_device):\n",
      "        inputs = (\n",
      "            Variable(torch.randn(2, 4).cuda(0), requires_grad=True),\n",
      "            Variable(torch.randn(2, 4).cuda(1), requires_grad=True)\n",
      "        )\n",
      "        result = dp.gather(inputs, output_device)\n",
      "        self.assertEqual(result.size(), torch.Size([4, 4]))\n",
      "        self.assertEqual(result[:2], inputs[0])\n",
      "        self.assertEqual(result[2:], inputs[1])\n",
      "        if output_device != -1:\n",
      "            self.assertEqual(result.get_device(), output_device)\n",
      "        else:\n",
      "            self.assertFalse(result.is_cuda)\n",
      "        grad = torch.randn(4, 4)\n",
      "        if output_device != -1:\n",
      "            grad = grad.cuda(output_device)\n",
      "        result.backward(grad)\n",
      "        self.assertEqual(inputs[0].grad.data, grad[:2])\n",
      "        self.assertEqual(inputs[1].grad.data, grad[2:])\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_gather_cpu(self):\n",
      "        self._test_gather(-1)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_gather_gpu(self):\n",
      "        self._test_gather(0)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_replicate(self):\n",
      "        module = nn.Linear(10, 5).float().cuda()\n",
      "        input = Variable(torch.randn(2, 10).float().cuda())\n",
      "        expected_output = module(input).data\n",
      "        replicas = dp.replicate(module, (0, 1))\n",
      "        for i, replica in enumerate(replicas):\n",
      "            for p in replica.parameters():\n",
      "                self.assertEqual(p.get_device(), i)\n",
      "            replica_input = input.cuda(i)\n",
      "            self.assertEqual(replica(replica_input).data, expected_output)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_replicate_buffers(self):\n",
      "        net = nn.Module()\n",
      "        net.bn = nn.BatchNorm2d(10)\n",
      "        net.cuda()\n",
      "        replicas = dp.replicate(net, (0, 1))\n",
      "        for i, replica in enumerate(replicas):\n",
      "            self.assertEqual(replica.bn.running_mean.get_device(), i, 'buffer on wrong device')\n",
      "            self.assertEqual(replica.bn.running_var.get_device(), i, 'buffer on wrong device')\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_parallel_apply(self):\n",
      "        l1 = nn.Linear(10, 5).float().cuda(0)\n",
      "        l2 = nn.Linear(10, 5).float().cuda(1)\n",
      "        i1 = Variable(torch.randn(2, 10).float().cuda(0))\n",
      "        i2 = Variable(torch.randn(2, 10).float().cuda(1))\n",
      "        expected1 = l1(i1).data\n",
      "        expected2 = l2(i2).data\n",
      "        inputs = ((i1,), (i2,))\n",
      "        modules = (l1, l2)\n",
      "        expected_outputs = (expected1, expected2)\n",
      "\n",
      "        outputs = dp.parallel_apply(modules, inputs, None)\n",
      "        for out, expected in zip(outputs, expected_outputs):\n",
      "            self.assertEqual(out.data, expected)\n",
      "\n",
      "        inputs = (i1, Variable(i2.data.new()))\n",
      "        expected_outputs = (expected1, expected2.new())\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_data_parallel_multiple_input(self):\n",
      "        class TestModule(nn.Module):\n",
      "\n",
      "            def forward(self, var1, var2, float1, var3=None):\n",
      "                if var3 is None:\n",
      "                    return float1 * (var1 * var2)\n",
      "                else:\n",
      "                    return float1 * (var1 * var2 + var3)\n",
      "\n",
      "        m = TestModule()\n",
      "        var1 = Variable(torch.randn(5, 5).float(), requires_grad=True)\n",
      "        var2 = Variable(torch.randn(5, 5).float(), requires_grad=True)\n",
      "        var3 = Variable(torch.randn(5, 5).float(), requires_grad=False)\n",
      "\n",
      "        float1 = torch.randn(1)[0]\n",
      "\n",
      "        expected = m(var1, var2, float1)\n",
      "        loss = expected.sum()\n",
      "        loss.backward()\n",
      "        gvar1_exp = var1.grad.clone()\n",
      "        gvar2_exp = var2.grad.clone()\n",
      "\n",
      "        def local_test(out):\n",
      "            var1.grad.data.fill_(0.0)\n",
      "            var2.grad.data.fill_(0.0)\n",
      "            loss = out.sum()\n",
      "            loss.backward()\n",
      "            self.assertEqual(out, expected)\n",
      "            self.assertEqual(gvar1_exp, var1.grad)\n",
      "            self.assertEqual(gvar2_exp, var2.grad)\n",
      "\n",
      "        out = dp.data_parallel(m, (var1, var2, float1), (0, 1))\n",
      "        local_test(out)\n",
      "\n",
      "        out = dp.data_parallel(m, (var1, var2, float1), (1, 0))\n",
      "        local_test(out)\n",
      "\n",
      "        out = dp.data_parallel(m, (var1, var2, float1), (0,))\n",
      "        local_test(out)\n",
      "\n",
      "        var1.grad.data.fill_(0.0)\n",
      "        var2.grad.data.fill_(0.0)\n",
      "        expected = m(var1, var2, float1, var3=var3)\n",
      "        loss = expected.sum()\n",
      "        loss.backward()\n",
      "        gvar1_exp = var1.grad.clone()\n",
      "        gvar2_exp = var2.grad.clone()\n",
      "\n",
      "        dpm = nn.DataParallel(TestModule())\n",
      "        out = dpm(var1, var2, float1, var3=var3)\n",
      "        local_test(out)\n",
      "\n",
      "        dpm = nn.DataParallel(TestModule(), device_ids=[0])\n",
      "        out = dpm(var1, var2, float1, var3=var3)\n",
      "        local_test(out)\n",
      "\n",
      "        kwarg_wrap = {'var3': var3}\n",
      "        out = dp.data_parallel(\n",
      "            m, (var1, var2, float1), (0, 1), module_kwargs=kwarg_wrap)\n",
      "        local_test(out)\n",
      "\n",
      "        out = dp.data_parallel(\n",
      "            m, (var1, var2, float1), (0,), module_kwargs=kwarg_wrap)\n",
      "        local_test(out)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_data_parallel_small_back(self):\n",
      "        l = nn.Linear(10, 5).float().cuda()\n",
      "        i = Variable(torch.randn(20, 10).float().cuda())\n",
      "        out = dp.data_parallel(l, i, (0, 1))\n",
      "        self.assertEqual(out, l(i))\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_data_parallel(self):\n",
      "        l = nn.Linear(10, 5).float().cuda()\n",
      "        i = Variable(torch.randn(20, 10).float().cuda(1))\n",
      "        l.cuda(1)\n",
      "        expected_out = l(i).data\n",
      "        l.cuda(0)\n",
      "        out = dp.data_parallel(l, i, (0, 1))\n",
      "        self.assertEqual(out.get_device(), 0)\n",
      "        self.assertEqual(out.data, expected_out)\n",
      "\n",
      "        # Check for None device_ids\n",
      "        out = dp.data_parallel(l, i)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_data_parallel_nested_output(self):\n",
      "        def fn(input):\n",
      "            return [input, (input.sin(), input.cos(), [input.add(1)]), input]\n",
      "\n",
      "        class Net(nn.Module):\n",
      "            def forward(self, input):\n",
      "                return fn(input)\n",
      "\n",
      "        i = Variable(torch.randn(2, 2).float().cuda(1))\n",
      "        gpus = range(torch.cuda.device_count())\n",
      "        output = dp.data_parallel(Net(), i, gpus)\n",
      "        self.assertEqual(output, fn(i))\n",
      "        self.assertIsInstance(output[0], Variable)\n",
      "        self.assertIsInstance(output[1], tuple)\n",
      "        self.assertIsInstance(output[1][0], Variable)\n",
      "        self.assertIsInstance(output[1][1], Variable)\n",
      "        self.assertIsInstance(output[1][2], list)\n",
      "        self.assertIsInstance(output[1][2][0], Variable)\n",
      "        self.assertIsInstance(output[2], Variable)\n",
      "\n",
      "    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n",
      "    def test_data_parallel_nested_input(self):\n",
      "        def fn(input):\n",
      "            return input[1][0]\n",
      "\n",
      "        class Net(nn.Module):\n",
      "            def forward(self, *input):\n",
      "                return fn(input)\n",
      "\n",
      "        i = Variable(torch.randn(20, 3).float().cuda(1))\n",
      "        input = (i.cos(), (i.sin(), i), i.sin())\n",
      "        gpus = range(torch.cuda.device_count())\n",
      "        output = dp.data_parallel(Net(), input, gpus)\n",
      "        self.assertEqual(output, fn(input))\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_data_parallel_module(self):\n",
      "        l = nn.Linear(10, 5).float().cuda()\n",
      "        i = Variable(torch.randn(20, 10).float().cuda())\n",
      "        expected_out = l(i).data\n",
      "        net = nn.DataParallel(l)\n",
      "        out = net(i)\n",
      "        self.assertEqual(out.get_device(), 0)\n",
      "        self.assertEqual(out.data, expected_out)\n",
      "\n",
      "    def test_state_dict(self):\n",
      "        l = nn.Linear(5, 5)\n",
      "        block = nn.Module()\n",
      "        block.conv = nn.Conv2d(3, 3, 3, bias=False)\n",
      "        net = nn.Module()\n",
      "        net.linear1 = l\n",
      "        net.linear2 = l\n",
      "        net.bn = nn.BatchNorm2d(2)\n",
      "        net.block = block\n",
      "        net.add_module('empty', None)\n",
      "\n",
      "        state_dict = net.state_dict()\n",
      "        self.assertEqual(len(state_dict), 9)\n",
      "        self.assertIn('linear1.weight', state_dict)\n",
      "        self.assertIn('linear1.bias', state_dict)\n",
      "        self.assertIn('linear2.weight', state_dict)\n",
      "        self.assertIn('linear2.bias', state_dict)\n",
      "        self.assertIn('block.conv.weight', state_dict)\n",
      "        self.assertIn('block.conv.weight', state_dict)\n",
      "        self.assertNotIn('block.conv.bias', state_dict)\n",
      "        self.assertIn('bn.weight', state_dict)\n",
      "        self.assertIn('bn.bias', state_dict)\n",
      "        self.assertIn('bn.running_var', state_dict)\n",
      "        self.assertIn('bn.running_mean', state_dict)\n",
      "        self.assertFalse(any(map(lambda k: k.startswith('empty'), state_dict.keys())))\n",
      "        for k, v in state_dict.items():\n",
      "            param = net\n",
      "            for component in k.split('.'):\n",
      "                param = getattr(param, component)\n",
      "                if isinstance(param, Parameter):\n",
      "                    param = param.data\n",
      "            self.assertIs(v, param)\n",
      "\n",
      "        l = nn.Linear(5, 5)\n",
      "        state_dict = l.state_dict()\n",
      "        self.assertEqual(len(state_dict), 2)\n",
      "        self.assertIs(state_dict['weight'], l.weight.data)\n",
      "        self.assertIs(state_dict['bias'], l.bias.data)\n",
      "\n",
      "    def test_load_state_dict(self):\n",
      "        l = nn.Linear(5, 5)\n",
      "        block = nn.Module()\n",
      "        block.conv1 = nn.Conv2d(3, 3, 3, bias=True)\n",
      "        block.conv2 = nn.Conv2d(3, 3, 3, bias=False)\n",
      "        net = nn.Module()\n",
      "        net.linear1 = l\n",
      "        net.linear2 = l\n",
      "        net.bn = nn.BatchNorm2d(2)\n",
      "        net.block = block\n",
      "        net.add_module('empty', None)\n",
      "\n",
      "        state_dict = net.state_dict()\n",
      "        state_dict.update({\n",
      "            'linear1.weight': torch.ones(5, 5),\n",
      "            'block.conv1.bias': torch.arange(1, 4),\n",
      "            'bn.running_mean': torch.randn(2),\n",
      "        })\n",
      "        net.load_state_dict(state_dict)\n",
      "        self.assertEqual(net.linear1.weight.data, state_dict['linear1.weight'])\n",
      "        self.assertEqual(net.block.conv1.bias.data, state_dict['block.conv1.bias'])\n",
      "        self.assertEqual(net.bn.running_mean, state_dict['bn.running_mean'])\n",
      "\n",
      "        state_dict = net.state_dict()\n",
      "        state_dict.update({'extra': torch.ones(5)})\n",
      "        self.assertRaises(KeyError, lambda: net.load_state_dict(state_dict))\n",
      "\n",
      "        state_dict = net.state_dict()\n",
      "        del state_dict['linear1.weight']\n",
      "        self.assertRaises(KeyError, lambda: net.load_state_dict(state_dict))\n",
      "\n",
      "    def test_parameter_assignment(self):\n",
      "        l = nn.Linear(5, 5)\n",
      "\n",
      "        def num_params():\n",
      "            return len(list(l.parameters()))\n",
      "\n",
      "        self.assertEqual(num_params(), 2)\n",
      "\n",
      "        new_param = Parameter(torch.randn(5, 5))\n",
      "        l.param_name = new_param\n",
      "        self.assertEqual(num_params(), 3)\n",
      "        self.assertObjectIn(new_param, l.parameters())\n",
      "\n",
      "        var = Variable(torch.randn(5, 5))\n",
      "        l.var_name = var\n",
      "        self.assertEqual(num_params(), 3)\n",
      "        self.assertNotIn(id(var), map(id, l.parameters()))\n",
      "\n",
      "        # Make sure Variables are not saved as parameters\n",
      "        l.variable_attr = Variable(torch.Tensor(5, 5))\n",
      "        self.assertEqual(num_params(), 3)\n",
      "        l.param_attr = Parameter(torch.Tensor(5, 5))\n",
      "        self.assertEqual(num_params(), 4)\n",
      "\n",
      "        # It shouldn't be possible to replace a parameter with a Variable\n",
      "        def assign_var():\n",
      "            l.param_attr = Variable(torch.Tensor(5, 5))\n",
      "\n",
      "        self.assertRaises(TypeError, assign_var)\n",
      "        # But replacing it with None should be fine\n",
      "        l.param_attr = None\n",
      "        self.assertEqual(num_params(), 3)\n",
      "\n",
      "    def test_assignment(self):\n",
      "        l = nn.Module()\n",
      "        a = nn.Parameter(torch.randn(2))\n",
      "        b = nn.Parameter(torch.randn(3))\n",
      "        c = nn.Parameter(torch.randn(4))\n",
      "        q = nn.Linear(4, 4)\n",
      "        r = nn.Linear(5, 5)\n",
      "        w = nn.Linear(6, 6)\n",
      "\n",
      "        def test_assignments(get_list, a, b, c):\n",
      "            # Check that None can be shadowed\n",
      "            l.a = None\n",
      "            self.assertIsNone(l.a)\n",
      "            self.assertIn('a', l.__dict__)\n",
      "            l.a = a\n",
      "            self.assertIs(l.a, a)\n",
      "            self.assertEqual(get_list(), [a])\n",
      "            self.assertNotIn('a', l.__dict__)\n",
      "\n",
      "            # Assign second object\n",
      "            l.b = None\n",
      "            self.assertIsNone(l.b)\n",
      "            self.assertIn('b', l.__dict__)\n",
      "            l.b = b\n",
      "            self.assertIs(l.b, b)\n",
      "            self.assertEqual(get_list(), [a, b])\n",
      "            self.assertNotIn('b', l.__dict__)\n",
      "\n",
      "            # Remove and add the object back. Order should be unchanged.\n",
      "            l.a = None\n",
      "            self.assertIsNone(l.a)\n",
      "            self.assertEqual(get_list(), [b])\n",
      "            l.a = a\n",
      "            self.assertIs(l.a, a)\n",
      "            self.assertEqual(get_list(), [a, b])\n",
      "\n",
      "            # Replace object with another one. Order should be unchanged.\n",
      "            l.a = c\n",
      "            self.assertIs(l.a, c)\n",
      "            self.assertEqual(get_list(), [c, b])\n",
      "\n",
      "            # Remove and reassign an attribute. It should appear at the end of the list now.\n",
      "            del l.a\n",
      "            self.assertFalse(hasattr(l, 'a'))\n",
      "            l.a = a\n",
      "            self.assertIs(l.a, a)\n",
      "            self.assertEqual(get_list(), [b, a])\n",
      "\n",
      "        test_assignments(lambda: list(l.parameters()), a, b, c)\n",
      "        del l.a, l.b\n",
      "        self.assertEqual(list(l.parameters()), [])\n",
      "\n",
      "        test_assignments(lambda: list(l.children()), q, r, w)\n",
      "        del l.a, l.b\n",
      "        self.assertEqual(list(l.children()), [])\n",
      "\n",
      "        buf = torch.randn(10)\n",
      "        l.register_buffer('buf', buf)\n",
      "        self.assertIs(l.buf, buf)\n",
      "        l.buf = None\n",
      "        self.assertIs(l.buf, None)\n",
      "        self.assertNotIn('buf', l.__dict__)  # should be stored in l._buffers\n",
      "        l.buf = buf\n",
      "        self.assertIn('buf', l.state_dict())\n",
      "        self.assertIs(l.state_dict()['buf'], buf)\n",
      "\n",
      "    def test_Conv2d_inconsistent_types(self):\n",
      "        inputs = Variable(torch.randn(4, 1, 7, 7).float())\n",
      "        weights = Variable(torch.randn(1, 1, 3, 3).double())\n",
      "        # inconsistent types should raise an exception\n",
      "        self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\n",
      "        # but it should work with the same type\n",
      "        nn.functional.conv2d(inputs.float(), weights.float())\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_Conv2d_inconsistent_types_on_GPU_without_cudnn(self):\n",
      "        inputs = Variable(torch.randn(4, 1, 7, 7).float().cuda())\n",
      "        weights = Variable(torch.randn(1, 1, 3, 3).double().cuda())\n",
      "        bias = Variable(torch.randn(1).double().cuda())\n",
      "\n",
      "        torch.backends.cudnn.enabled = False\n",
      "        # inconsistent types should raise an exception\n",
      "        self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\n",
      "        self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights.float(), bias))\n",
      "\n",
      "        # but it should work with the same type\n",
      "        nn.functional.conv2d(inputs.float(), weights.float(), bias.float())\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    @unittest.skipIf(not TEST_CUDNN, 'CUDNN not available')\n",
      "    def test_Conv2d_inconsistent_types_on_GPU_with_cudnn(self):\n",
      "        inputs = Variable(torch.randn(4, 1, 7, 7).float().cuda())\n",
      "        weights = Variable(torch.randn(1, 1, 3, 3).double().cuda())\n",
      "        bias = Variable(torch.randn(1).double().cuda())\n",
      "\n",
      "        torch.backends.cudnn.enabled = True\n",
      "        # inconsistent types should raise an exception\n",
      "        self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\n",
      "        self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights.float(), bias))\n",
      "\n",
      "        # but it should work with the same type\n",
      "        nn.functional.conv2d(inputs.float(), weights.float(), bias.float())\n",
      "\n",
      "    def test_Conv2d_missing_argument(self):\n",
      "        c = nn.Conv2d(3, 3, 3)\n",
      "        self.assertRaises(RuntimeError, lambda: c(None))\n",
      "\n",
      "    def test_Conv2d_backward_twice(self):\n",
      "        input = Variable(torch.randn(2, 3, 5, 5))\n",
      "        c = nn.Conv2d(3, 3, 3)\n",
      "        o1 = c(input)\n",
      "        o1.sum().backward()\n",
      "        self.assertRaisesRegex(RuntimeError, 'Specify retain_graph=True',\n",
      "                               lambda: o1.sum().backward())\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_Conv2d_large_workspace(self):\n",
      "        # These sizes require huge cuDNN workspaces. Make sure we choose a\n",
      "        # reasonable algorithm that does not run out of memory\n",
      "        sizes = [\n",
      "            (1, 256, 109, 175),\n",
      "            (1, 256, 80, 128),\n",
      "            (1, 256, 120, 192),\n",
      "        ]\n",
      "        dtype = torch.cuda.FloatTensor\n",
      "\n",
      "        def run_test(benchmark):\n",
      "            torch.backends.cudnn.benchmark = benchmark\n",
      "            conv = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1).type(dtype)\n",
      "            for size in sizes:\n",
      "                x = torch.randn(size).type(dtype)\n",
      "                out = conv(Variable(x, requires_grad=True))\n",
      "                out.backward(torch.ones(out.size()).type(dtype))\n",
      "\n",
      "        b = torch.backends.cudnn.benchmark\n",
      "        try:\n",
      "            run_test(benchmark=False)\n",
      "            run_test(benchmark=True)\n",
      "        finally:\n",
      "            torch.backends.cudnn.benchmark = b\n",
      "\n",
      "    def test_conv_modules_raise_error_on_incorrect_input_size(self):\n",
      "        modules = [nn.Conv1d(3, 8, 3), nn.ConvTranspose1d(3, 8, 3),\n",
      "                   nn.Conv2d(3, 8, 3), nn.ConvTranspose2d(3, 8, 3),\n",
      "                   nn.Conv3d(3, 8, 3), nn.ConvTranspose3d(3, 8, 3)]\n",
      "\n",
      "        invalid_input_dims = [(2, 4), (2, 4),\n",
      "                              (3, 5), (3, 5),\n",
      "                              (4, 6), (4, 6)]\n",
      "\n",
      "        for invalid_dims, module in zip(invalid_input_dims, modules):\n",
      "            for dims in invalid_dims:\n",
      "                input = Variable(torch.Tensor(torch.Size((3, ) * dims)))\n",
      "                self.assertRaises(ValueError, lambda: module(input))\n",
      "\n",
      "    def test_ConvTranspose2d_output_size(self):\n",
      "        m = nn.ConvTranspose2d(3, 4, 3, 3, 0, 2)\n",
      "        i = Variable(torch.randn(2, 3, 6, 6))\n",
      "        for h in range(15, 22):\n",
      "            for w in range(15, 22):\n",
      "                if 18 <= h <= 20 and 18 <= w <= 20:\n",
      "                    output = m(i, output_size=(h, w))\n",
      "                    self.assertEqual(output.size()[2:], (h, w))\n",
      "                else:\n",
      "                    self.assertRaises(ValueError, lambda: m(i, (h, w)))\n",
      "\n",
      "    def test_Conv2d_naive_groups(self):\n",
      "        # Check that grouped convolutions matches two half convolutions\n",
      "        m = nn.Conv2d(4, 4, kernel_size=3, groups=2)\n",
      "        i = Variable(torch.randn(2, 4, 6, 6), requires_grad=True)\n",
      "        output = m(i)\n",
      "        grad_output = torch.randn(2, 4, 4, 4)\n",
      "        output.backward(grad_output)\n",
      "\n",
      "        m1 = nn.Conv2d(2, 2, kernel_size=3)\n",
      "        m1.weight.data.copy_(m.weight.data[:2])\n",
      "        m1.bias.data.copy_(m.bias.data[:2])\n",
      "        i1 = Variable(i.data[:, :2].contiguous(), requires_grad=True)\n",
      "        output1 = m1(i1)\n",
      "        output1.backward(grad_output[:, :2].contiguous())\n",
      "\n",
      "        m2 = nn.Conv2d(2, 2, kernel_size=3)\n",
      "        m2.weight.data.copy_(m.weight.data[2:])\n",
      "        m2.bias.data.copy_(m.bias.data[2:])\n",
      "        i2 = Variable(i.data[:, 2:].contiguous(), requires_grad=True)\n",
      "        output2 = m2(i2)\n",
      "        output2.backward(grad_output[:, 2:].contiguous())\n",
      "\n",
      "        self.assertEqual(output, torch.cat([output1, output2], 1))\n",
      "        self.assertEqual(i.grad.data,\n",
      "                         torch.cat([i1.grad.data, i2.grad.data], 1))\n",
      "        self.assertEqual(m.bias.grad.data,\n",
      "                         torch.cat([m1.bias.grad.data, m2.bias.grad.data], 0))\n",
      "        self.assertEqual(m.weight.grad.data,\n",
      "                         torch.cat([m1.weight.grad.data, m2.weight.grad.data], 0))\n",
      "\n",
      "    # For https://github.com/pytorch/pytorch/pull/1273\n",
      "    # Almost identical to the above `test_Conv2d_naive_groups`\n",
      "    def test_Conv2d_groups_nobias(self):\n",
      "        m = nn.Conv2d(4, 4, kernel_size=3, groups=2, bias=False)\n",
      "        i = Variable(torch.randn(2, 4, 6, 6), requires_grad=True)\n",
      "        output = m(i)\n",
      "        grad_output = torch.randn(2, 4, 4, 4)\n",
      "        output.backward(grad_output)\n",
      "\n",
      "        m1 = nn.Conv2d(2, 2, kernel_size=3, bias=False)\n",
      "        m1.weight.data.copy_(m.weight.data[:2])\n",
      "        i1 = Variable(i.data[:, :2].contiguous(), requires_grad=True)\n",
      "        output1 = m1(i1)\n",
      "        output1.backward(grad_output[:, :2].contiguous())\n",
      "\n",
      "        m2 = nn.Conv2d(2, 2, kernel_size=3, bias=False)\n",
      "        m2.weight.data.copy_(m.weight.data[2:])\n",
      "        i2 = Variable(i.data[:, 2:].contiguous(), requires_grad=True)\n",
      "        output2 = m2(i2)\n",
      "        output2.backward(grad_output[:, 2:].contiguous())\n",
      "\n",
      "        self.assertEqual(output, torch.cat([output1, output2], 1))\n",
      "        self.assertEqual(i.grad.data,\n",
      "                         torch.cat([i1.grad.data, i2.grad.data], 1))\n",
      "        self.assertEqual(m.weight.grad.data,\n",
      "                         torch.cat([m1.weight.grad.data, m2.weight.grad.data], 0))\n",
      "\n",
      "    def test_MaxUnpool2d_output_size(self):\n",
      "        m = nn.MaxPool2d(3, stride=2, return_indices=True)\n",
      "        mu = nn.MaxUnpool2d(3, stride=2)\n",
      "        big_t = torch.rand(1, 1, 6, 6)\n",
      "        big_t[0][0][4][4] = 100\n",
      "        output_big, indices_big = m(Variable(big_t))\n",
      "        self.assertRaises(RuntimeError, lambda: mu(output_big, indices_big))\n",
      "\n",
      "        small_t = torch.rand(1, 1, 5, 5)\n",
      "        for i in range(0, 4, 2):\n",
      "            for j in range(0, 4, 2):\n",
      "                small_t[:, :, i, j] = 100\n",
      "        output_small, indices_small = m(Variable(small_t))\n",
      "        for h in range(3, 10):\n",
      "            for w in range(3, 10):\n",
      "                if 4 <= h <= 6 and 4 <= w <= 6:\n",
      "                    size = (h, w)\n",
      "                    if h == 5:\n",
      "                        size = torch.LongStorage(size)\n",
      "                    elif h == 6:\n",
      "                        size = torch.LongStorage((1, 1) + size)\n",
      "                    mu(output_small, indices_small, output_size=size)\n",
      "                else:\n",
      "                    self.assertRaises(ValueError, lambda: mu(output_small, indices_small, (h, w)))\n",
      "\n",
      "    def test_container_copy(self):\n",
      "        class Model(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Model, self).__init__()\n",
      "                self.linear = nn.Linear(4, 5)\n",
      "\n",
      "            def forward(self, input):\n",
      "                return self.linear(input)\n",
      "\n",
      "        input = Variable(torch.randn(2, 4))\n",
      "\n",
      "        model = Model()\n",
      "        model_cp = deepcopy(model)\n",
      "        self.assertEqual(model(input).data, model_cp(input).data)\n",
      "\n",
      "        model_cp.linear.weight.data[:] = 2\n",
      "        self.assertNotEqual(model(input).data, model_cp(input).data)\n",
      "\n",
      "    def test_RNN_cell(self):\n",
      "        # this is just a smoke test; these modules are implemented through\n",
      "        # autograd so no Jacobian test is needed\n",
      "        for module in (nn.RNNCell, nn.GRUCell):\n",
      "            for bias in (True, False):\n",
      "                input = Variable(torch.randn(3, 10))\n",
      "                hx = Variable(torch.randn(3, 20))\n",
      "                cell = module(10, 20, bias=bias)\n",
      "                for i in range(6):\n",
      "                    hx = cell(input, hx)\n",
      "\n",
      "                hx.sum().backward()\n",
      "\n",
      "    def test_invalid_dropout_p(self):\n",
      "        v = Variable(torch.ones(1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout(-0.1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout(1.1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout2d(-0.1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout2d(1.1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout3d(-0.1))\n",
      "        self.assertRaises(ValueError, lambda: nn.Dropout3d(1.1))\n",
      "        self.assertRaises(ValueError, lambda: F.dropout(v, -0.1))\n",
      "        self.assertRaises(ValueError, lambda: F.dropout(v, 1.1))\n",
      "\n",
      "    def test_pack_padded_sequence(self):\n",
      "        def pad(tensor, length):\n",
      "            return torch.cat([tensor, tensor.new(length - tensor.size(0), *tensor.size()[1:]).zero_()])\n",
      "        lengths = [10, 8, 4, 2, 2, 2, 1]\n",
      "        max_length = lengths[0]\n",
      "        batch_sizes = [sum(map(bool, filter(lambda x: x >= i, lengths))) for i in range(1, max_length + 1)]\n",
      "        offset = 0\n",
      "        padded = torch.cat([pad(i * 100 + torch.arange(1, 5 * l + 1).view(l, 1, 5), max_length)\n",
      "                            for i, l in enumerate(lengths, 1)], 1)\n",
      "        padded = Variable(padded, requires_grad=True)\n",
      "        expected_data = [[torch.arange(1, 6) + (i + 1) * 100 + 5 * n for i in range(batch_size)]\n",
      "                         for n, batch_size in enumerate(batch_sizes)]\n",
      "        expected_data = list(itertools.chain.from_iterable(expected_data))\n",
      "        expected_data = torch.stack(expected_data, dim=0)\n",
      "\n",
      "        for batch_first in (True, False):\n",
      "            src = padded\n",
      "            if batch_first:\n",
      "                src = src.transpose(0, 1)\n",
      "\n",
      "            # check output\n",
      "            packed = rnn_utils.pack_padded_sequence(src, lengths, batch_first=batch_first)\n",
      "            self.assertEqual(packed.data.data, expected_data)\n",
      "            self.assertEqual(packed.batch_sizes, batch_sizes)\n",
      "\n",
      "            # test inverse\n",
      "            unpacked, unpacked_len = rnn_utils.pad_packed_sequence(packed, batch_first=batch_first)\n",
      "            self.assertEqual(unpacked, src)\n",
      "            self.assertEqual(unpacked_len, lengths)\n",
      "\n",
      "            # check grad\n",
      "            if padded.grad is not None:\n",
      "                padded.grad.data.zero_()\n",
      "            grad_output = unpacked.data.clone().normal_()\n",
      "            unpacked.backward(grad_output)\n",
      "            if batch_first:\n",
      "                grad_output.transpose_(0, 1)\n",
      "            for i, l in enumerate(lengths):\n",
      "                self.assertEqual(padded.grad.data[:l, i], grad_output[:l, i])\n",
      "                if l < 10:\n",
      "                    self.assertEqual(padded.grad.data[l:, i].abs().sum(), 0)\n",
      "\n",
      "    def _test_variable_sequence(self, cuda):\n",
      "        def pad(var, length):\n",
      "            if var.size(0) == length:\n",
      "                return var\n",
      "            return torch.cat([var, Variable(var.data.new(length - var.size(0), *var.size()[1:]).zero_())])\n",
      "\n",
      "        lengths = [10, 10, 6, 2, 2, 1, 1]\n",
      "        max_length = lengths[0]\n",
      "        x_leaf = Variable(torch.randn(max_length, len(lengths), 3), requires_grad=True)\n",
      "        lstm = nn.LSTM(3, 4, bidirectional=True, num_layers=2)\n",
      "        lstm2 = deepcopy(lstm)\n",
      "        if cuda:\n",
      "            x = x_leaf.cuda()\n",
      "            lstm.cuda()\n",
      "            lstm2.cuda()\n",
      "        else:\n",
      "            x = x_leaf\n",
      "\n",
      "        # Compute sequences separately\n",
      "        seq_outs = []\n",
      "        seq_hiddens = []\n",
      "        for i, l in enumerate(lengths):\n",
      "            out, hid = lstm2(x[:l, i:i + 1])\n",
      "            out_pad = pad(out, max_length)\n",
      "            seq_outs.append(out_pad)\n",
      "            seq_hiddens.append(hid)\n",
      "        seq_out = torch.cat(seq_outs, 1)\n",
      "        seq_hidden = tuple(torch.cat(hids, 1) for hids in zip(*seq_hiddens))\n",
      "\n",
      "        # Use packed format\n",
      "        packed = rnn_utils.pack_padded_sequence(x, lengths)\n",
      "        packed_out, packed_hidden = lstm(packed)\n",
      "        unpacked, unpacked_len = rnn_utils.pad_packed_sequence(packed_out)\n",
      "\n",
      "        # Check forward\n",
      "        self.assertEqual(packed_hidden, seq_hidden)\n",
      "        self.assertEqual(unpacked, seq_out)\n",
      "        self.assertEqual(unpacked_len, lengths)\n",
      "\n",
      "        # Check backward\n",
      "        seq_out.sum().backward()\n",
      "        grad_x = x_leaf.grad.data.clone()\n",
      "        x_leaf.grad.data.zero_()\n",
      "        unpacked.sum().backward()\n",
      "\n",
      "        self.assertEqual(x_leaf.grad.data, grad_x)\n",
      "        for p1, p2 in zip(lstm.parameters(), lstm2.parameters()):\n",
      "            self.assertEqual(p1.grad, p2.grad)\n",
      "\n",
      "    def test_variable_sequence(self):\n",
      "        self._test_variable_sequence(False)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_variable_sequence_cuda(self):\n",
      "        self._test_variable_sequence(True)\n",
      "\n",
      "    def test_LSTM_cell(self):\n",
      "        # this is just a smoke test; these modules are implemented through\n",
      "        # autograd so no Jacobian test is needed\n",
      "        for bias in (True, False):\n",
      "            input = Variable(torch.randn(3, 10))\n",
      "            hx = Variable(torch.randn(3, 20))\n",
      "            cx = Variable(torch.randn(3, 20))\n",
      "            lstm = nn.LSTMCell(10, 20, bias=bias)\n",
      "            for i in range(6):\n",
      "                hx, cx = lstm(input, (hx, cx))\n",
      "\n",
      "            (hx + cx).sum().backward()\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_cuda_rnn_fused(self):\n",
      "        def copy_rnn(rnn1, rnn2):\n",
      "            for x_layer, y_layer in zip(rnn1.all_weights, rnn2.all_weights):\n",
      "                for x, y in zip(x_layer, y_layer):\n",
      "                    x.data.copy_(y.data)\n",
      "\n",
      "        def check_rnn_grads(rnn1, rnn2):\n",
      "            for x_layer, y_layer in zip(rnn1.all_weights, rnn2.all_weights):\n",
      "                for x, y in zip(x_layer, y_layer):\n",
      "                    self.assertEqual(x.grad, y.grad, prec=5e-5)\n",
      "\n",
      "        input_size = 10\n",
      "        hidden_size = 6\n",
      "        num_layers = 2\n",
      "        seq_length = 7\n",
      "        batch = 6\n",
      "        input_val = torch.randn(seq_length, batch, input_size)\n",
      "        grad_output = torch.randn(seq_length, batch, hidden_size)\n",
      "        hx_val = torch.randn(num_layers, batch, hidden_size)\n",
      "        grad_hy = torch.randn(num_layers, batch, hidden_size)\n",
      "        prev = torch.backends.cudnn.enabled\n",
      "        try:\n",
      "            torch.backends.cudnn.enabled = False\n",
      "            for module in (nn.GRU, nn.LSTM):\n",
      "                for bias in (True, False):\n",
      "                    rnn = module(input_size, hidden_size, num_layers, bias=bias)\n",
      "                    rnn_cuda = module(input_size, hidden_size, num_layers, bias=bias).cuda()\n",
      "                    copy_rnn(rnn, rnn_cuda)\n",
      "\n",
      "                    is_lstm = isinstance(rnn, nn.LSTM)\n",
      "                    if is_lstm:\n",
      "                        hx = (Variable(hx_val.clone(), requires_grad=True),\n",
      "                              Variable(hx_val.clone().add(1), requires_grad=True))\n",
      "                        hx_cuda = (Variable(hx_val.clone().cuda(), requires_grad=True),\n",
      "                                   Variable(hx_val.clone().cuda().add(1), requires_grad=True))\n",
      "                    else:\n",
      "                        hx = Variable(hx_val.clone(), requires_grad=True)\n",
      "                        hx_cuda = Variable(hx_val.clone().cuda(), requires_grad=True)\n",
      "\n",
      "                    inp = Variable(input_val.clone(), requires_grad=True)\n",
      "                    inp_cu = Variable(input_val.clone().cuda(), requires_grad=True)\n",
      "                    output1, hy1 = rnn(inp, hx)\n",
      "                    output2, hy2 = rnn_cuda(inp_cu, hx_cuda)\n",
      "                    if is_lstm:\n",
      "                        torch.autograd.backward(\n",
      "                            [output1, hy1[0], hy1[1]], [grad_output, grad_hy, grad_hy + 1]\n",
      "                        )\n",
      "                        torch.autograd.backward(\n",
      "                            [output2, hy2[0], hy2[1]],\n",
      "                            [grad_output.cuda(), grad_hy.cuda(), (grad_hy + 1).cuda()]\n",
      "                        )\n",
      "                    else:\n",
      "                        torch.autograd.backward([output1, hy1], [grad_output, grad_hy])\n",
      "                        torch.autograd.backward([output2, hy2], [grad_output.cuda(), grad_hy.cuda()])\n",
      "\n",
      "                    self.assertEqual(output1, output2)\n",
      "                    self.assertEqual(hy1, hy2)\n",
      "\n",
      "                    check_rnn_grads(rnn, rnn_cuda)\n",
      "                    self.assertEqual(inp.grad.data, inp_cu.grad.data)\n",
      "                    if is_lstm:\n",
      "                        self.assertEqual(hx[0].grad.data, hx_cuda[0].grad.data)\n",
      "                        self.assertEqual(hx[1].grad.data, hx_cuda[1].grad.data)\n",
      "                    else:\n",
      "                        self.assertEqual(hx.grad.data, hx_cuda.grad.data)\n",
      "        finally:\n",
      "            torch.backends.cudnn.enabled = prev\n",
      "\n",
      "    def test_rnn_initial_hidden_state(self):\n",
      "        rnn_modes = ['RNN', 'GRU', 'LSTM']\n",
      "        for mode in rnn_modes:\n",
      "            rnn = getattr(nn, mode)(30, 20, 2)\n",
      "            input = Variable(torch.randn(10, 32, 30))\n",
      "            hidden = Variable(torch.Tensor(2, 32, 20).zero_())\n",
      "\n",
      "            if mode is 'LSTM':\n",
      "                hidden = (hidden, hidden)\n",
      "            output1, hidden1 = rnn(input, hidden)\n",
      "            output2, hidden2 = rnn(input)\n",
      "            self.assertEqual(output1, output2)\n",
      "            self.assertEqual(hidden1, hidden2)\n",
      "\n",
      "    def _test_rnn_retain_variables(self, dtype):\n",
      "        rnns = [nn.LSTM(10, 20, num_layers=2).type(dtype),\n",
      "                nn.GRU(10, 20, num_layers=2).type(dtype),\n",
      "                nn.RNN(10, 20, num_layers=2).type(dtype)]\n",
      "        for rnn in rnns:\n",
      "            input = Variable(torch.randn(5, 6, 10).type(dtype), requires_grad=True)\n",
      "            output = rnn(input)\n",
      "            output[0].sum().backward(retain_graph=True)\n",
      "            grads = [input.grad.data.clone()] + [p.grad.data.clone() for p in rnn.parameters()]\n",
      "            for i in range(4):\n",
      "                rnn.zero_grad()\n",
      "                input.grad.data.zero_()\n",
      "                output[0].sum().backward(retain_graph=True)\n",
      "                grads2 = [input.grad.data] + [p.grad.data for p in rnn.parameters()]\n",
      "                self.assertEqual(grads, grads2)\n",
      "\n",
      "    def test_rnn_retain_variables(self):\n",
      "        self._test_rnn_retain_variables(torch.DoubleTensor)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_rnn_retain_variables_cuda(self):\n",
      "        try:\n",
      "            torch.backends.cudnn.enabled = False\n",
      "            self._test_rnn_retain_variables(torch.cuda.FloatTensor)\n",
      "        finally:\n",
      "            torch.backends.cudnn.enabled = True\n",
      "        self._test_rnn_retain_variables(torch.cuda.FloatTensor)\n",
      "\n",
      "    def _test_RNN_cpu_vs_cudnn(self, dropout):\n",
      "\n",
      "        def forward_backward(cuda, rnn, input_val, hx_val, grad_output, grad_hy, weights_val):\n",
      "            is_lstm = isinstance(rnn, nn.LSTM)\n",
      "\n",
      "            for x_layer, y_layer in zip(rnn.all_weights, weights_val):\n",
      "                for x, y in zip(x_layer, y_layer):\n",
      "                    x.data.copy_(y.data)\n",
      "\n",
      "            if isinstance(input_val, rnn_utils.PackedSequence):\n",
      "                input = rnn_utils.PackedSequence(\n",
      "                    Variable(input_val.data, requires_grad=True), input_val.batch_sizes)\n",
      "                input_var = input.data\n",
      "            else:\n",
      "                input = Variable(input_val.clone(), requires_grad=True)\n",
      "                input_var = input\n",
      "            if is_lstm:\n",
      "                hx = (Variable(hx_val.clone(), requires_grad=True),\n",
      "                      Variable(hx_val.add(1), requires_grad=True))\n",
      "            else:\n",
      "                hx = Variable(hx_val.clone(), requires_grad=True)\n",
      "\n",
      "            if cuda:\n",
      "                rnn.cuda()\n",
      "                input_var.data = input_var.data.cuda()\n",
      "                if is_lstm:\n",
      "                    hx[0].data = hx[0].data.cuda()\n",
      "                    hx[1].data = hx[1].data.cuda()\n",
      "                else:\n",
      "                    hx.data = hx.data.cuda()\n",
      "                grad_hy = grad_hy.cuda()\n",
      "                grad_output = grad_output.cuda()\n",
      "\n",
      "            output, hy = rnn(input, hx)\n",
      "\n",
      "            if isinstance(output, rnn_utils.PackedSequence):\n",
      "                output = output.data\n",
      "\n",
      "            if is_lstm:\n",
      "                torch.autograd.backward([output, hy[0], hy[1]], [grad_output, grad_hy, grad_hy + 1])\n",
      "            else:\n",
      "                torch.autograd.backward([output, hy], [grad_output, grad_hy])\n",
      "\n",
      "            return {'output': output.data,\n",
      "                    'hy': hy[0].data if is_lstm else hy.data,\n",
      "                    'weights': rnn.all_weights,\n",
      "                    'grad_input': input_var.grad.data,\n",
      "                    'grad_hx': hx[0].grad.data if is_lstm else hx.grad.data,\n",
      "                    'cy': hy[1].data if is_lstm else None,\n",
      "                    'grad_cx': hx[1].grad.data if is_lstm else None}\n",
      "\n",
      "        input_size = 10\n",
      "        hidden_size = 6\n",
      "        num_layers = 2\n",
      "        seq_length = 7\n",
      "        batch = 6\n",
      "\n",
      "        def make_noncontig(tensor):\n",
      "            ndim = tensor.dim()\n",
      "            return torch.stack([tensor.clone().zero_(), tensor], ndim).select(ndim, 1)\n",
      "\n",
      "        def compare_cpu_gpu(outputs_cpu, outputs_gpu):\n",
      "            self.assertEqual(list(outputs_cpu.keys()), list(outputs_gpu.keys()))\n",
      "            for key in outputs_cpu.keys():\n",
      "                if key != 'weights':\n",
      "                    self.assertEqual(outputs_cpu[key], outputs_gpu[key], prec=5e-5, message=key)\n",
      "\n",
      "            # check grad weights separately, as nested dict\n",
      "            for cpu_layer_weight, gpu_layer_weight in zip(outputs_cpu['weights'], outputs_gpu['weights']):\n",
      "                for (cpu_weight, gpu_weight) in zip(cpu_layer_weight, gpu_layer_weight):\n",
      "                    self.assertEqual(cpu_weight.grad.data, gpu_weight.grad.data, prec=5e-5)\n",
      "\n",
      "        for module in (nn.RNN, nn.LSTM, nn.GRU):\n",
      "            for bias, bidirectional, batch_first, contig, variable_len in product((True, False), repeat=5):\n",
      "                num_directions = 2 if bidirectional else 1\n",
      "                if batch_first:\n",
      "                    input_val = torch.randn(batch, seq_length, input_size)\n",
      "                    grad_output = torch.randn(batch, seq_length, hidden_size * num_directions)\n",
      "                else:\n",
      "                    input_val = torch.randn(seq_length, batch, input_size)\n",
      "                    grad_output = torch.randn(seq_length, batch, hidden_size * num_directions)\n",
      "\n",
      "                if not contig:\n",
      "                    grad_output = make_noncontig(grad_output)\n",
      "                    grad_hy = make_noncontig(grad_hy)\n",
      "                    input_var = make_noncontig(input_val)\n",
      "                    hx_val = make_noncontig(hx_val)\n",
      "\n",
      "                hx_val = torch.randn(num_layers * num_directions, batch, hidden_size)\n",
      "                grad_hy = torch.randn(num_layers * num_directions, batch, hidden_size)\n",
      "\n",
      "                if variable_len:\n",
      "                    batch_sizes = [7, 5, 5, 2, 1, 1]\n",
      "                    input_val = rnn_utils.pack_padded_sequence(input_val, batch_sizes, batch_first=batch_first)\n",
      "                    grad_output = rnn_utils.pack_padded_sequence(grad_output, batch_sizes, batch_first=batch_first).data\n",
      "\n",
      "                rnn = module(input_size,\n",
      "                             hidden_size,\n",
      "                             num_layers,\n",
      "                             bias=bias,\n",
      "                             dropout=dropout,\n",
      "                             bidirectional=bidirectional,\n",
      "                             batch_first=batch_first)\n",
      "\n",
      "                outputs_cpu = forward_backward(\n",
      "                    False, rnn, input_val, hx_val, grad_output, grad_hy, rnn.all_weights)\n",
      "\n",
      "                rnn_gpu = module(input_size,\n",
      "                                 hidden_size,\n",
      "                                 num_layers,\n",
      "                                 bias=bias,\n",
      "                                 dropout=dropout,\n",
      "                                 bidirectional=bidirectional,\n",
      "                                 batch_first=batch_first)\n",
      "\n",
      "                outputs_gpu = forward_backward(\n",
      "                    True, rnn_gpu, input_val, hx_val, grad_output, grad_hy, rnn.all_weights)\n",
      "\n",
      "                compare_cpu_gpu(outputs_cpu, outputs_gpu)\n",
      "\n",
      "        for nonlinearity in ('tanh', 'relu'):\n",
      "            hx_val = torch.randn(num_layers, batch, hidden_size)\n",
      "            input_val = torch.randn(seq_length, batch, input_size)\n",
      "            grad_output = torch.randn(\n",
      "                seq_length, batch, hidden_size * num_directions)\n",
      "            grad_hy = torch.randn(\n",
      "                num_layers * num_directions, batch, hidden_size)\n",
      "\n",
      "            rnn = nn.RNN(input_size, hidden_size, num_layers, bias=bias, nonlinearity=nonlinearity)\n",
      "            outputs_cpu = forward_backward(False, rnn, input_val, hx_val, grad_output, grad_hy, rnn.all_weights)\n",
      "\n",
      "            rnn_gpu = nn.RNN(input_size, hidden_size, num_layers, bias=bias, nonlinearity=nonlinearity)\n",
      "            outputs_gpu = forward_backward(True, rnn_gpu, input_val, hx_val, grad_output, grad_hy, rnn.all_weights)\n",
      "\n",
      "            compare_cpu_gpu(outputs_cpu, outputs_gpu)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDNN, \"needs cudnn\")\n",
      "    @default_tensor_type(torch.FloatTensor)  # FIXME: just until torch.cuda.DoubleTensor.sum() implemented\n",
      "    def test_RNN_cpu_vs_cudnn_no_dropout(self):\n",
      "        self._test_RNN_cpu_vs_cudnn(0)\n",
      "\n",
      "    @unittest.skipIf(not (TEST_CUDNN and TEST_CUDNN_VERSION >= 5103), \"needs cudnn >= 5.1\")\n",
      "    @default_tensor_type(torch.FloatTensor)  # FIXME: just until torch.cuda.DoubleTensor.sum() implemented\n",
      "    def test_RNN_cpu_vs_cudnn_with_dropout(self):\n",
      "        # Because of dropout randomness, can only compare dropout=0 and dropout=1\n",
      "        self._test_RNN_cpu_vs_cudnn(1)\n",
      "\n",
      "    @unittest.skipIf(not (TEST_CUDNN and TEST_CUDNN_VERSION >= 5103), \"needs cudnn >= 5.1\")\n",
      "    def test_RNN_dropout(self):\n",
      "        # checking the assumption that cuDNN sticks dropout in between\n",
      "        # RNN layers\n",
      "        for p in (0, 0.276, 0.731, 1):\n",
      "            for train in (True, False):\n",
      "                for cuda in (True, False):\n",
      "                    rnn = nn.RNN(10, 1000, 2, bias=False, dropout=p, nonlinearity='relu')\n",
      "                    if cuda:\n",
      "                        rnn.cuda()\n",
      "\n",
      "                    if train:\n",
      "                        rnn.train()\n",
      "                    else:\n",
      "                        rnn.eval()\n",
      "                    rnn.weight_ih_l0.data.fill_(1)\n",
      "                    rnn.weight_hh_l0.data.fill_(1)\n",
      "                    rnn.weight_ih_l1.data.fill_(1)\n",
      "                    rnn.weight_hh_l1.data.fill_(1)\n",
      "                    input = Variable(torch.Tensor(1, 1, 10).fill_(1))\n",
      "                    hx = Variable(torch.Tensor(2, 1, 1000).fill_(0))\n",
      "                    if cuda:\n",
      "                        input = input.cuda()\n",
      "                        hx = hx.cuda()\n",
      "\n",
      "                    output, hy = rnn(input, hx)\n",
      "                    self.assertEqual(output.data.min(), output.data.max())\n",
      "                    output_val = output.data[0][0][0]\n",
      "                    if p == 0 or not train:\n",
      "                        self.assertEqual(output_val, 10000)\n",
      "                    elif p == 1:\n",
      "                        self.assertEqual(output_val, 0)\n",
      "                    else:\n",
      "                        self.assertGreater(output_val, 8000)\n",
      "                        self.assertLess(output_val, 12000)\n",
      "                        denorm_mod = (output_val * (1 - p)) % 10\n",
      "                        self.assertLess(min(denorm_mod, 10 - denorm_mod), 1e-2)\n",
      "\n",
      "                    self.assertEqual(hy[0].data.min(), hy[0].data.max())\n",
      "                    self.assertEqual(hy[1].data.min(), hy[1].data.max())\n",
      "                    self.assertEqual(hy.data[0][0][0], 10)\n",
      "                    self.assertEqual(hy.data[1][0][0], output_val)\n",
      "\n",
      "    @unittest.skipIf(not (TEST_CUDNN and TEST_CUDNN_VERSION >= 5103), \"needs cudnn >= 5.1\")\n",
      "    def test_RNN_dropout_state(self):\n",
      "        import sys\n",
      "        if sys.version_info[0] == 2:\n",
      "            import cPickle as pickle\n",
      "        else:\n",
      "            import pickle\n",
      "        for p in (0, 0.1234):\n",
      "            for train in (True, False):\n",
      "                for cuda in (True, False):\n",
      "                    rnn = nn.RNN(100, 100, 2, bias=False, dropout=p, nonlinearity='relu')\n",
      "                    if cuda:\n",
      "                        rnn.cuda()\n",
      "\n",
      "                    if train:\n",
      "                        rnn.train()\n",
      "                    else:\n",
      "                        rnn.eval()\n",
      "                    input = Variable(torch.Tensor(1, 1, 100).uniform_())\n",
      "                    hx = Variable(torch.Tensor(2, 1, 100).uniform_())\n",
      "                    if cuda:\n",
      "                        input = input.cuda()\n",
      "                        hx = hx.cuda()\n",
      "\n",
      "                    output1, hy1 = rnn(input, hx)\n",
      "                    output2, hy2 = rnn(input, hx)\n",
      "\n",
      "                    rnn_pickle = pickle.dumps(rnn)\n",
      "                    rnn2 = pickle.loads(rnn_pickle)\n",
      "                    output3, hy3 = rnn2(input, hx)\n",
      "\n",
      "                    if p == 0 or not train:\n",
      "                        self.assertEqual(output1, output2)\n",
      "                        self.assertEqual(output1, output3)\n",
      "                        self.assertEqual(hy1, hy2)\n",
      "                        self.assertEqual(hy1, hy3)\n",
      "                    else:\n",
      "                        self.assertNotEqual(output1, output2)\n",
      "                        self.assertNotEqual(output1, output3)\n",
      "                        self.assertNotEqual(hy1, hy2)\n",
      "                        self.assertNotEqual(hy1, hy3)\n",
      "\n",
      "    @unittest.skipIf(not (TEST_CUDNN and TEST_CUDNN_VERSION >= 5103), \"needs cudnn >= 5.1\")\n",
      "    def test_RNN_change_dropout(self):\n",
      "        for train, cuda in product((True, False), repeat=2):\n",
      "            rnn = nn.RNN(100, 100, 2, dropout=0, nonlinearity='relu')\n",
      "            input = Variable(torch.Tensor(3, 2, 100).uniform_())\n",
      "            if cuda:\n",
      "                input.data = input.data.cuda()\n",
      "                rnn.cuda()\n",
      "\n",
      "            if train:\n",
      "                rnn.train()\n",
      "            else:\n",
      "                rnn.eval()\n",
      "\n",
      "            prev_output = None\n",
      "            for p in (0, 0.5, 0, 0.7, 0.2, 1, 0.2, 0):\n",
      "                rnn.dropout = p\n",
      "                output1, hy1 = rnn(input)\n",
      "                output2, hy2 = rnn(input)\n",
      "\n",
      "                if p == 0 or p == 1 or not train:\n",
      "                    self.assertEqual(output1, output2)\n",
      "                    self.assertEqual(hy1, hy2)\n",
      "                else:\n",
      "                    self.assertNotEqual(output1, output2)\n",
      "                    self.assertNotEqual(hy1, hy2)\n",
      "\n",
      "                if prev_output is not None:\n",
      "                    if not train:\n",
      "                        self.assertEqual(output1.data, prev_output)\n",
      "                        self.assertEqual(output2.data, prev_output)\n",
      "                    else:\n",
      "                        self.assertNotEqual(output1.data, prev_output)\n",
      "                        self.assertNotEqual(output2.data, prev_output)\n",
      "                prev_output = output1.data\n",
      "\n",
      "    def _verify_pixel_shuffle(self, input, output, upscale_factor):\n",
      "        for c in range(output.size(1)):\n",
      "            for h in range(output.size(2)):\n",
      "                for w in range(output.size(3)):\n",
      "                    height_idx = h // upscale_factor\n",
      "                    weight_idx = w // upscale_factor\n",
      "                    channel_idx = (upscale_factor * (h % upscale_factor)) + (w % upscale_factor) + \\\n",
      "                                  (c * upscale_factor ** 2)\n",
      "                    self.assertEqual(output[:, c, h, w], input[:, channel_idx, height_idx, weight_idx])\n",
      "\n",
      "    def test_inplace_thnn(self):\n",
      "        modules = [nn.ReLU, nn.ELU, nn.SELU, nn.RReLU]\n",
      "        for mod in modules:\n",
      "            r = mod(inplace=True)\n",
      "            input = Variable(torch.randn(5, 5), requires_grad=True)\n",
      "            output = r(input + 0)\n",
      "            grad_output = torch.randn(5, 5)\n",
      "            grad_output_clone = grad_output.clone()\n",
      "            output.backward(grad_output)\n",
      "            self.assertEqual(grad_output, grad_output_clone)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, 'CUDA not available')\n",
      "    def test_noncontig_conv_grad(self):\n",
      "        # FIXME: remove after adding non-contiguous grad tests for all modules\n",
      "        module = nn.Conv2d(3, 5, kernel_size=3, padding=1).cuda()\n",
      "        input = Variable(torch.randn(2, 3, 10, 10).cuda(), requires_grad=True)\n",
      "        output = module(input)\n",
      "\n",
      "        grad = torch.randn(2, 2, 5, 10, 10).cuda()[:, 1]\n",
      "        assert not grad.is_contiguous()\n",
      "        output.backward(grad, retain_graph=True)\n",
      "        self.assertIsNotNone(input.grad)\n",
      "        result = input.grad.data.clone()\n",
      "        input.grad.data.zero_()\n",
      "\n",
      "        output.backward(grad.contiguous())\n",
      "        self.assertEqual(result, input.grad.data)\n",
      "\n",
      "    def test_pixel_shuffle(self):\n",
      "        batch_size = random.randint(1, 3)\n",
      "        upscale_factor = random.randint(2, 5)\n",
      "        channels = random.randint(1, 4) * upscale_factor ** 2\n",
      "        height = random.randint(5, 10)\n",
      "        width = random.randint(5, 10)\n",
      "\n",
      "        input = Variable(torch.Tensor(batch_size, channels, height, width).uniform_(), requires_grad=True)\n",
      "        ps = nn.PixelShuffle(upscale_factor)\n",
      "        output = ps(input)\n",
      "        self._verify_pixel_shuffle(input.data, output.data, upscale_factor)\n",
      "        output.backward(output.data)\n",
      "        self.assertEqual(input.data, input.grad.data)\n",
      "\n",
      "    def test_bce_with_logits_raises_if_target_and_input_are_different_size(self):\n",
      "        target = Variable(torch.rand(5))\n",
      "        input = Variable(torch.rand(5, 1))\n",
      "        with self.assertRaises(ValueError):\n",
      "            nn.BCEWithLogitsLoss()(input, target)\n",
      "\n",
      "        target = Variable(torch.rand(5, 1))\n",
      "        input = Variable(torch.rand(5))\n",
      "        with self.assertRaises(ValueError):\n",
      "            nn.BCEWithLogitsLoss()(input, target)\n",
      "\n",
      "    def test_bce_with_logits_gives_same_result_as_sigmooid_and_bce_loss(self):\n",
      "        sigmoid = nn.Sigmoid()\n",
      "\n",
      "        target = Variable(torch.rand(64, 4))\n",
      "        output = Variable(torch.rand(64, 4) - 0.5)\n",
      "\n",
      "        self.assertEqual(nn.BCEWithLogitsLoss()(output, target), nn.BCELoss()(sigmoid(output), target))\n",
      "\n",
      "        weight = torch.rand(4)\n",
      "        self.assertEqual(nn.BCEWithLogitsLoss(weight)(output, target), nn.BCELoss(weight)(sigmoid(output), target))\n",
      "\n",
      "    def test_bce_with_logits_broadcasts_weights(self):\n",
      "        target = Variable(torch.rand(16, 4))\n",
      "        output = Variable(torch.rand(16, 4) - 0.5)\n",
      "\n",
      "        weight = torch.rand(4)\n",
      "        out1 = nn.BCEWithLogitsLoss(weight)(output, target)\n",
      "\n",
      "        weight = weight.expand(16, 4).contiguous()\n",
      "        out2 = nn.BCEWithLogitsLoss(weight)(output, target)\n",
      "\n",
      "        self.assertEqual(out1, out2)\n",
      "\n",
      "        weight = torch.rand(16, 1)\n",
      "        out1 = nn.BCEWithLogitsLoss(weight)(output, target)\n",
      "\n",
      "        weight = weight.expand(16, 4).contiguous()\n",
      "        out2 = nn.BCEWithLogitsLoss(weight)(output, target)\n",
      "\n",
      "        self.assertEqual(out1, out2)\n",
      "\n",
      "    def test_bce_loss_broadcasts_weights(self):\n",
      "        sigmoid = nn.Sigmoid()\n",
      "        target = Variable(torch.rand(16, 4))\n",
      "        output = Variable(torch.rand(16, 4) - 0.5)\n",
      "\n",
      "        weight = torch.rand(4)\n",
      "        out1 = nn.BCELoss(weight)(sigmoid(output), target)\n",
      "\n",
      "        weight = weight.expand(16, 4).contiguous()\n",
      "        out2 = nn.BCELoss(weight)(sigmoid(output), target)\n",
      "\n",
      "        self.assertEqual(out1, out2)\n",
      "\n",
      "        weight = torch.rand(16, 1)\n",
      "        out1 = nn.BCELoss(weight)(sigmoid(output), target)\n",
      "\n",
      "        weight = weight.expand(16, 4).contiguous()\n",
      "        out2 = nn.BCELoss(weight)(sigmoid(output), target)\n",
      "\n",
      "        self.assertEqual(out1, out2)\n",
      "\n",
      "    def test_batchnorm_raises_error_if_running_mean_is_not_same_size_as_input(self):\n",
      "        input = Variable(torch.rand(2, 10))\n",
      "        running_var = torch.rand(10)\n",
      "        wrong_sizes = [9, 11]\n",
      "        for size in wrong_sizes:\n",
      "            with self.assertRaises(RuntimeError):\n",
      "                F.batch_norm(input, torch.rand(size), running_var)\n",
      "\n",
      "    def test_batchnorm_raises_error_if_running_var_is_not_same_size_as_input(self):\n",
      "        input = Variable(torch.rand(2, 10))\n",
      "        running_mean = torch.rand(10)\n",
      "        wrong_sizes = [9, 11]\n",
      "        for size in wrong_sizes:\n",
      "            with self.assertRaises(RuntimeError):\n",
      "                F.batch_norm(input, running_mean, torch.rand(size))\n",
      "\n",
      "    def test_batchnorm_raises_error_if_weight_is_not_same_size_as_input(self):\n",
      "        input = Variable(torch.rand(2, 10))\n",
      "        running_mean = torch.rand(10)\n",
      "        running_var = torch.rand(10)\n",
      "        wrong_sizes = [9, 11]\n",
      "        for size in wrong_sizes:\n",
      "            with self.assertRaises(RuntimeError):\n",
      "                F.batch_norm(input, running_mean, running_var, weight=Parameter(torch.rand(size)))\n",
      "\n",
      "    def test_batchnorm_raises_error_if_bias_is_not_same_size_as_input(self):\n",
      "        input = Variable(torch.rand(2, 10))\n",
      "        running_mean = torch.rand(10)\n",
      "        running_var = torch.rand(10)\n",
      "        wrong_sizes = [9, 11]\n",
      "        for size in wrong_sizes:\n",
      "            with self.assertRaises(RuntimeError):\n",
      "                F.batch_norm(input, running_mean, running_var, bias=Parameter(torch.rand(size)))\n",
      "\n",
      "    def test_batchnorm_eval(self):\n",
      "        types = (torch.FloatTensor,)\n",
      "        if TEST_CUDA:\n",
      "            types += (torch.cuda.FloatTensor,)\n",
      "        for tp in types:\n",
      "            module = nn.BatchNorm1d(3).type(tp)\n",
      "            module.eval()\n",
      "\n",
      "            data = Variable(torch.rand(4, 3).type(tp), requires_grad=True)\n",
      "            grad = torch.rand(4, 3).type(tp)\n",
      "\n",
      "            # 1st pass\n",
      "            res1 = module(data)\n",
      "            res1.backward(grad)\n",
      "            grad1 = data.grad.data.clone()\n",
      "\n",
      "            # 2nd pass\n",
      "            if data.grad is not None:\n",
      "                data.grad.data.zero_()\n",
      "\n",
      "            res2 = module(data)\n",
      "            res2.backward(grad)\n",
      "            grad2 = data.grad.data.clone()\n",
      "            self.assertEqual(res1, res2)\n",
      "            self.assertEqual(grad1, grad2)\n",
      "\n",
      "    def test_pairwise_distance(self):\n",
      "        input1 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input2 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x, y: F.pairwise_distance(x, y), (input1, input2)))\n",
      "\n",
      "    def test_triplet_margin_loss(self):\n",
      "        input1 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input2 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input3 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x1, x2, x3: F.triplet_margin_loss(\n",
      "            x1, x2, x3), (input1, input2, input3)))\n",
      "\n",
      "    def test_triplet_margin_swap_loss(self):\n",
      "        input1 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input2 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input3 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x1, x2, x3: F.triplet_margin_loss(\n",
      "            x1, x2, x3, swap=True), (input1, input2, input3)))\n",
      "\n",
      "    def test_cosine_similarity(self):\n",
      "        input1 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        input2 = Variable(torch.randn(4, 4), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x, y: F.cosine_similarity(x, y), (input1, input2)))\n",
      "\n",
      "        input1 = Variable(torch.randn(4, 5, 6), requires_grad=True)\n",
      "        input2 = Variable(torch.randn(4, 5, 6), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x, y: F.cosine_similarity(x, y, dim=0), (input1, input2)))\n",
      "        self.assertTrue(gradcheck(lambda x, y: F.cosine_similarity(x, y, dim=-1), (input1, input2)))\n",
      "\n",
      "    def test_grid_sample(self):\n",
      "        # test known input on CPU\n",
      "        input = Variable(torch.arange(1, 11).view(1, 1, 2, 5))\n",
      "        grid = Variable(torch.Tensor(\n",
      "            [[-1, -0.5, 0, 0.2, 1],\n",
      "             [-1, -0.333, 0, 0.5, 1],\n",
      "             [-1, -0.5, 0, 0.3333, 1],\n",
      "             [-1, -0.2, 0, 0.2, 1]]).view(1, 2, 5, 2))\n",
      "        output = F.grid_sample(input, grid)\n",
      "        groundtruth = torch.Tensor(\n",
      "            [[2.2500, 6.0000000000, 5.0000, 4.8340, 9.0000],\n",
      "             [2.2500, 6.333250045, 5.0000, 5.1000, 8.4000]]).view(1, 1, 2, 5)\n",
      "        self.assertEqual(output.data, groundtruth)\n",
      "\n",
      "        # do gradcheck\n",
      "        N = random.randint(1, 8)\n",
      "        C = random.randint(1, 8)\n",
      "        H = random.randint(1, 8)\n",
      "        W = random.randint(1, 8)\n",
      "        input = Variable(torch.randn(N, C, H, W), requires_grad=True)\n",
      "        grid = Variable(torch.randn(N, H, W, 2), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda inp, grid: F.grid_sample(inp, grid), (input, grid)))\n",
      "\n",
      "        # test CPU against CUDA\n",
      "        if TEST_CUDNN:\n",
      "            def test_shape(N, C, IH, IW, H, W):\n",
      "                input_cpu = Variable(torch.randn(C, N, IH, IW).transpose(0, 1), requires_grad=True)\n",
      "                grid_cpu = Variable(torch.randn(H, N, W, 2).transpose(0, 1), requires_grad=True)\n",
      "                out_cpu = F.grid_sample(input_cpu, grid_cpu)\n",
      "                self.assertTrue(out_cpu.size() == torch.Size([N, C, H, W]))\n",
      "\n",
      "                input_cuda = Variable(input_cpu.data.transpose(0, 1).cuda().transpose(0, 1), requires_grad=True)\n",
      "                grid_cuda = Variable(grid_cpu.data.transpose(0, 1).cuda().transpose(0, 1), requires_grad=True)\n",
      "                out_cuda = F.grid_sample(input_cuda, grid_cuda)\n",
      "                self.assertEqual(out_cpu, out_cuda)\n",
      "\n",
      "                gradients = out_cpu.data.new(out_cpu.size()).normal_()\n",
      "                out_cpu.backward(gradients)\n",
      "                out_cuda.backward(gradients.cuda())\n",
      "                self.assertEqual(input_cpu.grad, input_cuda.grad)\n",
      "                self.assertEqual(grid_cpu.grad, grid_cuda.grad)\n",
      "\n",
      "                # check that zero-dimensional input strides dont error out\n",
      "                base_input = torch.randn(C, IH, IW)\n",
      "                input_cpu = Variable(base_input.expand(input_cuda.size()), requires_grad=True)\n",
      "                grid_cpu = Variable(torch.randn(N, H, W, 2), requires_grad=True)\n",
      "                out_cpu = F.grid_sample(input_cpu, grid_cpu)\n",
      "\n",
      "                input_cuda = Variable(base_input.cuda().expand(input_cuda.size()), requires_grad=True)\n",
      "                grid_cuda = Variable(grid_cpu.data.cuda(), requires_grad=True)\n",
      "                out_cuda = F.grid_sample(input_cuda, grid_cuda)\n",
      "                self.assertEqual(out_cpu, out_cuda)\n",
      "\n",
      "            # test same size output\n",
      "            test_shape(N, C, H, W, H, W)\n",
      "\n",
      "            # test larger output\n",
      "            N = random.randint(1, 8)\n",
      "            C = random.randint(1, 8)\n",
      "            IH = random.randint(1, 8)\n",
      "            IW = random.randint(1, 8)\n",
      "            H = random.randint(IH + 1, 12)\n",
      "            W = random.randint(IH + 1, 12)\n",
      "            test_shape(N, C, IH, IW, H, W)\n",
      "\n",
      "            # test smaller output\n",
      "            N = random.randint(1, 8)\n",
      "            C = random.randint(1, 8)\n",
      "            IH = random.randint(1, 8)\n",
      "            IW = random.randint(1, 8)\n",
      "            H = random.randint(1, IH)\n",
      "            W = random.randint(1, IW)\n",
      "            test_shape(N, C, IH, IW, H, W)\n",
      "\n",
      "    def test_affine_grid(self):\n",
      "        # test known input on CPU\n",
      "        input = Variable(torch.arange(1, 7).view(1, 2, 3))\n",
      "        output = F.affine_grid(input, torch.Size([1, 1, 2, 2]))\n",
      "        groundtruth = torch.Tensor(\n",
      "            [[[0, -3], [2, 5]], [[4, 7], [6, 15]]]).view(1, 2, 2, 2)\n",
      "        self.assertEqual(output.data, groundtruth)\n",
      "\n",
      "        # do gradcheck\n",
      "        N = random.randint(1, 8)\n",
      "        C = random.randint(1, 8)\n",
      "        H = random.randint(1, 8)\n",
      "        W = random.randint(1, 8)\n",
      "        sz = torch.Size([N, C, H, W])\n",
      "        inp = Variable(torch.randn(N, 2, 3), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda inp: F.affine_grid(inp, sz), (inp,)))\n",
      "\n",
      "        # test CPU against CUDA\n",
      "        if TEST_CUDNN:\n",
      "            input_cpu = Variable(torch.randn(N, 2, 3), requires_grad=True)\n",
      "            out_cpu = F.affine_grid(input_cpu, sz)\n",
      "            gradients = torch.randn(out_cpu.size())\n",
      "            out_cpu.backward(gradients)\n",
      "            input_gpu = Variable(input_cpu.data.cuda(), requires_grad=True)\n",
      "            out_cuda = F.affine_grid(input_gpu, sz)\n",
      "            out_cuda.backward(gradients.cuda())\n",
      "            self.assertEqual(out_cpu, out_cuda)\n",
      "            self.assertEqual(input_cpu.grad, input_gpu.grad)\n",
      "\n",
      "    def test_upsamplingNearest2d(self):\n",
      "        m = nn.Upsample(size=4, mode='nearest')\n",
      "        in_t = torch.ones(1, 1, 2, 2)\n",
      "        out_t = m(Variable(in_t))\n",
      "        self.assertEqual(torch.ones(1, 1, 4, 4), out_t.data)\n",
      "\n",
      "        input = Variable(torch.randn(1, 1, 2, 2), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.upsample(x, 4, mode='nearest'), (input,)))\n",
      "\n",
      "    def test_upsamplingBilinear2d(self):\n",
      "        m = nn.Upsample(size=4, mode='bilinear')\n",
      "        in_t = torch.ones(1, 1, 2, 2)\n",
      "        out_t = m(Variable(in_t))\n",
      "        self.assertEqual(torch.ones(1, 1, 4, 4), out_t.data)\n",
      "\n",
      "        input = Variable(torch.randn(1, 1, 2, 2), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.upsample(x, 4, mode='bilinear'), (input,)))\n",
      "\n",
      "    def test_upsamplingNearest3d(self):\n",
      "        m = nn.Upsample(size=4, mode='nearest')\n",
      "        in_t = torch.ones(1, 1, 2, 2, 2)\n",
      "        out_t = m(Variable(in_t))\n",
      "        self.assertEqual(torch.ones(1, 1, 4, 4, 4), out_t.data)\n",
      "\n",
      "        input = Variable(torch.randn(1, 1, 2, 2, 2), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.upsample(x, 4, mode='nearest'), (input,)))\n",
      "\n",
      "    def test_upsamplingTrilinear3d(self):\n",
      "        m = nn.Upsample(size=4, mode='trilinear')\n",
      "        in_t = torch.ones(1, 1, 2, 2, 2)\n",
      "        out_t = m(Variable(in_t))\n",
      "        self.assertEqual(torch.ones(1, 1, 4, 4, 4), out_t.data)\n",
      "\n",
      "        input = Variable(torch.randn(1, 1, 2, 2, 2), requires_grad=True)\n",
      "        self.assertTrue(gradcheck(lambda x: F.upsample(x, 4, mode='trilinear'), (input,)))\n",
      "\n",
      "    def test_linear_broadcasting(self):\n",
      "        m = nn.Linear(5, 8)\n",
      "        inp = Variable(torch.randn(2, 3, 5))\n",
      "        expected = m(inp.view(6, 5)).view(2, 3, 8)\n",
      "        self.assertEqual(expected, m(inp))\n",
      "\n",
      "    def test_bilinear(self):\n",
      "        module = nn.Bilinear(10, 10, 8)\n",
      "        module_legacy = legacy.Bilinear(10, 10, 8)\n",
      "\n",
      "        module_legacy.weight.copy_(module.weight.data)\n",
      "        module_legacy.bias.copy_(module.bias.data)\n",
      "\n",
      "        input1 = torch.randn(4, 10)\n",
      "        input2 = torch.randn(4, 10)\n",
      "\n",
      "        output = module(Variable(input1), Variable(input2))\n",
      "        output_legacy = module_legacy.forward([input1, input2])\n",
      "\n",
      "        self.assertEqual(output.data, output_legacy)\n",
      "\n",
      "        input1_1 = Variable(input1, requires_grad=True)\n",
      "        input2_1 = Variable(input2, requires_grad=True)\n",
      "\n",
      "        module.zero_grad()\n",
      "        module_legacy.zeroGradParameters()\n",
      "\n",
      "        output = module(input1_1, input2_1)\n",
      "        grad_output = torch.randn(*output.size())\n",
      "        gi1_legacy, gi2_legacy = module_legacy.backward([input1, input2], grad_output)\n",
      "        output.backward(grad_output)\n",
      "        gi1 = input1_1.grad.data.clone()\n",
      "        gi2 = input2_1.grad.data.clone()\n",
      "\n",
      "        self.assertEqual(gi1, gi1_legacy)\n",
      "        self.assertEqual(gi2, gi2_legacy)\n",
      "        self.assertEqual(module.weight.grad.data, module_legacy.gradWeight)\n",
      "        self.assertEqual(module.bias.grad.data, module_legacy.gradBias)\n",
      "\n",
      "        self._assertGradAndGradgradChecks(lambda x1, x2: F.bilinear(x1, x2, module.weight, module.bias),\n",
      "                                          (input1_1, input2_1))\n",
      "\n",
      "    def run_conv_double_back_test(self, kern, stride, padding, chan_in, chan_out, batch_size,\n",
      "                                  inp_size, dilation, no_weight, groups=1, use_cuda=False, use_bias=True):\n",
      "        tensor = torch.Tensor(1)\n",
      "        if use_cuda:\n",
      "            tensor = tensor.cuda()\n",
      "\n",
      "        x = Variable(tensor.new(batch_size, chan_in, inp_size, inp_size), requires_grad=True)\n",
      "        x.data.normal_()\n",
      "        weight = Variable(tensor.new(chan_out, chan_in // groups, kern, kern), requires_grad=True)\n",
      "        weight.data.normal_()\n",
      "        if use_bias:\n",
      "            bias = Variable(tensor.new(chan_out), requires_grad=True)\n",
      "            bias.data.normal_()\n",
      "        else:\n",
      "            bias = None\n",
      "\n",
      "        def func(*inputs):\n",
      "            if no_weight:\n",
      "                lweight = weight\n",
      "                if use_bias:\n",
      "                    lx, lbias = inputs\n",
      "                else:\n",
      "                    lx, = inputs\n",
      "                    lbias = None\n",
      "            else:\n",
      "                if use_bias:\n",
      "                    lx, lweight, lbias = inputs\n",
      "                else:\n",
      "                    lx, lweight = inputs\n",
      "                    lbias = None\n",
      "            # We disable cudnn during forward to avoid finite difference imprecision issues\n",
      "            with use_cudnn(False):\n",
      "                out = F.conv2d(lx, lweight, lbias, stride, padding, dilation, groups)\n",
      "            return out\n",
      "\n",
      "        if no_weight:\n",
      "            inputs = (x, bias)\n",
      "        else:\n",
      "            inputs = (x, weight, bias)\n",
      "\n",
      "        if not use_bias:\n",
      "            inputs = inputs[:-1]\n",
      "\n",
      "        dummy_out = func(*inputs)\n",
      "        grad_y = Variable(tensor.new(dummy_out.size()), requires_grad=True)\n",
      "        grad_y.data.normal_()\n",
      "\n",
      "        return gradgradcheck(func, inputs, (grad_y,))\n",
      "\n",
      "    def test_conv_double_backward(self):\n",
      "        batch_size = 2\n",
      "        for kern, inp_size, dilations in [(3, 6, [1, 2]), (3, 7, [1]), (4, 9, [1])]:\n",
      "            for stride, padding, chan_in, chan_out, dilation in \\\n",
      "                    product([1, 2], [0, 2], [2], [3], dilations):\n",
      "                no_weight = stride == 2\n",
      "                result = self.run_conv_double_back_test(kern, stride,\n",
      "                                                        padding, chan_in, chan_out,\n",
      "                                                        batch_size, inp_size, dilation,\n",
      "                                                        no_weight)\n",
      "                self.assertTrue(result,\n",
      "                                \"Conv double backward test failed with parameters:\" +\n",
      "                                \"\\nkern: \" + str(kern) +\n",
      "                                \"\\nstride: \" + str(stride) +\n",
      "                                \"\\npadding: \" + str(padding) +\n",
      "                                \"\\nchan_in: \" + str(chan_in) +\n",
      "                                \"\\nchan_out: \" + str(chan_out) +\n",
      "                                \"\\nbatch_size: \" + str(batch_size) +\n",
      "                                \"\\ninp_size: \" + str(inp_size) +\n",
      "                                \"\\ndilation: \" + str(dilation))\n",
      "\n",
      "    def test_conv_double_backward_no_bias(self):\n",
      "        kern = 3\n",
      "        stride = 1\n",
      "        padding = 2\n",
      "        chan_in, chan_out = 2, 4\n",
      "        batch_size = 2\n",
      "        inp_size = 6\n",
      "        dilation = 1\n",
      "        no_weight = False\n",
      "        use_bias = True\n",
      "        result = self.run_conv_double_back_test(kern, stride,\n",
      "                                                padding, chan_in, chan_out,\n",
      "                                                batch_size, inp_size, dilation,\n",
      "                                                no_weight, use_bias=use_bias)\n",
      "        self.assertTrue(result,\n",
      "                        \"Conv double backward test failed with parameters:\" +\n",
      "                        \"\\nkern: \" + str(kern) +\n",
      "                        \"\\nstride: \" + str(stride) +\n",
      "                        \"\\npadding: \" + str(padding) +\n",
      "                        \"\\nchan_in: \" + str(chan_in) +\n",
      "                        \"\\nchan_out: \" + str(chan_out) +\n",
      "                        \"\\nbatch_size: \" + str(batch_size) +\n",
      "                        \"\\ninp_size: \" + str(inp_size) +\n",
      "                        \"\\ndilation: \" + str(dilation))\n",
      "\n",
      "    def test_conv_double_backward_groups(self):\n",
      "        kern = 3\n",
      "        stride = 1\n",
      "        padding = 2\n",
      "        chan_in, chan_out = 2, 4\n",
      "        batch_size = 2\n",
      "        inp_size = 6\n",
      "        dilation = 1\n",
      "        no_weight = False\n",
      "        groups = 2\n",
      "        result = self.run_conv_double_back_test(kern, stride,\n",
      "                                                padding, chan_in * groups, chan_out * groups,\n",
      "                                                batch_size, inp_size, dilation,\n",
      "                                                no_weight, groups=groups)\n",
      "        self.assertTrue(result,\n",
      "                        \"Conv double backward test failed with parameters:\" +\n",
      "                        \"\\nkern: \" + str(kern) +\n",
      "                        \"\\nstride: \" + str(stride) +\n",
      "                        \"\\npadding: \" + str(padding) +\n",
      "                        \"\\nchan_in: \" + str(chan_in) +\n",
      "                        \"\\nchan_out: \" + str(chan_out) +\n",
      "                        \"\\nbatch_size: \" + str(batch_size) +\n",
      "                        \"\\ninp_size: \" + str(inp_size) +\n",
      "                        \"\\ndilation: \" + str(dilation) +\n",
      "                        \"\\ngroups: \" + str(groups))\n",
      "\n",
      "    def test_error_conv_double_backward(self):\n",
      "        batch_size = 2\n",
      "\n",
      "        # Cannot provide ggW when stride is > 1\n",
      "        for kern, inp_size, dilations in [(3, 5, [1, 2]), (3, 7, [1])]:\n",
      "            for stride, padding, chan_in, chan_out, dilation in product([2], [0, 1], [1], [2], dilations):\n",
      "                no_weight = False\n",
      "                with self.assertRaises(RuntimeError):\n",
      "                    self.run_conv_double_back_test(kern, stride,\n",
      "                                                   padding, chan_in, chan_out,\n",
      "                                                   batch_size, inp_size, dilation,\n",
      "                                                   no_weight)\n",
      "\n",
      "    @unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\n",
      "    def test_conv_double_backward_cuda(self):\n",
      "        batch_size = 1\n",
      "        for kern, inp_size, dilations in [(3, 5, [1, 2]), (4, 9, [1])]:\n",
      "            for stride, padding, chan_in, chan_out, dilation in product([1], [2], [2], [3], dilations):\n",
      "                no_weight = stride == 2\n",
      "                result = self.run_conv_double_back_test(kern, stride,\n",
      "                                                        padding, chan_in, chan_out,\n",
      "                                                        batch_size, inp_size, dilation,\n",
      "                                                        no_weight, use_cuda=True)\n",
      "                self.assertTrue(result,\n",
      "                                \"Conv double backward test failed with parameters:\" +\n",
      "                                \"\\nkern: \" + str(kern) +\n",
      "                                \"\\nstride: \" + str(stride) +\n",
      "                                \"\\npadding: \" + str(padding) +\n",
      "                                \"\\nchan_in: \" + str(chan_in) +\n",
      "                                \"\\nchan_out: \" + str(chan_out) +\n",
      "                                \"\\nbatch_size: \" + str(batch_size) +\n",
      "                                \"\\ninp_size: \" + str(inp_size) +\n",
      "                                \"\\ndilation: \" + str(dilation))\n",
      "\n",
      "\n",
      "class TestNNInit(TestCase):\n",
      "    def setUp(self):\n",
      "        random.seed(123)\n",
      "        torch.manual_seed(123)\n",
      "\n",
      "    def _is_normal(self, tensor, mean, std):\n",
      "        if isinstance(tensor, Variable):\n",
      "            tensor = tensor.data\n",
      "        samples = list(tensor.view(-1))\n",
      "        p_value = stats.kstest(samples, 'norm', args=(mean, std)).pvalue\n",
      "        return p_value > 0.0001\n",
      "\n",
      "    def _is_uniform(self, tensor, a, b):\n",
      "        if isinstance(tensor, Variable):\n",
      "            tensor = tensor.data\n",
      "        samples = list(tensor.view(-1))\n",
      "        p_value = stats.kstest(samples, 'uniform', args=(a, (b - a))).pvalue\n",
      "        return p_value > 0.0001\n",
      "\n",
      "    def _create_random_nd_tensor(self, dims, size_min, size_max, as_variable):\n",
      "        size = [random.randint(size_min, size_max) for _ in range(dims)]\n",
      "        tensor = torch.zeros(size)\n",
      "        if as_variable:\n",
      "            tensor = Variable(tensor)\n",
      "        return tensor\n",
      "\n",
      "    def _random_float(self, a, b):\n",
      "        return (b - a) * random.random() + a\n",
      "\n",
      "    def test_calculate_gain_linear(self):\n",
      "        for fn in ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose2d', 'conv_transpose2d', 'conv_transpose3d']:\n",
      "            gain = init.calculate_gain(fn)\n",
      "            self.assertEqual(gain, 1)\n",
      "\n",
      "    def test_calculate_gain_nonlinear(self):\n",
      "        for fn in ['sigmoid', 'tanh', 'relu', 'leaky_relu']:\n",
      "            gain = init.calculate_gain(fn)\n",
      "            if fn == 'sigmoid':\n",
      "                self.assertEqual(gain, 1)\n",
      "            elif fn == 'tanh':  # 5 / 3\n",
      "                self.assertEqual(gain, 1.6666666666666667)\n",
      "            elif fn == 'relu':  # sqrt(2)\n",
      "                self.assertEqual(gain, 1.4142135623730951)\n",
      "            elif fn == 'leaky_relu':  # sqrt(2 / 1 + slope^2))\n",
      "                self.assertEqual(gain, 1.4141428569978354)\n",
      "\n",
      "    def test_calculate_gain_leaky_relu(self):\n",
      "        for param in [None, 0, 0.01, 10]:\n",
      "            gain = init.calculate_gain('leaky_relu', param)\n",
      "            if param is None:  # Default slope is 0.01\n",
      "                self.assertEqual(gain, 1.4141428569978354)\n",
      "            elif param == 0:  # No slope = same gain as normal ReLU\n",
      "                self.assertEqual(gain, 1.4142135623730951)\n",
      "            elif param == 0.01:\n",
      "                self.assertEqual(gain, 1.4141428569978354)\n",
      "            elif param == 10:\n",
      "                self.assertEqual(gain, 0.14071950894605836)\n",
      "\n",
      "    def test_calculate_gain_leaky_relu_only_accepts_numbers(self):\n",
      "        for param in [True, [1], {'a': 'b'}]:\n",
      "            with self.assertRaises(ValueError):\n",
      "                init.calculate_gain('leaky_relu', param)\n",
      "\n",
      "    def test_calculate_gain_only_accepts_valid_nonlinearities(self):\n",
      "        for n in [2, 5, 25]:\n",
      "            # Generate random strings of lengths that definitely aren't supported\n",
      "            random_string = ''.join([random.choice(string.ascii_lowercase) for i in range(n)])\n",
      "            with self.assertRaises(ValueError):\n",
      "                init.calculate_gain(random_string)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_uniform(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 2, 4]:\n",
      "                input_tensor = self._create_random_nd_tensor(dims, size_min=30, size_max=50, as_variable=as_variable)\n",
      "                a = self._random_float(-3, 3)\n",
      "                b = a + self._random_float(1, 5)\n",
      "                init.uniform(input_tensor, a=a, b=b)\n",
      "                assert self._is_uniform(input_tensor, a, b)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_normal(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 2, 4]:\n",
      "                input_tensor = self._create_random_nd_tensor(dims, size_min=30, size_max=50, as_variable=as_variable)\n",
      "                mean = self._random_float(-3, 3)\n",
      "                std = self._random_float(1, 5)\n",
      "                init.normal(input_tensor, mean=mean, std=std)\n",
      "\n",
      "                assert self._is_normal(input_tensor, mean, std)\n",
      "\n",
      "    def test_constant(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 2, 4]:\n",
      "                input_tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=5, as_variable=as_variable)\n",
      "                val = self._random_float(1, 10)\n",
      "                init.constant(input_tensor, val)\n",
      "                if as_variable:\n",
      "                    input_tensor = input_tensor.data\n",
      "\n",
      "                self.assertEqual(input_tensor, input_tensor.clone().fill_(val))\n",
      "\n",
      "    def test_eye(self):\n",
      "        for as_variable in [True, False]:\n",
      "            input_tensor = self._create_random_nd_tensor(2, size_min=1, size_max=5, as_variable=as_variable)\n",
      "            init.eye(input_tensor)\n",
      "            if as_variable:\n",
      "                input_tensor = input_tensor.data\n",
      "\n",
      "            # Check every single element\n",
      "            for i in range(input_tensor.size(0)):\n",
      "                for j in range(input_tensor.size(1)):\n",
      "                    if i == j:\n",
      "                        assert input_tensor[i][j] == 1\n",
      "                    else:\n",
      "                        assert input_tensor[i][j] == 0\n",
      "\n",
      "    def test_eye_only_works_on_2d_inputs(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 3]:\n",
      "                with self.assertRaises(ValueError):\n",
      "                    tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=3, as_variable=as_variable)\n",
      "                    init.eye(tensor)\n",
      "\n",
      "    def test_dirac_properties(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [3, 4, 5]:\n",
      "                input_tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=5, as_variable=as_variable)\n",
      "                init.dirac(input_tensor)\n",
      "                if as_variable:\n",
      "                    input_tensor = input_tensor.data\n",
      "\n",
      "                c_out, c_in = input_tensor.size(0), input_tensor.size(1)\n",
      "                min_d = min(c_out, c_in)\n",
      "                # Check number of nonzeros is equivalent to smallest dim\n",
      "                assert torch.nonzero(input_tensor).size(0) == min_d\n",
      "                # Check sum of values (can have precision issues, hence assertEqual) is also equivalent\n",
      "                self.assertEqual(input_tensor.sum(), min_d)\n",
      "\n",
      "    def test_dirac_identity(self):\n",
      "        batch, in_c, out_c, size, kernel_size = 8, 3, 4, 5, 3\n",
      "        # Test 1D\n",
      "        input_var = Variable(torch.randn(batch, in_c, size))\n",
      "        filter_var = Variable(torch.zeros(out_c, in_c, kernel_size))\n",
      "        init.dirac(filter_var)\n",
      "        output_var = F.conv1d(input_var, filter_var)\n",
      "        input_tensor, output_tensor = input_var.data, output_var.data  # Variables do not support nonzero\n",
      "        self.assertEqual(input_tensor[:, :, 1:-1], output_tensor[:, :in_c, :])  # Assert in_c outputs are preserved\n",
      "        assert torch.nonzero(output_tensor[:, in_c:, :]).numel() == 0  # Assert extra outputs are 0\n",
      "\n",
      "        # Test 2D\n",
      "        input_var = Variable(torch.randn(batch, in_c, size, size))\n",
      "        filter_var = Variable(torch.zeros(out_c, in_c, kernel_size, kernel_size))\n",
      "        init.dirac(filter_var)\n",
      "        output_var = F.conv2d(input_var, filter_var)\n",
      "        input_tensor, output_tensor = input_var.data, output_var.data\n",
      "        self.assertEqual(input_tensor[:, :, 1:-1, 1:-1], output_tensor[:, :in_c, :, :])\n",
      "        assert torch.nonzero(output_tensor[:, in_c:, :, :]).numel() == 0\n",
      "\n",
      "        # Test 3D\n",
      "        input_var = Variable(torch.randn(batch, in_c, size, size, size))\n",
      "        filter_var = Variable(torch.zeros(out_c, in_c, kernel_size, kernel_size, kernel_size))\n",
      "        init.dirac(filter_var)\n",
      "        output_var = F.conv3d(input_var, filter_var)\n",
      "        input_tensor, output_tensor = input_var.data, output_var.data\n",
      "        self.assertEqual(input_tensor[:, :, 1:-1, 1:-1, 1:-1], output_tensor[:, :in_c, :, :])\n",
      "        assert torch.nonzero(output_tensor[:, in_c:, :, :, :]).numel() == 0\n",
      "\n",
      "    def test_dirac_only_works_on_3_4_5d_inputs(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 2, 6]:\n",
      "                with self.assertRaises(ValueError):\n",
      "                    tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=3, as_variable=as_variable)\n",
      "                    init.dirac(tensor)\n",
      "\n",
      "    def test_xavier_uniform_errors_on_inputs_smaller_than_2d(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [0, 1]:\n",
      "                tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=1, as_variable=as_variable)\n",
      "                with self.assertRaises(ValueError):\n",
      "                    init.xavier_uniform(tensor)\n",
      "\n",
      "    def test_xavier_normal_errors_on_inputs_smaller_than_2d(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [0, 1]:\n",
      "                tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=1, as_variable=as_variable)\n",
      "                with self.assertRaises(ValueError):\n",
      "                    init.xavier_normal(tensor)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_xavier_uniform(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_gain in [True, False]:\n",
      "                for dims in [2, 4]:\n",
      "                    input_tensor = self._create_random_nd_tensor(dims, size_min=20, size_max=25,\n",
      "                                                                 as_variable=as_variable)\n",
      "                    gain = 1\n",
      "\n",
      "                    if use_gain:\n",
      "                        gain = self._random_float(0.1, 2)\n",
      "                        init.xavier_uniform(input_tensor, gain=gain)\n",
      "                    else:\n",
      "                        init.xavier_uniform(input_tensor)\n",
      "\n",
      "                    if as_variable:\n",
      "                        input_tensor = input_tensor.data\n",
      "\n",
      "                    fan_in = input_tensor.size(1)\n",
      "                    fan_out = input_tensor.size(0)\n",
      "                    if input_tensor.dim() > 2:\n",
      "                        fan_in *= input_tensor[0, 0].numel()\n",
      "                        fan_out *= input_tensor[0, 0].numel()\n",
      "\n",
      "                    expected_std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
      "                    bounds = expected_std * math.sqrt(3)\n",
      "                    assert self._is_uniform(input_tensor, -bounds, bounds)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_xavier_normal(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_gain in [True, False]:\n",
      "                for dims in [2, 4]:\n",
      "                    input_tensor = self._create_random_nd_tensor(dims, size_min=20, size_max=25,\n",
      "                                                                 as_variable=as_variable)\n",
      "                    gain = 1\n",
      "\n",
      "                    if use_gain:\n",
      "                        gain = self._random_float(0.1, 2)\n",
      "                        init.xavier_normal(input_tensor, gain=gain)\n",
      "                    else:\n",
      "                        init.xavier_normal(input_tensor)\n",
      "\n",
      "                    if as_variable:\n",
      "                        input_tensor = input_tensor.data\n",
      "\n",
      "                    fan_in = input_tensor.size(1)\n",
      "                    fan_out = input_tensor.size(0)\n",
      "                    if input_tensor.dim() > 2:\n",
      "                        fan_in *= input_tensor[0, 0].numel()\n",
      "                        fan_out *= input_tensor[0, 0].numel()\n",
      "\n",
      "                    expected_std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
      "                    assert self._is_normal(input_tensor, 0, expected_std)\n",
      "\n",
      "    def test_kaiming_uniform_errors_on_inputs_smaller_than_2d(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [0, 1]:\n",
      "                with self.assertRaises(ValueError):\n",
      "                    tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=1, as_variable=as_variable)\n",
      "                    init.kaiming_uniform(tensor)\n",
      "\n",
      "    def test_kaiming_normal_errors_on_inputs_smaller_than_2d(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [0, 1]:\n",
      "                with self.assertRaises(ValueError):\n",
      "                    tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=1, as_variable=as_variable)\n",
      "                    init.kaiming_normal(tensor)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_kaiming_uniform(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_a in [True, False]:\n",
      "                for dims in [2, 4]:\n",
      "                    for mode in ['fan_in', 'fan_out']:\n",
      "                        input_tensor = self._create_random_nd_tensor(dims, size_min=20, size_max=25,\n",
      "                                                                     as_variable=as_variable)\n",
      "                        if use_a:\n",
      "                            a = self._random_float(0.1, 2)\n",
      "                            init.kaiming_uniform(input_tensor, a=a, mode=mode)\n",
      "                        else:\n",
      "                            a = 0\n",
      "                            init.kaiming_uniform(input_tensor, mode=mode)\n",
      "\n",
      "                        if as_variable:\n",
      "                            input_tensor = input_tensor.data\n",
      "\n",
      "                        fan_in = input_tensor.size(1)\n",
      "                        fan_out = input_tensor.size(0)\n",
      "                        if input_tensor.dim() > 2:\n",
      "                            fan_in *= input_tensor[0, 0].numel()\n",
      "                            fan_out *= input_tensor[0, 0].numel()\n",
      "\n",
      "                        if mode == 'fan_in':\n",
      "                            n = fan_in\n",
      "                        else:\n",
      "                            n = fan_out\n",
      "\n",
      "                        expected_std = math.sqrt(2.0 / ((1 + a**2) * n))\n",
      "                        bounds = expected_std * math.sqrt(3.0)\n",
      "                        assert self._is_uniform(input_tensor, -bounds, bounds)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_kaiming_normal(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_a in [True, False]:\n",
      "                for dims in [2, 4]:\n",
      "                    for mode in ['fan_in', 'fan_out']:\n",
      "                        input_tensor = self._create_random_nd_tensor(dims, size_min=20, size_max=25,\n",
      "                                                                     as_variable=as_variable)\n",
      "                        if use_a:\n",
      "                            a = self._random_float(0.1, 2)\n",
      "                            init.kaiming_normal(input_tensor, a=a, mode=mode)\n",
      "                        else:\n",
      "                            a = 0\n",
      "                            init.kaiming_normal(input_tensor, mode=mode)\n",
      "\n",
      "                        if as_variable:\n",
      "                            input_tensor = input_tensor.data\n",
      "\n",
      "                        fan_in = input_tensor.size(1)\n",
      "                        fan_out = input_tensor.size(0)\n",
      "                        if input_tensor.dim() > 2:\n",
      "                            fan_in *= input_tensor[0, 0].numel()\n",
      "                            fan_out *= input_tensor[0, 0].numel()\n",
      "\n",
      "                        if mode == 'fan_in':\n",
      "                            n = fan_in\n",
      "                        else:\n",
      "                            n = fan_out\n",
      "\n",
      "                        expected_std = math.sqrt(2.0 / ((1 + a**2) * n))\n",
      "                        assert self._is_normal(input_tensor, 0, expected_std)\n",
      "\n",
      "    def test_sparse_only_works_on_2d_inputs(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for dims in [1, 3]:\n",
      "                with self.assertRaises(ValueError):\n",
      "                    sparsity = self._random_float(0.1, 0.9)\n",
      "                    tensor = self._create_random_nd_tensor(dims, size_min=1, size_max=3, as_variable=as_variable)\n",
      "                    init.sparse(tensor, sparsity)\n",
      "\n",
      "    @unittest.skipIf(not TEST_SCIPY, \"Scipy not found.\")\n",
      "    def test_sparse_default_std(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_random_std in [True, False]:\n",
      "                input_tensor = self._create_random_nd_tensor(2, size_min=30, size_max=35, as_variable=as_variable)\n",
      "                rows, cols = input_tensor.size(0), input_tensor.size(1)\n",
      "                sparsity = self._random_float(0.1, 0.2)\n",
      "\n",
      "                std = 0.01  # default std\n",
      "                if use_random_std:\n",
      "                    std = self._random_float(0.01, 0.2)\n",
      "                    init.sparse(input_tensor, sparsity=sparsity, std=std)\n",
      "                else:\n",
      "                    init.sparse(input_tensor, sparsity=sparsity)\n",
      "\n",
      "                if as_variable:\n",
      "                    input_tensor = input_tensor.data\n",
      "\n",
      "                for col_idx in range(input_tensor.size(1)):\n",
      "                    column = input_tensor[:, col_idx]\n",
      "                    assert column[column == 0].nelement() >= math.ceil(sparsity * cols)\n",
      "\n",
      "                assert self._is_normal(input_tensor[input_tensor != 0], 0, std)\n",
      "\n",
      "    @skipIfNoLapack\n",
      "    def test_orthogonal(self):\n",
      "        for as_variable in [True, False]:\n",
      "            for use_gain in [True, False]:\n",
      "                for tensor_size in [[3, 4], [4, 3], [20, 2, 3, 4], [2, 3, 4, 5]]:\n",
      "                    input_tensor = torch.zeros(tensor_size)\n",
      "                    gain = 1.0\n",
      "\n",
      "                    if as_variable:\n",
      "                        input_tensor = Variable(input_tensor)\n",
      "\n",
      "                    if use_gain:\n",
      "                        gain = self._random_float(0.1, 2)\n",
      "                        init.orthogonal(input_tensor, gain=gain)\n",
      "                    else:\n",
      "                        init.orthogonal(input_tensor)\n",
      "\n",
      "                    if as_variable:\n",
      "                        input_tensor = input_tensor.data\n",
      "\n",
      "                    rows, cols = tensor_size[0], reduce(mul, tensor_size[1:])\n",
      "                    flattened_tensor = input_tensor.view(rows, cols)\n",
      "                    if rows > cols:\n",
      "                        self.assertEqual(torch.mm(flattened_tensor.t(), flattened_tensor),\n",
      "                                         torch.eye(cols) * gain ** 2, prec=1e-6)\n",
      "                    else:\n",
      "                        self.assertEqual(torch.mm(flattened_tensor, flattened_tensor.t()),\n",
      "                                         torch.eye(rows) * gain ** 2, prec=1e-6)\n",
      "\n",
      "\n",
      "def add_test(test):\n",
      "    test_name = test.get_name()\n",
      "    cuda_test_name = test_name + '_cuda'\n",
      "    if hasattr(TestNN, test_name):\n",
      "        raise RuntimeError('Found two tests with the same name: ' + test_name)\n",
      "    if hasattr(TestNN, cuda_test_name):\n",
      "        raise RuntimeError('Found two tests with the same name: ' + cuda_test_name)\n",
      "    setattr(TestNN, test_name, lambda self, test=test: test(self))\n",
      "    setattr(TestNN, cuda_test_name, lambda self, test=test: test.test_cuda(self))\n",
      "\n",
      "\n",
      "new_criterion_tests = [\n",
      "    dict(\n",
      "        module_name='BCEWithLogitsLoss',\n",
      "        input=torch.rand(15, 10).clamp_(1e-2, 1 - 1e-2),\n",
      "        target=torch.randn(15, 10).gt(0).double()\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BCEWithLogitsLoss',\n",
      "        constructor_args=(torch.rand(10),),\n",
      "        input=torch.rand(15, 10).clamp_(1e-2, 1 - 1e-2),\n",
      "        target=torch.randn(15, 10).gt(0).double(),\n",
      "        desc='weights'\n",
      "    ),\n",
      "]\n",
      "\n",
      "new_module_tests = [\n",
      "    dict(\n",
      "        module_name='BatchNorm1d',\n",
      "        constructor_args=(10,),\n",
      "        input_size=(4, 10),\n",
      "        cudnn=True,\n",
      "        desc='affine'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm1d',\n",
      "        constructor_args=(5,),\n",
      "        input_size=(4, 5, 3),\n",
      "        cudnn=True,\n",
      "        desc='3d_input'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm1d',\n",
      "        constructor_args=(10, 1e-3, 0.3, False),\n",
      "        input_size=(4, 10),\n",
      "        cudnn=True,\n",
      "        desc='not_affine'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm2d',\n",
      "        constructor_args=(3,),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm2d',\n",
      "        constructor_args=(3, 1e-3, 0.8),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        cudnn=True,\n",
      "        desc='momentum',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm2d',\n",
      "        constructor_args=(3, 1e-3, 0.8, False),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        cudnn=True,\n",
      "        desc='no_affine',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm3d',\n",
      "        constructor_args=(3,),\n",
      "        input_size=(2, 3, 4, 4, 4),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm3d',\n",
      "        constructor_args=(3, 1e-3, 0.7),\n",
      "        input_size=(2, 3, 4, 4, 4),\n",
      "        cudnn=True,\n",
      "        desc='momentum'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='BatchNorm3d',\n",
      "        constructor_args=(3, 1e-3, 0.7, False),\n",
      "        input_size=(2, 3, 4, 4, 4),\n",
      "        cudnn=True,\n",
      "        desc='no_affine'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 5, 3),\n",
      "        input_size=(2, 4, 10),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 5, 3, 2),\n",
      "        input_size=(2, 4, 10),\n",
      "        cudnn=True,\n",
      "        desc='stride'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 5, 3, 1, 1),\n",
      "        input_size=(2, 4, 10),\n",
      "        cudnn=True,\n",
      "        desc='pad1'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 5, 5, 1, 2),\n",
      "        input_size=(2, 4, 10),\n",
      "        cudnn=True,\n",
      "        desc='pad2'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 4, 3, 1, 1),\n",
      "        input_size=(1, 4, 1),\n",
      "        cudnn=True,\n",
      "        desc='pad1size1'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv1d',\n",
      "        constructor_args=(4, 4, 5, 1, 2),\n",
      "        input_size=(1, 4, 1),\n",
      "        cudnn=True,\n",
      "        desc='pad2size1'\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv1d_dilated',\n",
      "        constructor=lambda: nn.Conv1d(4, 5, kernel_size=3, dilation=2),\n",
      "        input_size=(2, 4, 10),\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv1d_groups',\n",
      "        constructor=lambda: nn.Conv1d(4, 6, kernel_size=3, groups=2),\n",
      "        input_size=(2, 4, 6),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ConvTranspose1d',\n",
      "        constructor_args=(3, 4, 3, (3,), 1, (1,)),\n",
      "        cudnn=True,\n",
      "        input_size=(1, 3, 7)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ConvTranspose1d',\n",
      "        constructor_args=(3, 4, 3, 2, 1, 1, 1, False),\n",
      "        input_size=(1, 3, 6),\n",
      "        cudnn=True,\n",
      "        desc='no_bias'\n",
      "    ),\n",
      "    # TODO\n",
      "    # dict(\n",
      "    #     module_name='ConvTranspose1d',\n",
      "    #     constructor_args=(3, 4, 3, 2, 1, 1, 1, True, 2),\n",
      "    #     input_size=(1, 3, 6),\n",
      "    #     cudnn=True,\n",
      "    #     desc='dilated'\n",
      "    # ),\n",
      "    dict(\n",
      "        module_name='MaxPool1d',\n",
      "        constructor_args=(4,),\n",
      "        input_size=(2, 10, 4)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='MaxPool1d',\n",
      "        constructor_args=(4, 4),\n",
      "        input_size=(2, 10, 4),\n",
      "        desc='stride'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv2d',\n",
      "        constructor_args=(3, 4, (3, 2)),\n",
      "        input_size=(2, 3, 7, 5),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv2d',\n",
      "        constructor_args=(3, 4, (3, 3), (2, 2)),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        cudnn=True,\n",
      "        desc='strided'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv2d',\n",
      "        constructor_args=(3, 4, (3, 3), (2, 2), (1, 1)),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        cudnn=True,\n",
      "        desc='padding'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv2d',\n",
      "        constructor_args=(3, 2, (3, 3), (2, 2), (1, 1), (2, 2)),\n",
      "        input_size=(2, 3, 8, 8),\n",
      "        cudnn=True,\n",
      "        desc='dilated'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv2d',\n",
      "        constructor_args=(3, 4, (3, 2), 1, 0, 1, 1, False),\n",
      "        input_size=(2, 3, 6, 5),\n",
      "        cudnn=True,\n",
      "        desc='no_bias',\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv2d_groups',\n",
      "        constructor=lambda: nn.Conv2d(4, 6, (3, 2), groups=2),\n",
      "        input_size=(2, 4, 6, 5),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv2d_groups_thnn',\n",
      "        constructor=lambda: nn.Conv2d(4, 6, (3, 2), groups=2),\n",
      "        input_size=(2, 4, 6, 5),\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ConvTranspose2d',\n",
      "        constructor_args=(3, 4, 3, (3, 2), 1, (1, 1)),\n",
      "        cudnn=True,\n",
      "        input_size=(1, 3, 7, 6)\n",
      "    ),\n",
      "    # TODO\n",
      "    # dict(\n",
      "    #     module_name='ConvTranspose2d',\n",
      "    #     constructor_args=(3, 4, 3, (2, 3), 1, (1, 1), 1, False, (2, 2)),\n",
      "    #     input_size=(1, 3, 6, 7),\n",
      "    #     cudnn=True,\n",
      "    #     desc='dilated'\n",
      "    # ),\n",
      "    dict(\n",
      "        module_name='ConvTranspose2d',\n",
      "        constructor_args=(3, 4, 3, (2, 3), 1, (1, 1), 1, False),\n",
      "        input_size=(1, 3, 6, 7),\n",
      "        cudnn=True,\n",
      "        desc='no_bias'\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='ConvTranspose2d_groups',\n",
      "        constructor=lambda: nn.ConvTranspose2d(2, 4, (2, 3), groups=2),\n",
      "        input_size=(1, 2, 4, 5),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='MaxPool2d',\n",
      "        constructor_args=((3, 3), (2, 2), (1, 1)),\n",
      "        input_size=(1, 3, 7, 7)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool1d',\n",
      "        constructor_args=(2,),\n",
      "        input_size=(2, 3, 6),\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool1d',\n",
      "        constructor_args=((2,), (2,)),\n",
      "        input_size=(2, 3, 6),\n",
      "        desc='stride',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool1d',\n",
      "        constructor_args=(2, 2, 1),\n",
      "        input_size=(2, 3, 6),\n",
      "        desc='stride_pad',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool2d',\n",
      "        constructor_args=((2, 2),),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool2d',\n",
      "        constructor_args=((2, 2), (2, 2)),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        desc='stride',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool2d',\n",
      "        constructor_args=((2, 2), (2, 2), (1, 1)),\n",
      "        input_size=(2, 3, 6, 6),\n",
      "        desc='stride_pad',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='LPPool2d',\n",
      "        constructor_args=(2, (2, 2), 2),\n",
      "        input_size=(1, 3, 7, 7)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='LPPool2d',\n",
      "        constructor_args=(1.5, 2),\n",
      "        input=torch.rand(1, 3, 7, 7),\n",
      "        desc='norm'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ReflectionPad2d',\n",
      "        constructor_args=((1, 2, 3, 4),),\n",
      "        input_size=(2, 3, 8, 8)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ReplicationPad2d',\n",
      "        constructor_args=((1, 2, 3, 4),),\n",
      "        input_size=(2, 3, 4, 4)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ZeroPad2d',\n",
      "        constructor_args=((1, 2, 3, 4),),\n",
      "        input_size=(2, 3, 4, 4)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ConstantPad2d',\n",
      "        constructor_args=((1, 2, 3, 4), 2),\n",
      "        input_size=(2, 3, 4, 4)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv3d',\n",
      "        constructor_args=(3, 4, (2, 3, 4)),\n",
      "        input_size=(2, 3, 3, 4, 5),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv3d',\n",
      "        constructor_args=(3, 4, (2, 3, 4), 1, 0, 1, 1, False),\n",
      "        input_size=(2, 3, 3, 4, 5),\n",
      "        cudnn=True,\n",
      "        desc='no_bias'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv3d',\n",
      "        constructor_args=(3, 4, 2, 2),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "        cudnn=True,\n",
      "        desc='stride'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Conv3d',\n",
      "        constructor_args=(3, 4, 2, 2, 1),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "        cudnn=True,\n",
      "        desc='stride_padding'\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv3d_groups',\n",
      "        constructor=lambda: nn.Conv3d(4, 6, kernel_size=3, groups=2),\n",
      "        input_size=(2, 4, 4, 5, 4),\n",
      "        cudnn=True,\n",
      "    ),\n",
      "    dict(\n",
      "        fullname='Conv3d_dilated',\n",
      "        constructor=lambda: nn.Conv3d(3, 4, kernel_size=2, dilation=2),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ConvTranspose3d',\n",
      "        constructor_args=(2, 3, (2, 3, 2)),\n",
      "        cudnn=True,\n",
      "        input_size=(1, 2, 4, 5, 4)\n",
      "    ),\n",
      "    # TODO\n",
      "    # dict(\n",
      "    #     module_name='ConvTranspose3d',\n",
      "    #     constructor_args=(2, 3, (2, 3, 2), 1, 0, 0, 1, True, (2, 2, 2)),\n",
      "    #     cudnn=True,\n",
      "    #     input_size=(1, 2, 4, 5, 4),\n",
      "    #     desc='dilated'\n",
      "    # ),\n",
      "    dict(\n",
      "        module_name='MaxPool3d',\n",
      "        constructor_args=((2, 2, 2),),\n",
      "        input_size=(2, 3, 5, 5, 5)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='MaxPool3d',\n",
      "        constructor_args=(2, (2, 2, 2)),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "        desc='stride'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='MaxPool3d',\n",
      "        constructor_args=(2, 2, (1, 1, 1)),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "        desc='stride_padding'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool3d',\n",
      "        constructor_args=((2, 2, 2),),\n",
      "        input_size=(2, 3, 4, 4, 4)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AvgPool3d',\n",
      "        constructor_args=(2, (2, 2, 2)),\n",
      "        input_size=(2, 3, 5, 5, 5),\n",
      "        desc='stride'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='ReplicationPad3d',\n",
      "        constructor_args=((1, 2, 3, 4, 5, 6),),\n",
      "        input_size=(2, 3, 5, 5, 5)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Embedding',\n",
      "        constructor_args=(4, 3),\n",
      "        input=Variable(torch.randperm(2).repeat(1, 2)),\n",
      "        jacobian_input=False\n",
      "    ),\n",
      "    dict(\n",
      "        constructor=lambda: nn.Embedding(4, 3, sparse=True),\n",
      "        input=Variable(torch.randperm(2).repeat(1, 2)),\n",
      "        jacobian_input=False,\n",
      "        fullname='Embedding_sparse'\n",
      "    ),\n",
      "    dict(\n",
      "        constructor=lambda: nn.FractionalMaxPool2d(\n",
      "            2, output_ratio=0.5, _random_samples=torch.DoubleTensor(1, 3, 2).uniform_()),\n",
      "        input_size=(1, 3, 5, 5),\n",
      "        fullname='FractionalMaxPool2d_ratio',\n",
      "        test_cuda=False\n",
      "    ),\n",
      "    dict(\n",
      "        constructor=lambda: nn.FractionalMaxPool2d((2, 2), output_size=(\n",
      "            4, 4), _random_samples=torch.DoubleTensor(1, 3, 2).uniform_()),\n",
      "        input_size=(1, 3, 7, 7),\n",
      "        fullname='FractionalMaxPool2d_size',\n",
      "        test_cuda=False\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='PixelShuffle',\n",
      "        constructor_args=(3,),\n",
      "        input_size=(1, 9, 4, 4),\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(12, None, 'nearest'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='nearest_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=((12, 16), None, 'nearest'),\n",
      "        input_size=(1, 2, 3, 4),\n",
      "        desc='nearest_tuple_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, 4, 'nearest'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='nearest_scale_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(12, None, 'bilinear'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='bilinear_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=((4, 6), None, 'bilinear'),\n",
      "        input_size=(1, 2, 2, 3),\n",
      "        desc='bilinear_tuple_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, 4, 'bilinear'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='bilinear_scale_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, (2, 2), 'bilinear'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='bilinear_scale_tuple_shared_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, (2, 1), 'bilinear'),\n",
      "        input_size=(1, 2, 4, 4),\n",
      "        desc='bilinear_scale_tuple_skewed_2d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(12, None, 'nearest'),\n",
      "        input_size=(1, 2, 4, 4, 4),\n",
      "        desc='nearest_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=((12, 16, 16), None, 'nearest'),\n",
      "        input_size=(1, 2, 3, 4, 4),\n",
      "        desc='nearest_tuple_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, 4, 'nearest'),\n",
      "        input_size=(1, 2, 4, 4, 4),\n",
      "        desc='nearest_scale_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(12, None, 'trilinear'),\n",
      "        input_size=(1, 2, 4, 4, 4),\n",
      "        desc='trilinear_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=((4, 6, 6), None, 'trilinear'),\n",
      "        input_size=(1, 2, 2, 3, 3),\n",
      "        desc='trilinear_tuple_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='Upsample',\n",
      "        constructor_args=(None, 4, 'trilinear'),\n",
      "        input_size=(1, 2, 4, 4, 4),\n",
      "        desc='trilinear_scale_3d'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveMaxPool1d',\n",
      "        constructor_args=(3,),\n",
      "        input=torch.rand(1, 3, 5)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveMaxPool2d',\n",
      "        constructor_args=(3,),\n",
      "        input=torch.rand(1, 3, 5, 6),\n",
      "        desc='single'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveMaxPool2d',\n",
      "        constructor_args=((3, 4),),\n",
      "        input=torch.rand(1, 3, 5, 6),\n",
      "        desc='tuple'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveAvgPool1d',\n",
      "        constructor_args=(3,),\n",
      "        input=torch.rand(1, 3, 5)\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveAvgPool2d',\n",
      "        constructor_args=(3,),\n",
      "        input=torch.rand(1, 3, 5, 6),\n",
      "        desc='single'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='AdaptiveAvgPool2d',\n",
      "        constructor_args=((3, 4),),\n",
      "        input=torch.rand(1, 3, 5, 6),\n",
      "        desc='tuple'\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='SELU',\n",
      "        input_size=(3, 2, 5),\n",
      "        check_inplace=True\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='GLU',\n",
      "        input_size=(5, 6),\n",
      "    ),\n",
      "]\n",
      "\n",
      "\n",
      "new_criterion_tests = [\n",
      "    dict(\n",
      "        module_name='PoissonNLLLoss',\n",
      "        input=torch.randn(2, 3, 4, 5),\n",
      "        target=torch.randn(2, 3, 4, 5).floor_().abs_(),\n",
      "        desc='reduced_loss',\n",
      "    ),\n",
      "    dict(\n",
      "        module_name='PoissonNLLLoss',\n",
      "        constructor_args=(False, True, True),\n",
      "        input=torch.randn(2, 3, 4, 5).abs_().add_(0.001),\n",
      "        target=torch.randn(2, 3, 4, 5).floor_().abs_(),\n",
      "        desc='full_loss',\n",
      "    ),\n",
      "]\n",
      "\n",
      "\n",
      "for test_params in module_tests + new_module_tests:\n",
      "    # TODO: CUDA is not implemented yet\n",
      "    if 'constructor' not in test_params:\n",
      "        name = test_params.pop('module_name')\n",
      "        test_params['constructor'] = getattr(nn, name)\n",
      "    test = NewModuleTest(**test_params)\n",
      "    add_test(test)\n",
      "\n",
      "for test_params in criterion_tests + new_criterion_tests:\n",
      "    name = test_params.pop('module_name')\n",
      "    test_params['constructor'] = getattr(nn, name)\n",
      "    test = NewCriterionTest(**test_params)\n",
      "    add_test(test)\n",
      "\n",
      "\n",
      "class UnpoolingNet(nn.Module):\n",
      "    def __init__(self, pool, unpool):\n",
      "        super(UnpoolingNet, self).__init__()\n",
      "        self.pool = pool\n",
      "        self.unpool = unpool\n",
      "\n",
      "    def forward(self, input):\n",
      "        return self.unpool(*self.pool(input))\n",
      "\n",
      "\n",
      "add_test(NewModuleTest(\n",
      "    constructor=lambda: UnpoolingNet(\n",
      "        nn.MaxPool1d(2, return_indices=True),\n",
      "        nn.MaxUnpool1d(2)),\n",
      "    input_size=(1, 1, 4),\n",
      "    fullname='MaxUnpool1d_net'))\n",
      "add_test(NewModuleTest(\n",
      "    constructor=lambda: UnpoolingNet(\n",
      "        nn.MaxPool2d(2, return_indices=True),\n",
      "        nn.MaxUnpool2d(2)),\n",
      "    input_size=(1, 1, 2, 4),\n",
      "    fullname='MaxUnpool2d_net'))\n",
      "add_test(NewModuleTest(\n",
      "    constructor=lambda: UnpoolingNet(\n",
      "        nn.MaxPool3d(2, return_indices=True),\n",
      "        nn.MaxUnpool3d(2)),\n",
      "    input_size=(1, 1, 2, 4, 6),\n",
      "    fullname='MaxUnpool3d_net'))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    run_tests()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(model_list[0]) as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ast.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax error in  /home/irteam/users/data/pytorch_code/longcw/yolo2-pytorch/darknet.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/picture_demo.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/web_demo.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation/caffe_to_pytorch/convert.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/sunshineatnoon/Paper-Implementations/BEGAN/models.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/Cadene/pretrained-models.pytorch/pretrainedmodels/wideresnet.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/isht7/pytorch-deeplab-resnet/deeplab_resnet.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_language.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_mnist.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/caogang/wgan-gp/gan_toy.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/marvis/pytorch-yolo2/models/caffe_net.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/jacobgil/pytorch-pruning/finetune.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/faster_rcnn.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/PureDiors/pytorch_RFCN/faster_rcnn/rfcn.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/ypxie/pytorch-cramer-Gan/cramerGan/Models.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/andrewliao11/pytorch-a3c-mujoco/model.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/thnkim/OpenFacePytorch/SpatialCrossMapLRN_temp.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/cmusjtuliuyuan/SequenceTagging/CRF.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/JamesChuanggg/pytorch-REINFORCE/reinforce_continuous.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/JamesChuanggg/pytorch-REINFORCE/reinforce_discrete.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/rogertrullo/pytorch_convlstm/conv_lstm.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/Jiaming-Liu/pytorch-misc/glorot_uniform_init.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/dandelin/Dynamic-memory-networks-plus-Pytorch/babi_main.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/gpleiss/efficient_densenet_pytorch/models/densenet.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/gpleiss/efficient_densenet_pytorch/models/densenet_efficient.py\n",
      "Syntax error in  /home/irteam/users/data/pytorch_code/huaijin-chen/pytorch-PersonReID/module/l2normalize.py\n"
     ]
    }
   ],
   "source": [
    "conv_list = []\n",
    "linear_list = []\n",
    "error_list = []\n",
    "for filename in model_list:\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    try:\n",
    "        tree = ast.parse(text)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.ClassDef):\n",
    "                code = astunparse.unparse(node)\n",
    "                if 'nn.Linear' in code:\n",
    "                    linear_list.append(code)\n",
    "                if 'nn.Conv' in code:\n",
    "                    conv_list.append(code)\n",
    "    except SyntaxError:\n",
    "        print(\"Syntax error in \",filename)\n",
    "        error_list.append(filename)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "    'Simple CNN model to convert an input image to (1+9) heatmaps.'\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
      "        self.bn1 = nn.BatchNorm2d(16)\n",
      "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
      "        self.bn2 = nn.BatchNorm2d(32)\n",
      "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
      "        self.bn3 = nn.BatchNorm2d(64)\n",
      "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
      "        self.bn4 = nn.BatchNorm2d(128)\n",
      "        self.conv5 = nn.Conv2d(128, 128, 3, dilation=2, padding=2)\n",
      "        self.bn5 = nn.BatchNorm2d(128)\n",
      "        self.conv6 = nn.Conv2d(128, 128, 3, dilation=4, padding=4)\n",
      "        self.bn6 = nn.BatchNorm2d(128)\n",
      "        self.conv7 = nn.Conv2d(128, (1 + 9), 3, padding=1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        'Forward input images through the network to generate heatmaps.'\n",
      "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
      "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)\n",
      "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
      "        x = F.relu(self.bn4(self.conv4(x)))\n",
      "        x = F.relu(self.bn5(self.conv5(x)))\n",
      "        x = F.relu(self.bn6(self.conv6(x)))\n",
      "        x = F.sigmoid(self.conv7(x))\n",
      "        return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conv_list[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = node.names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'math'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
